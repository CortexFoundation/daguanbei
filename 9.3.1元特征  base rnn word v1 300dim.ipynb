{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "window.load_remote_theme = true\n",
       "var theme_js = \"https://odhk.github.io/hyrule_theme/custom.js\";\n",
       "\n",
       "window.load_local_theme = function(){\n",
       "    var hostname = document.location.hostname\n",
       "    return ((hostname == \"localhost\" || hostname == '127.0.0.1') && !load_remote_theme)\n",
       "}\n",
       "\n",
       "var url = load_local_theme() ? document.location.origin + \"/files/theme/custom.js\" : theme_js\n",
       "\n",
       "$.getScript(url)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "window.load_remote_theme = true\n",
    "var theme_js = \"https://odhk.github.io/hyrule_theme/custom.js\";\n",
    "\n",
    "window.load_local_theme = function(){\n",
    "    var hostname = document.location.hostname\n",
    "    return ((hostname == \"localhost\" || hostname == '127.0.0.1') && !load_remote_theme)\n",
    "}\n",
    "\n",
    "var url = load_local_theme() ? document.location.origin + \"/files/theme/custom.js\" : theme_js\n",
    "\n",
    "$.getScript(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/libo/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "1271461it [01:45, 12058.38it/s]\n",
      "  2%|▏         | 30100/1271460 [00:00<00:04, 300937.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1271460 unique tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1271460/1271460 [00:04<00:00, 287511.47it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import keras\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Lambda, Embedding,constraints,\\\n",
    "Dropout, Activation,GRU,Bidirectional,Subtract, Permute, TimeDistributed, Reshape\n",
    "from keras.layers import Conv1D,Conv2D,MaxPooling2D,GlobalAveragePooling1D,GlobalMaxPooling1D, MaxPooling1D, Flatten\n",
    "from keras.layers import CuDNNGRU, CuDNNLSTM, SpatialDropout1D,Layer, initializers, regularizers\n",
    "from keras.layers.merge import concatenate, Concatenate, Average, Dot, Maximum, Multiply, Subtract\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from keras.activations import softmax\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import *\n",
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy.special import erfinv\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train = pd.read_csv('train_set.csv')\n",
    "test = pd.read_csv('test_set.csv')\n",
    "\n",
    "label = train['class']\n",
    "\n",
    "char_embedding_index = {}\n",
    "f = open('word_embedding_300dim_new2.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0].lower()\n",
    "    embeddings = np.asarray(values[1:], dtype = 'float32')\n",
    "    char_embedding_index[word] = embeddings\n",
    "f.close()\n",
    "\n",
    "len(char_embedding_index)\n",
    "\n",
    "api_embedding_file_path = 'word_embedding_300dim_new2.txt'\n",
    "embedding_dim = 300\n",
    "max_nb_api = len(char_embedding_index) + 1\n",
    "max_sequence_length_api =1200\n",
    "\n",
    "all_word = list(train['word_seg'].values) + list(test['word_seg'].values)\n",
    "\n",
    "all_word_list = []\n",
    "for i in list(all_word):\n",
    "    all_word_list.append(i.split(' '))\n",
    "\n",
    "len(all_word_list)\n",
    "\n",
    "train['word_seg'].apply(lambda x:len(x.split(' '))).describe()\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "tokenizer_api = Tokenizer()\n",
    "\n",
    "tokenizer_api.fit_on_texts(all_word_list)\n",
    "\n",
    "api_seq = tokenizer_api.texts_to_sequences(all_word_list)\n",
    "\n",
    "data['word_seq'] = pad_sequences(api_seq, maxlen = max_sequence_length_api, padding='post').tolist()\n",
    "\n",
    "api_index = tokenizer_api.word_index\n",
    "print('Found %s unique tokens' % len(api_index))\n",
    "nb_words = min(max_nb_api, len(char_embedding_index)+1)\n",
    "api_embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "\n",
    "for word, i in tqdm(api_index.items()):\n",
    "    embedding_vector = char_embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        api_embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        print('no char%s'%word)\n",
    "\n",
    "data['id'] = train['id']\n",
    "\n",
    "train_FE = data.iloc[:len(label),:]\n",
    "test_FE = data.iloc[len(label):,:]\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "def get_input(df):\n",
    "    \n",
    "    _input = [np.array(df.word_seq.values.tolist())]\n",
    "    \n",
    "    \n",
    "    return _input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN1():\n",
    "    \n",
    "    #Input layer\n",
    "    char_input = Input(shape=(max_sequence_length_api,), dtype='int32')\n",
    "    \n",
    "    api_embedding_layer = Embedding(nb_words,\n",
    "        embedding_dim,\n",
    "        weights=[api_embedding_matrix],\n",
    "        input_length=max_sequence_length_api,\n",
    "        trainable=False)\n",
    "    \n",
    "    #RNN\n",
    "    char_rnn_layer_1 = Bidirectional(CuDNNLSTM(250, \n",
    "                               return_sequences=True))\n",
    "    char_rnn_layer_2 = Bidirectional(CuDNNLSTM(250, \n",
    "                               return_sequences=True))\n",
    "    \n",
    "    char_embedding = api_embedding_layer(char_input)\n",
    "    #char_embedding = SpatialDropout1D(0.2)(char_embedding)\n",
    "    \n",
    "    #双层BiLSTM\n",
    "    char_rnn_1= char_rnn_layer_1(char_embedding)\n",
    "    char_rnn_2= char_rnn_layer_2(char_rnn_1)\n",
    "    \n",
    "    #max avg min\n",
    "    char_att_maxpooling_1 = GlobalMaxPooling1D()(char_rnn_1)\n",
    "    char_att_avgpooling_1 = GlobalAveragePooling1D()(char_rnn_1)\n",
    "    \n",
    "    char_att_maxpooling_2 = GlobalMaxPooling1D()(char_rnn_2)\n",
    "    char_att_avgpooling_2 = GlobalAveragePooling1D()(char_rnn_2)\n",
    "    \n",
    "    sub_char_abs_1 = Lambda(lambda x:K.abs(x[0] - x[1]))([char_att_maxpooling_1, char_att_avgpooling_1])\n",
    "    sub_char_abs_2 = Lambda(lambda x:K.abs(x[0] - x[1]))([char_att_maxpooling_2, char_att_avgpooling_2])\n",
    "    \n",
    "    merged = concatenate([char_att_maxpooling_1, char_att_avgpooling_1, \\\n",
    "                          char_att_maxpooling_2, char_att_avgpooling_2, sub_char_abs_1, sub_char_abs_2])\n",
    "    \n",
    "    merged = Dropout(0.2)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dense(400, activation='relu')(merged)\n",
    "    merged = Dropout(0.1)(merged)\n",
    "    merged = BatchNormalization()(merged) \n",
    "    preds = Dense(19, activation = 'softmax')(merged)\n",
    "    \n",
    "    model = Model(inputs=[char_input],outputs=preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train = pd.DataFrame(np.array([None]*train_FE.shape[0]*19).reshape(train_FE.shape[0], 19))\n",
    "meta_train.columns = ['prob' + str(x) for x in range(19)]\n",
    "meta_test = pd.DataFrame()\n",
    "\n",
    "label = train['class']\n",
    "\n",
    "label = label - 1\n",
    "\n",
    "categorical_labels = keras.utils.np_utils.to_categorical(label, num_classes=19)\n",
    "\n",
    "index = pd.read_csv('fold.csv')\n",
    "index['id'] = train['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******0*******\n",
      "Train on 92040 samples, validate on 10237 samples\n",
      "Epoch 1/20\n",
      "92040/92040 [==============================] - 468s 5ms/step - loss: 1.0156 - acc: 0.7062 - val_loss: 0.9709 - val_acc: 0.7060\n",
      "Epoch 2/20\n",
      "92040/92040 [==============================] - 403s 4ms/step - loss: 0.8158 - acc: 0.7566 - val_loss: 0.8605 - val_acc: 0.7512\n",
      "Epoch 3/20\n",
      "92040/92040 [==============================] - 403s 4ms/step - loss: 0.7471 - acc: 0.7751 - val_loss: 0.7859 - val_acc: 0.7680\n",
      "Epoch 4/20\n",
      "92040/92040 [==============================] - 403s 4ms/step - loss: 0.6888 - acc: 0.7899 - val_loss: 0.8150 - val_acc: 0.7598\n",
      "Epoch 5/20\n",
      "92040/92040 [==============================] - 404s 4ms/step - loss: 0.6275 - acc: 0.8073 - val_loss: 0.8442 - val_acc: 0.7550\n",
      "10237/10237 [==============================] - 13s 1ms/step\n",
      "102277/102277 [==============================] - 129s 1ms/step\n",
      "*******1*******\n",
      "Train on 92043 samples, validate on 10234 samples\n",
      "Epoch 1/20\n",
      "92043/92043 [==============================] - 412s 4ms/step - loss: 1.0126 - acc: 0.7041 - val_loss: 1.0021 - val_acc: 0.7101\n",
      "Epoch 2/20\n",
      "92043/92043 [==============================] - 427s 5ms/step - loss: 0.8098 - acc: 0.7586 - val_loss: 0.8911 - val_acc: 0.7368\n",
      "Epoch 3/20\n",
      "92043/92043 [==============================] - 403s 4ms/step - loss: 0.7425 - acc: 0.7767 - val_loss: 0.8343 - val_acc: 0.7534\n",
      "Epoch 4/20\n",
      "92043/92043 [==============================] - 403s 4ms/step - loss: 0.6826 - acc: 0.7936 - val_loss: 0.8321 - val_acc: 0.7588\n",
      "Epoch 5/20\n",
      "92043/92043 [==============================] - 404s 4ms/step - loss: 0.6238 - acc: 0.8081 - val_loss: 0.8648 - val_acc: 0.7623\n",
      "Epoch 6/20\n",
      "92043/92043 [==============================] - 404s 4ms/step - loss: 0.5647 - acc: 0.8242 - val_loss: 0.8252 - val_acc: 0.7653\n",
      "Epoch 7/20\n",
      "92043/92043 [==============================] - 403s 4ms/step - loss: 0.5009 - acc: 0.8412 - val_loss: 0.8110 - val_acc: 0.7700\n",
      "Epoch 8/20\n",
      "92043/92043 [==============================] - 403s 4ms/step - loss: 0.4386 - acc: 0.8584 - val_loss: 0.8579 - val_acc: 0.7640\n",
      "Epoch 9/20\n",
      "92043/92043 [==============================] - 403s 4ms/step - loss: 0.3767 - acc: 0.8767 - val_loss: 0.8915 - val_acc: 0.7693\n",
      "10234/10234 [==============================] - 13s 1ms/step\n",
      "102277/102277 [==============================] - 127s 1ms/step\n",
      "*******2*******\n",
      "Train on 92044 samples, validate on 10233 samples\n",
      "Epoch 1/20\n",
      "92044/92044 [==============================] - 407s 4ms/step - loss: 1.0221 - acc: 0.7033 - val_loss: 0.9675 - val_acc: 0.7146\n",
      "Epoch 2/20\n",
      "92044/92044 [==============================] - 409s 4ms/step - loss: 0.8162 - acc: 0.7567 - val_loss: 0.9921 - val_acc: 0.7158\n",
      "Epoch 3/20\n",
      "92044/92044 [==============================] - 406s 4ms/step - loss: 0.7469 - acc: 0.7747 - val_loss: 0.7839 - val_acc: 0.7693\n",
      "Epoch 4/20\n",
      "92044/92044 [==============================] - 412s 4ms/step - loss: 0.6884 - acc: 0.7909 - val_loss: 0.7898 - val_acc: 0.7671\n",
      "Epoch 5/20\n",
      "92044/92044 [==============================] - 408s 4ms/step - loss: 0.6322 - acc: 0.8056 - val_loss: 0.7757 - val_acc: 0.7768\n",
      "Epoch 6/20\n",
      "92044/92044 [==============================] - 405s 4ms/step - loss: 0.5672 - acc: 0.8222 - val_loss: 0.8004 - val_acc: 0.7656\n",
      "Epoch 7/20\n",
      "92044/92044 [==============================] - 405s 4ms/step - loss: 0.5018 - acc: 0.8401 - val_loss: 0.7775 - val_acc: 0.7818\n",
      "Epoch 8/20\n",
      "92044/92044 [==============================] - 405s 4ms/step - loss: 0.4424 - acc: 0.8574 - val_loss: 0.7787 - val_acc: 0.7818\n",
      "Epoch 9/20\n",
      "92044/92044 [==============================] - 405s 4ms/step - loss: 0.3852 - acc: 0.8735 - val_loss: 0.8113 - val_acc: 0.7782\n",
      "10233/10233 [==============================] - 13s 1ms/step\n",
      "102277/102277 [==============================] - 126s 1ms/step\n",
      "*******3*******\n",
      "Train on 92046 samples, validate on 10231 samples\n",
      "Epoch 1/20\n",
      "92046/92046 [==============================] - 404s 4ms/step - loss: 1.0233 - acc: 0.7021 - val_loss: 1.0667 - val_acc: 0.7051\n",
      "Epoch 2/20\n",
      "92046/92046 [==============================] - 403s 4ms/step - loss: 0.8167 - acc: 0.7558 - val_loss: 0.8877 - val_acc: 0.7420\n",
      "Epoch 3/20\n",
      "92046/92046 [==============================] - 403s 4ms/step - loss: 0.7476 - acc: 0.7753 - val_loss: 0.8526 - val_acc: 0.7503\n",
      "Epoch 4/20\n",
      "92046/92046 [==============================] - 404s 4ms/step - loss: 0.6877 - acc: 0.7904 - val_loss: 0.7759 - val_acc: 0.7714\n",
      "Epoch 5/20\n",
      "92046/92046 [==============================] - 404s 4ms/step - loss: 0.6263 - acc: 0.8073 - val_loss: 0.7644 - val_acc: 0.7767\n",
      "Epoch 6/20\n",
      "92046/92046 [==============================] - 404s 4ms/step - loss: 0.5640 - acc: 0.8244 - val_loss: 0.7885 - val_acc: 0.7782\n",
      "Epoch 7/20\n",
      "92046/92046 [==============================] - 404s 4ms/step - loss: 0.5024 - acc: 0.8406 - val_loss: 0.7881 - val_acc: 0.7771\n",
      "Epoch 8/20\n",
      "92046/92046 [==============================] - 404s 4ms/step - loss: 0.4417 - acc: 0.8573 - val_loss: 0.8242 - val_acc: 0.7802\n",
      "Epoch 9/20\n",
      "92046/92046 [==============================] - 404s 4ms/step - loss: 0.3800 - acc: 0.8750 - val_loss: 0.8608 - val_acc: 0.7728\n",
      "Epoch 10/20\n",
      "92046/92046 [==============================] - 403s 4ms/step - loss: 0.3258 - acc: 0.8915 - val_loss: 0.9360 - val_acc: 0.7702\n",
      "10231/10231 [==============================] - 13s 1ms/step\n",
      "102277/102277 [==============================] - 128s 1ms/step\n",
      "*******4*******\n",
      "Train on 92049 samples, validate on 10228 samples\n",
      "Epoch 1/20\n",
      "92049/92049 [==============================] - 405s 4ms/step - loss: 1.0159 - acc: 0.7052 - val_loss: 1.0430 - val_acc: 0.7040\n",
      "Epoch 2/20\n",
      "92049/92049 [==============================] - 404s 4ms/step - loss: 0.8165 - acc: 0.7572 - val_loss: 0.8492 - val_acc: 0.7460\n",
      "Epoch 3/20\n",
      "92049/92049 [==============================] - 404s 4ms/step - loss: 0.7465 - acc: 0.7746 - val_loss: 0.8514 - val_acc: 0.7461\n",
      "Epoch 4/20\n",
      "92049/92049 [==============================] - 404s 4ms/step - loss: 0.6878 - acc: 0.7918 - val_loss: 0.7937 - val_acc: 0.7659\n",
      "Epoch 5/20\n",
      "92049/92049 [==============================] - 404s 4ms/step - loss: 0.6296 - acc: 0.8061 - val_loss: 0.7552 - val_acc: 0.7784\n",
      "Epoch 6/20\n",
      "92049/92049 [==============================] - 404s 4ms/step - loss: 0.5689 - acc: 0.8229 - val_loss: 0.8424 - val_acc: 0.7579\n",
      "Epoch 7/20\n",
      "92049/92049 [==============================] - 404s 4ms/step - loss: 0.5039 - acc: 0.8393 - val_loss: 0.8021 - val_acc: 0.7736\n",
      "10228/10228 [==============================] - 13s 1ms/step\n",
      "102277/102277 [==============================] - 126s 1ms/step\n",
      "*******5*******\n",
      "Train on 92051 samples, validate on 10226 samples\n",
      "Epoch 1/20\n",
      "92051/92051 [==============================] - 405s 4ms/step - loss: 1.0178 - acc: 0.7056 - val_loss: 0.8995 - val_acc: 0.7430\n",
      "Epoch 2/20\n",
      "92051/92051 [==============================] - 403s 4ms/step - loss: 0.8164 - acc: 0.7579 - val_loss: 0.8591 - val_acc: 0.7510\n",
      "Epoch 3/20\n",
      "92051/92051 [==============================] - 403s 4ms/step - loss: 0.7449 - acc: 0.7754 - val_loss: 0.7818 - val_acc: 0.7659\n",
      "Epoch 4/20\n",
      "92051/92051 [==============================] - 403s 4ms/step - loss: 0.6856 - acc: 0.7914 - val_loss: 0.7917 - val_acc: 0.7636\n",
      "Epoch 5/20\n",
      "92051/92051 [==============================] - 403s 4ms/step - loss: 0.6254 - acc: 0.8079 - val_loss: 0.7907 - val_acc: 0.7691\n",
      "Epoch 6/20\n",
      "92051/92051 [==============================] - 404s 4ms/step - loss: 0.5622 - acc: 0.8241 - val_loss: 0.7712 - val_acc: 0.7769\n",
      "Epoch 7/20\n",
      "92051/92051 [==============================] - 404s 4ms/step - loss: 0.4988 - acc: 0.8405 - val_loss: 0.8533 - val_acc: 0.7676\n",
      "Epoch 8/20\n",
      "92051/92051 [==============================] - 408s 4ms/step - loss: 0.4352 - acc: 0.8591 - val_loss: 0.8544 - val_acc: 0.7676\n",
      "10226/10226 [==============================] - 13s 1ms/step\n",
      "102277/102277 [==============================] - 126s 1ms/step\n",
      "*******6*******\n",
      "Train on 92053 samples, validate on 10224 samples\n",
      "Epoch 1/20\n",
      "92053/92053 [==============================] - 416s 5ms/step - loss: 1.0145 - acc: 0.7053 - val_loss: 0.8633 - val_acc: 0.7429\n",
      "Epoch 2/20\n",
      "92053/92053 [==============================] - 404s 4ms/step - loss: 0.8150 - acc: 0.7575 - val_loss: 0.7534 - val_acc: 0.7776\n",
      "Epoch 3/20\n",
      "92053/92053 [==============================] - 404s 4ms/step - loss: 0.7464 - acc: 0.7754 - val_loss: 0.7479 - val_acc: 0.7813\n",
      "Epoch 4/20\n",
      "92053/92053 [==============================] - 405s 4ms/step - loss: 0.6887 - acc: 0.7910 - val_loss: 0.7443 - val_acc: 0.7787\n",
      "Epoch 5/20\n",
      "92053/92053 [==============================] - 404s 4ms/step - loss: 0.6283 - acc: 0.8070 - val_loss: 0.7305 - val_acc: 0.7889\n",
      "Epoch 6/20\n",
      "92053/92053 [==============================] - 404s 4ms/step - loss: 0.5630 - acc: 0.8247 - val_loss: 0.7658 - val_acc: 0.7809\n",
      "Epoch 7/20\n",
      "92053/92053 [==============================] - 404s 4ms/step - loss: 0.5034 - acc: 0.8400 - val_loss: 0.7918 - val_acc: 0.7894\n",
      "Epoch 8/20\n",
      "92053/92053 [==============================] - 404s 4ms/step - loss: 0.4370 - acc: 0.8583 - val_loss: 0.7879 - val_acc: 0.7858\n",
      "Epoch 9/20\n",
      "92053/92053 [==============================] - 404s 4ms/step - loss: 0.3797 - acc: 0.8746 - val_loss: 0.8638 - val_acc: 0.7900\n",
      "Epoch 10/20\n",
      "92053/92053 [==============================] - 404s 4ms/step - loss: 0.3293 - acc: 0.8891 - val_loss: 0.8467 - val_acc: 0.7861\n",
      "Epoch 11/20\n",
      "92053/92053 [==============================] - 405s 4ms/step - loss: 0.2825 - acc: 0.9052 - val_loss: 0.9209 - val_acc: 0.7819\n",
      "10224/10224 [==============================] - 13s 1ms/step\n",
      "102277/102277 [==============================] - 126s 1ms/step\n",
      "*******7*******\n",
      "Train on 92054 samples, validate on 10223 samples\n",
      "Epoch 1/20\n",
      "92054/92054 [==============================] - 403s 4ms/step - loss: 1.0189 - acc: 0.7047 - val_loss: 0.9161 - val_acc: 0.7368\n",
      "Epoch 2/20\n",
      "92054/92054 [==============================] - 402s 4ms/step - loss: 0.8157 - acc: 0.7569 - val_loss: 0.8651 - val_acc: 0.7476\n",
      "Epoch 3/20\n",
      "92054/92054 [==============================] - 402s 4ms/step - loss: 0.7465 - acc: 0.7756 - val_loss: 0.8048 - val_acc: 0.7640\n",
      "Epoch 4/20\n",
      "92054/92054 [==============================] - 401s 4ms/step - loss: 0.6870 - acc: 0.7904 - val_loss: 0.7807 - val_acc: 0.7704\n",
      "Epoch 5/20\n",
      "92054/92054 [==============================] - 402s 4ms/step - loss: 0.6282 - acc: 0.8072 - val_loss: 0.8092 - val_acc: 0.7631\n",
      "Epoch 6/20\n",
      "92054/92054 [==============================] - 402s 4ms/step - loss: 0.5653 - acc: 0.8236 - val_loss: 0.7774 - val_acc: 0.7758\n",
      "Epoch 7/20\n",
      "92054/92054 [==============================] - 402s 4ms/step - loss: 0.5028 - acc: 0.8402 - val_loss: 0.7973 - val_acc: 0.7798\n",
      "Epoch 8/20\n",
      "92054/92054 [==============================] - 402s 4ms/step - loss: 0.4380 - acc: 0.8578 - val_loss: 0.8259 - val_acc: 0.7761\n",
      "Epoch 9/20\n",
      "92054/92054 [==============================] - 402s 4ms/step - loss: 0.3784 - acc: 0.8756 - val_loss: 0.9224 - val_acc: 0.7678\n",
      "10223/10223 [==============================] - 12s 1ms/step\n",
      "102277/102277 [==============================] - 125s 1ms/step\n",
      "*******8*******\n",
      "Train on 92056 samples, validate on 10221 samples\n",
      "Epoch 1/20\n",
      "92056/92056 [==============================] - 403s 4ms/step - loss: 1.0197 - acc: 0.7039 - val_loss: 0.8780 - val_acc: 0.7450\n",
      "Epoch 2/20\n",
      "92056/92056 [==============================] - 402s 4ms/step - loss: 0.8187 - acc: 0.7566 - val_loss: 0.8734 - val_acc: 0.7410\n",
      "Epoch 3/20\n",
      "92056/92056 [==============================] - 402s 4ms/step - loss: 0.7489 - acc: 0.7756 - val_loss: 0.7645 - val_acc: 0.7654\n",
      "Epoch 4/20\n",
      "92056/92056 [==============================] - 402s 4ms/step - loss: 0.6882 - acc: 0.7908 - val_loss: 0.7474 - val_acc: 0.7759\n",
      "Epoch 5/20\n",
      "92056/92056 [==============================] - 402s 4ms/step - loss: 0.6311 - acc: 0.8061 - val_loss: 0.7597 - val_acc: 0.7704\n",
      "Epoch 6/20\n",
      "92056/92056 [==============================] - 406s 4ms/step - loss: 0.5668 - acc: 0.8233 - val_loss: 0.7704 - val_acc: 0.7741\n",
      "10221/10221 [==============================] - 13s 1ms/step\n",
      "102277/102277 [==============================] - 126s 1ms/step\n",
      "*******9*******\n",
      "Train on 92057 samples, validate on 10220 samples\n",
      "Epoch 1/20\n",
      "92057/92057 [==============================] - 403s 4ms/step - loss: 1.0179 - acc: 0.7060 - val_loss: 0.9033 - val_acc: 0.7385\n",
      "Epoch 2/20\n",
      "92057/92057 [==============================] - 402s 4ms/step - loss: 0.8130 - acc: 0.7586 - val_loss: 0.9224 - val_acc: 0.7221\n",
      "Epoch 3/20\n",
      "92057/92057 [==============================] - 402s 4ms/step - loss: 0.7465 - acc: 0.7753 - val_loss: 0.8213 - val_acc: 0.7528\n",
      "Epoch 4/20\n",
      "92057/92057 [==============================] - 402s 4ms/step - loss: 0.6878 - acc: 0.7913 - val_loss: 0.7662 - val_acc: 0.7684\n",
      "Epoch 5/20\n",
      "92057/92057 [==============================] - 402s 4ms/step - loss: 0.6261 - acc: 0.8082 - val_loss: 0.7626 - val_acc: 0.7780\n",
      "Epoch 6/20\n",
      "92057/92057 [==============================] - 402s 4ms/step - loss: 0.5674 - acc: 0.8229 - val_loss: 0.8199 - val_acc: 0.7683\n",
      "Epoch 7/20\n",
      "92057/92057 [==============================] - 402s 4ms/step - loss: 0.4996 - acc: 0.8429 - val_loss: 0.7722 - val_acc: 0.7813\n",
      "Epoch 8/20\n",
      "92057/92057 [==============================] - 400s 4ms/step - loss: 0.4386 - acc: 0.8588 - val_loss: 0.8167 - val_acc: 0.7740\n",
      "Epoch 9/20\n",
      "56768/92057 [=================>............] - ETA: 2:27 - loss: 0.3562 - acc: 0.8825"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#logloss_list = []\n",
    "for i in range(10):\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    print('*******%s*******'%i)\n",
    "    idx_train = np.array(index[index['fold'] != i]['id'])\n",
    "    idx_val = np.array(index[index['fold'] == i]['id'])\n",
    "    \n",
    "    \n",
    "    X_train = train_FE.loc[idx_train, :]\n",
    "    y_train = categorical_labels[idx_train]\n",
    "\n",
    "    X_val = train_FE.loc[idx_val, :]\n",
    "    y_val = categorical_labels[idx_val]\n",
    "\n",
    "    #dtest = test.loc[:,user_col]\n",
    "    \n",
    "    X_train = get_input(X_train)\n",
    "    X_val = get_input(X_val)\n",
    "    X_test = get_input(test_FE)\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "#     bst_model_path = '10folds_' + 'base_rnn_wrod_v1_300dim' + str(i) + '.hdf5'\n",
    "#     early_stopping =EarlyStopping(monitor='val_acc', patience=2)\n",
    "#     model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "#     callbacks = [\n",
    "#             early_stopping,\n",
    "#             model_checkpoint\n",
    "#         ]\n",
    "    model = RNN1()\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',\\\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    hist = model.fit(X_train, y_train, \\\n",
    "            validation_data=(X_val, y_val), \\\n",
    "            epochs=10, batch_size=BATCH_SIZE, shuffle=True)#callbacks=callbacks\n",
    "    \n",
    "    pred = pd.DataFrame(model.predict(X_val,batch_size=128,verbose=1))\n",
    "    \n",
    "    for i in range(19):\n",
    "        meta_train.loc[idx_val, 'prob' + str(i)] = pred[i].values\n",
    "    \n",
    "    prob = model.predict(X_test,batch_size=128,verbose=1)\n",
    "    meta_test = pd.concat([meta_test, pd.DataFrame(prob)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meta_base_rnn_wrod_v1_300dim_prob0</th>\n",
       "      <th>meta_base_rnn_wrod_v1_300dim_prob1</th>\n",
       "      <th>meta_base_rnn_wrod_v1_300dim_prob2</th>\n",
       "      <th>meta_base_rnn_wrod_v1_300dim_prob3</th>\n",
       "      <th>meta_base_rnn_wrod_v1_300dim_prob4</th>\n",
       "      <th>meta_base_rnn_wrod_v1_300dim_prob5</th>\n",
       "      <th>meta_base_rnn_wrod_v1_300dim_prob6</th>\n",
       "      <th>meta_base_rnn_wrod_v1_300dim_prob7</th>\n",
       "      <th>meta_base_rnn_wrod_v1_300dim_prob8</th>\n",
       "      <th>meta_base_rnn_wrod_v1_300dim_prob9</th>\n",
       "      <th>meta_base_rnn_wrod_v1_300dim_prob10</th>\n",
       "      <th>meta_base_rnn_wrod_v1_300dim_prob11</th>\n",
       "      <th>meta_base_rnn_wrod_v1_300dim_prob12</th>\n",
       "      <th>meta_base_rnn_wrod_v1_300dim_prob13</th>\n",
       "      <th>meta_base_rnn_wrod_v1_300dim_prob14</th>\n",
       "      <th>meta_base_rnn_wrod_v1_300dim_prob15</th>\n",
       "      <th>meta_base_rnn_wrod_v1_300dim_prob16</th>\n",
       "      <th>meta_base_rnn_wrod_v1_300dim_prob17</th>\n",
       "      <th>meta_base_rnn_wrod_v1_300dim_prob18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.843309</td>\n",
       "      <td>0.00385805</td>\n",
       "      <td>0.000102125</td>\n",
       "      <td>0.000280596</td>\n",
       "      <td>0.00113676</td>\n",
       "      <td>0.000297191</td>\n",
       "      <td>0.00201609</td>\n",
       "      <td>0.011277</td>\n",
       "      <td>0.000228425</td>\n",
       "      <td>0.0279983</td>\n",
       "      <td>0.000527547</td>\n",
       "      <td>0.000457277</td>\n",
       "      <td>0.000981998</td>\n",
       "      <td>0.0844372</td>\n",
       "      <td>0.00138605</td>\n",
       "      <td>0.00620003</td>\n",
       "      <td>0.00540942</td>\n",
       "      <td>0.00878303</td>\n",
       "      <td>0.00131441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0151885</td>\n",
       "      <td>0.00205575</td>\n",
       "      <td>0.205176</td>\n",
       "      <td>0.0150657</td>\n",
       "      <td>0.00104939</td>\n",
       "      <td>0.0114522</td>\n",
       "      <td>0.0107847</td>\n",
       "      <td>0.0178365</td>\n",
       "      <td>0.00477458</td>\n",
       "      <td>0.0044505</td>\n",
       "      <td>0.0104938</td>\n",
       "      <td>0.00337448</td>\n",
       "      <td>0.0326517</td>\n",
       "      <td>0.000376994</td>\n",
       "      <td>0.561974</td>\n",
       "      <td>0.00221263</td>\n",
       "      <td>0.00721158</td>\n",
       "      <td>0.057818</td>\n",
       "      <td>0.0360538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00957131</td>\n",
       "      <td>0.000227768</td>\n",
       "      <td>0.000799553</td>\n",
       "      <td>0.0167076</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.000451077</td>\n",
       "      <td>0.263447</td>\n",
       "      <td>0.000484068</td>\n",
       "      <td>0.00258696</td>\n",
       "      <td>0.00594361</td>\n",
       "      <td>0.00607816</td>\n",
       "      <td>0.00684792</td>\n",
       "      <td>0.00824489</td>\n",
       "      <td>0.673102</td>\n",
       "      <td>0.00141714</td>\n",
       "      <td>3.83659e-05</td>\n",
       "      <td>0.000402002</td>\n",
       "      <td>0.000100433</td>\n",
       "      <td>0.00276673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0123792</td>\n",
       "      <td>0.00113402</td>\n",
       "      <td>5.87818e-05</td>\n",
       "      <td>0.00197263</td>\n",
       "      <td>0.000478276</td>\n",
       "      <td>6.4085e-05</td>\n",
       "      <td>0.000301501</td>\n",
       "      <td>0.000593545</td>\n",
       "      <td>0.00014242</td>\n",
       "      <td>0.000111584</td>\n",
       "      <td>5.6173e-05</td>\n",
       "      <td>0.00204868</td>\n",
       "      <td>0.680166</td>\n",
       "      <td>0.280089</td>\n",
       "      <td>9.91731e-05</td>\n",
       "      <td>0.015348</td>\n",
       "      <td>0.0021364</td>\n",
       "      <td>0.000472363</td>\n",
       "      <td>0.00234889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00254471</td>\n",
       "      <td>6.4346e-05</td>\n",
       "      <td>0.000528202</td>\n",
       "      <td>0.318447</td>\n",
       "      <td>0.000129541</td>\n",
       "      <td>0.000277433</td>\n",
       "      <td>0.0834307</td>\n",
       "      <td>0.000307498</td>\n",
       "      <td>0.0101595</td>\n",
       "      <td>5.16035e-05</td>\n",
       "      <td>0.00849455</td>\n",
       "      <td>0.55004</td>\n",
       "      <td>0.00080656</td>\n",
       "      <td>0.0181468</td>\n",
       "      <td>0.00503975</td>\n",
       "      <td>2.06945e-05</td>\n",
       "      <td>0.00135884</td>\n",
       "      <td>1.57882e-05</td>\n",
       "      <td>0.000136033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0184902</td>\n",
       "      <td>0.000422167</td>\n",
       "      <td>0.0100747</td>\n",
       "      <td>0.000687973</td>\n",
       "      <td>0.000124711</td>\n",
       "      <td>7.7079e-05</td>\n",
       "      <td>0.000932308</td>\n",
       "      <td>0.000122889</td>\n",
       "      <td>8.12786e-05</td>\n",
       "      <td>0.000491437</td>\n",
       "      <td>0.0296258</td>\n",
       "      <td>0.00278336</td>\n",
       "      <td>0.922409</td>\n",
       "      <td>0.000342142</td>\n",
       "      <td>0.00279051</td>\n",
       "      <td>0.000950382</td>\n",
       "      <td>3.25806e-05</td>\n",
       "      <td>0.000489108</td>\n",
       "      <td>0.00907236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.788332</td>\n",
       "      <td>0.000334602</td>\n",
       "      <td>0.0185646</td>\n",
       "      <td>0.000574465</td>\n",
       "      <td>0.004053</td>\n",
       "      <td>0.000226971</td>\n",
       "      <td>0.0038321</td>\n",
       "      <td>0.0124038</td>\n",
       "      <td>0.00227509</td>\n",
       "      <td>0.0856138</td>\n",
       "      <td>0.0059898</td>\n",
       "      <td>0.0026417</td>\n",
       "      <td>0.000129325</td>\n",
       "      <td>0.0362188</td>\n",
       "      <td>0.00121623</td>\n",
       "      <td>3.37266e-05</td>\n",
       "      <td>0.0239614</td>\n",
       "      <td>0.00978395</td>\n",
       "      <td>0.00381497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.48919e-05</td>\n",
       "      <td>2.31249e-05</td>\n",
       "      <td>0.000202785</td>\n",
       "      <td>1.63342e-07</td>\n",
       "      <td>8.26145e-06</td>\n",
       "      <td>0.10912</td>\n",
       "      <td>8.14359e-06</td>\n",
       "      <td>0.000471771</td>\n",
       "      <td>5.30261e-06</td>\n",
       "      <td>0.889522</td>\n",
       "      <td>2.40785e-05</td>\n",
       "      <td>0.000184653</td>\n",
       "      <td>0.00018651</td>\n",
       "      <td>1.16147e-06</td>\n",
       "      <td>4.15998e-07</td>\n",
       "      <td>8.72374e-05</td>\n",
       "      <td>1.8724e-05</td>\n",
       "      <td>2.746e-06</td>\n",
       "      <td>5.79916e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0014666</td>\n",
       "      <td>0.000106759</td>\n",
       "      <td>3.04273e-05</td>\n",
       "      <td>2.22436e-05</td>\n",
       "      <td>0.000194526</td>\n",
       "      <td>3.09678e-05</td>\n",
       "      <td>8.60485e-05</td>\n",
       "      <td>0.0013385</td>\n",
       "      <td>2.27349e-05</td>\n",
       "      <td>0.996469</td>\n",
       "      <td>1.7496e-05</td>\n",
       "      <td>2.93614e-05</td>\n",
       "      <td>4.91412e-05</td>\n",
       "      <td>9.46555e-06</td>\n",
       "      <td>2.37934e-06</td>\n",
       "      <td>2.15814e-05</td>\n",
       "      <td>1.71409e-05</td>\n",
       "      <td>7.73404e-06</td>\n",
       "      <td>7.78797e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0673959</td>\n",
       "      <td>0.000113092</td>\n",
       "      <td>0.00371484</td>\n",
       "      <td>3.35084e-05</td>\n",
       "      <td>0.000685924</td>\n",
       "      <td>7.06593e-05</td>\n",
       "      <td>0.184815</td>\n",
       "      <td>0.000716093</td>\n",
       "      <td>0.0247319</td>\n",
       "      <td>0.000611504</td>\n",
       "      <td>0.0190444</td>\n",
       "      <td>0.156417</td>\n",
       "      <td>0.0320471</td>\n",
       "      <td>0.0458184</td>\n",
       "      <td>0.000870905</td>\n",
       "      <td>9.7643e-06</td>\n",
       "      <td>0.000411843</td>\n",
       "      <td>0.00335388</td>\n",
       "      <td>0.459139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000354028</td>\n",
       "      <td>0.000608471</td>\n",
       "      <td>8.71532e-05</td>\n",
       "      <td>1.95913e-06</td>\n",
       "      <td>2.76006e-05</td>\n",
       "      <td>4.04659e-06</td>\n",
       "      <td>1.55278e-05</td>\n",
       "      <td>1.34479e-05</td>\n",
       "      <td>0.00121565</td>\n",
       "      <td>0.000143187</td>\n",
       "      <td>4.99837e-05</td>\n",
       "      <td>3.62057e-05</td>\n",
       "      <td>0.000186148</td>\n",
       "      <td>0.00165968</td>\n",
       "      <td>0.000256742</td>\n",
       "      <td>0.000359101</td>\n",
       "      <td>2.04193e-05</td>\n",
       "      <td>0.979086</td>\n",
       "      <td>0.0158745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0134909</td>\n",
       "      <td>0.0399564</td>\n",
       "      <td>0.0054856</td>\n",
       "      <td>0.00638989</td>\n",
       "      <td>0.00467861</td>\n",
       "      <td>0.00028018</td>\n",
       "      <td>0.0359807</td>\n",
       "      <td>0.00700673</td>\n",
       "      <td>0.104248</td>\n",
       "      <td>0.00295146</td>\n",
       "      <td>0.00937905</td>\n",
       "      <td>0.0507639</td>\n",
       "      <td>0.0219055</td>\n",
       "      <td>0.0137073</td>\n",
       "      <td>0.00862758</td>\n",
       "      <td>7.04401e-05</td>\n",
       "      <td>0.0191357</td>\n",
       "      <td>0.0380617</td>\n",
       "      <td>0.617881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.33394e-06</td>\n",
       "      <td>3.97908e-06</td>\n",
       "      <td>3.45291e-06</td>\n",
       "      <td>2.69874e-07</td>\n",
       "      <td>1.45341e-06</td>\n",
       "      <td>3.53258e-05</td>\n",
       "      <td>5.56732e-06</td>\n",
       "      <td>4.35078e-08</td>\n",
       "      <td>0.999942</td>\n",
       "      <td>9.49475e-08</td>\n",
       "      <td>1.65568e-06</td>\n",
       "      <td>3.08469e-06</td>\n",
       "      <td>2.74073e-07</td>\n",
       "      <td>1.67233e-07</td>\n",
       "      <td>4.57625e-07</td>\n",
       "      <td>5.31762e-07</td>\n",
       "      <td>3.29898e-07</td>\n",
       "      <td>9.79328e-10</td>\n",
       "      <td>1.11036e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00985081</td>\n",
       "      <td>0.00017597</td>\n",
       "      <td>0.000283023</td>\n",
       "      <td>0.683768</td>\n",
       "      <td>0.000752208</td>\n",
       "      <td>0.00132947</td>\n",
       "      <td>0.0186075</td>\n",
       "      <td>0.00581888</td>\n",
       "      <td>0.000263542</td>\n",
       "      <td>0.00474559</td>\n",
       "      <td>4.40061e-05</td>\n",
       "      <td>0.0878157</td>\n",
       "      <td>0.015197</td>\n",
       "      <td>0.001573</td>\n",
       "      <td>0.00470924</td>\n",
       "      <td>0.000100012</td>\n",
       "      <td>4.14703e-05</td>\n",
       "      <td>1.77895e-05</td>\n",
       "      <td>0.164907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00102038</td>\n",
       "      <td>7.58863e-05</td>\n",
       "      <td>0.000195554</td>\n",
       "      <td>4.80269e-05</td>\n",
       "      <td>9.6694e-05</td>\n",
       "      <td>1.09512e-05</td>\n",
       "      <td>0.000543621</td>\n",
       "      <td>0.0012241</td>\n",
       "      <td>0.00361258</td>\n",
       "      <td>0.00015123</td>\n",
       "      <td>9.30802e-05</td>\n",
       "      <td>0.00073933</td>\n",
       "      <td>0.000286019</td>\n",
       "      <td>0.208645</td>\n",
       "      <td>0.000323229</td>\n",
       "      <td>1.02837e-05</td>\n",
       "      <td>0.781701</td>\n",
       "      <td>0.000216162</td>\n",
       "      <td>0.00100688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.421142</td>\n",
       "      <td>5.61804e-05</td>\n",
       "      <td>0.030726</td>\n",
       "      <td>0.000465937</td>\n",
       "      <td>0.00198172</td>\n",
       "      <td>0.0626795</td>\n",
       "      <td>0.107584</td>\n",
       "      <td>0.0144016</td>\n",
       "      <td>0.109521</td>\n",
       "      <td>0.00303408</td>\n",
       "      <td>0.0712067</td>\n",
       "      <td>0.0547523</td>\n",
       "      <td>0.00508214</td>\n",
       "      <td>0.00181246</td>\n",
       "      <td>0.00383306</td>\n",
       "      <td>4.71164e-05</td>\n",
       "      <td>0.000236426</td>\n",
       "      <td>0.0122447</td>\n",
       "      <td>0.099193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.00210349</td>\n",
       "      <td>7.44414e-07</td>\n",
       "      <td>1.15846e-05</td>\n",
       "      <td>3.82187e-05</td>\n",
       "      <td>1.43915e-05</td>\n",
       "      <td>6.39374e-05</td>\n",
       "      <td>6.17736e-06</td>\n",
       "      <td>7.52984e-06</td>\n",
       "      <td>1.23052e-06</td>\n",
       "      <td>2.31627e-05</td>\n",
       "      <td>4.06073e-05</td>\n",
       "      <td>0.000434232</td>\n",
       "      <td>0.996855</td>\n",
       "      <td>2.34857e-05</td>\n",
       "      <td>5.37127e-07</td>\n",
       "      <td>5.80306e-06</td>\n",
       "      <td>1.46535e-05</td>\n",
       "      <td>0.000175646</td>\n",
       "      <td>0.000179796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.00915159</td>\n",
       "      <td>0.00783279</td>\n",
       "      <td>1.67576e-06</td>\n",
       "      <td>7.1805e-07</td>\n",
       "      <td>3.12129e-05</td>\n",
       "      <td>0.000128984</td>\n",
       "      <td>7.77209e-05</td>\n",
       "      <td>0.00358537</td>\n",
       "      <td>4.40863e-06</td>\n",
       "      <td>0.977246</td>\n",
       "      <td>4.59887e-05</td>\n",
       "      <td>0.000178509</td>\n",
       "      <td>0.000993204</td>\n",
       "      <td>0.000425074</td>\n",
       "      <td>1.2448e-05</td>\n",
       "      <td>6.63985e-05</td>\n",
       "      <td>4.01371e-05</td>\n",
       "      <td>7.53856e-05</td>\n",
       "      <td>0.000102749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0483168</td>\n",
       "      <td>0.00482423</td>\n",
       "      <td>0.000653438</td>\n",
       "      <td>0.000184868</td>\n",
       "      <td>0.00118365</td>\n",
       "      <td>0.00489552</td>\n",
       "      <td>0.00127151</td>\n",
       "      <td>0.00394004</td>\n",
       "      <td>0.000183467</td>\n",
       "      <td>0.664278</td>\n",
       "      <td>0.00127097</td>\n",
       "      <td>0.00126869</td>\n",
       "      <td>0.201189</td>\n",
       "      <td>0.0373893</td>\n",
       "      <td>0.000619593</td>\n",
       "      <td>0.00334107</td>\n",
       "      <td>0.00553634</td>\n",
       "      <td>0.0124714</td>\n",
       "      <td>0.00718234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.00316796</td>\n",
       "      <td>1.51752e-05</td>\n",
       "      <td>4.10857e-05</td>\n",
       "      <td>0.000151936</td>\n",
       "      <td>4.56394e-05</td>\n",
       "      <td>7.48119e-06</td>\n",
       "      <td>0.000165341</td>\n",
       "      <td>0.00232355</td>\n",
       "      <td>7.12966e-05</td>\n",
       "      <td>0.00140398</td>\n",
       "      <td>0.000155878</td>\n",
       "      <td>0.000517777</td>\n",
       "      <td>0.0003786</td>\n",
       "      <td>0.979199</td>\n",
       "      <td>0.00142464</td>\n",
       "      <td>0.000576421</td>\n",
       "      <td>0.00869864</td>\n",
       "      <td>0.000175397</td>\n",
       "      <td>0.00148026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.00807058</td>\n",
       "      <td>6.70895e-05</td>\n",
       "      <td>0.000690838</td>\n",
       "      <td>3.3289e-05</td>\n",
       "      <td>0.0175</td>\n",
       "      <td>6.20337e-06</td>\n",
       "      <td>0.000658452</td>\n",
       "      <td>0.00119209</td>\n",
       "      <td>0.00023702</td>\n",
       "      <td>0.967911</td>\n",
       "      <td>0.000259943</td>\n",
       "      <td>7.14571e-06</td>\n",
       "      <td>0.000823718</td>\n",
       "      <td>0.00135199</td>\n",
       "      <td>2.8236e-05</td>\n",
       "      <td>1.86421e-05</td>\n",
       "      <td>1.70596e-05</td>\n",
       "      <td>0.000222627</td>\n",
       "      <td>0.00090435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000136551</td>\n",
       "      <td>6.94133e-06</td>\n",
       "      <td>0.000375048</td>\n",
       "      <td>2.63349e-07</td>\n",
       "      <td>0.000128928</td>\n",
       "      <td>6.83934e-06</td>\n",
       "      <td>7.83433e-05</td>\n",
       "      <td>8.34597e-06</td>\n",
       "      <td>0.999003</td>\n",
       "      <td>8.89361e-06</td>\n",
       "      <td>8.65836e-06</td>\n",
       "      <td>0.000119524</td>\n",
       "      <td>7.20069e-05</td>\n",
       "      <td>2.50557e-06</td>\n",
       "      <td>1.54169e-07</td>\n",
       "      <td>7.98029e-06</td>\n",
       "      <td>1.97872e-05</td>\n",
       "      <td>4.993e-07</td>\n",
       "      <td>1.53357e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.459499</td>\n",
       "      <td>0.000835216</td>\n",
       "      <td>0.000194947</td>\n",
       "      <td>0.00715274</td>\n",
       "      <td>0.00837022</td>\n",
       "      <td>0.000832582</td>\n",
       "      <td>0.000259338</td>\n",
       "      <td>0.422135</td>\n",
       "      <td>0.000498094</td>\n",
       "      <td>0.00663255</td>\n",
       "      <td>0.000327445</td>\n",
       "      <td>0.00515536</td>\n",
       "      <td>0.0713012</td>\n",
       "      <td>0.00499426</td>\n",
       "      <td>6.25562e-05</td>\n",
       "      <td>0.000235867</td>\n",
       "      <td>0.000365782</td>\n",
       "      <td>0.00606732</td>\n",
       "      <td>0.00508052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0357584</td>\n",
       "      <td>0.609898</td>\n",
       "      <td>0.00529043</td>\n",
       "      <td>0.0347792</td>\n",
       "      <td>0.00487074</td>\n",
       "      <td>0.000309201</td>\n",
       "      <td>0.00447058</td>\n",
       "      <td>0.021606</td>\n",
       "      <td>0.000778857</td>\n",
       "      <td>0.00216915</td>\n",
       "      <td>0.00110259</td>\n",
       "      <td>0.00476823</td>\n",
       "      <td>0.00635844</td>\n",
       "      <td>0.0353753</td>\n",
       "      <td>0.000603963</td>\n",
       "      <td>0.00161581</td>\n",
       "      <td>0.00359976</td>\n",
       "      <td>0.17978</td>\n",
       "      <td>0.0468655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.327348</td>\n",
       "      <td>0.00341869</td>\n",
       "      <td>0.00042174</td>\n",
       "      <td>0.00117918</td>\n",
       "      <td>0.00315594</td>\n",
       "      <td>2.741e-05</td>\n",
       "      <td>0.00552979</td>\n",
       "      <td>0.120705</td>\n",
       "      <td>0.000262624</td>\n",
       "      <td>0.0706039</td>\n",
       "      <td>0.000416691</td>\n",
       "      <td>0.0223186</td>\n",
       "      <td>0.40513</td>\n",
       "      <td>0.000800969</td>\n",
       "      <td>0.00339516</td>\n",
       "      <td>0.0115276</td>\n",
       "      <td>0.00213462</td>\n",
       "      <td>0.00173542</td>\n",
       "      <td>0.0198887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.029729</td>\n",
       "      <td>0.000410013</td>\n",
       "      <td>0.000575811</td>\n",
       "      <td>4.11418e-05</td>\n",
       "      <td>0.00101643</td>\n",
       "      <td>0.00146932</td>\n",
       "      <td>0.0560131</td>\n",
       "      <td>0.00995776</td>\n",
       "      <td>0.794888</td>\n",
       "      <td>0.0184709</td>\n",
       "      <td>0.000357167</td>\n",
       "      <td>0.000485464</td>\n",
       "      <td>0.00244877</td>\n",
       "      <td>0.00184609</td>\n",
       "      <td>0.0215438</td>\n",
       "      <td>1.18243e-05</td>\n",
       "      <td>0.0237253</td>\n",
       "      <td>0.000446713</td>\n",
       "      <td>0.0365636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.00138048</td>\n",
       "      <td>0.000333689</td>\n",
       "      <td>0.000594414</td>\n",
       "      <td>0.453065</td>\n",
       "      <td>0.000142077</td>\n",
       "      <td>1.86598e-05</td>\n",
       "      <td>0.260151</td>\n",
       "      <td>0.000143211</td>\n",
       "      <td>0.00331241</td>\n",
       "      <td>0.00041573</td>\n",
       "      <td>0.00805845</td>\n",
       "      <td>0.0886101</td>\n",
       "      <td>0.0203095</td>\n",
       "      <td>0.152873</td>\n",
       "      <td>0.00307336</td>\n",
       "      <td>3.09563e-05</td>\n",
       "      <td>6.95673e-05</td>\n",
       "      <td>3.12848e-05</td>\n",
       "      <td>0.0073875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.00196733</td>\n",
       "      <td>9.96504e-05</td>\n",
       "      <td>2.43339e-05</td>\n",
       "      <td>0.00274834</td>\n",
       "      <td>2.61035e-05</td>\n",
       "      <td>0.0012226</td>\n",
       "      <td>0.000471775</td>\n",
       "      <td>0.0463422</td>\n",
       "      <td>2.48006e-05</td>\n",
       "      <td>0.00412433</td>\n",
       "      <td>1.01023e-05</td>\n",
       "      <td>0.000162386</td>\n",
       "      <td>0.00143425</td>\n",
       "      <td>0.920396</td>\n",
       "      <td>0.0022569</td>\n",
       "      <td>0.00367671</td>\n",
       "      <td>0.0146426</td>\n",
       "      <td>1.30112e-05</td>\n",
       "      <td>0.000356258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0451425</td>\n",
       "      <td>0.000263728</td>\n",
       "      <td>3.03899e-05</td>\n",
       "      <td>3.2559e-05</td>\n",
       "      <td>0.00114426</td>\n",
       "      <td>0.000291917</td>\n",
       "      <td>0.000769414</td>\n",
       "      <td>0.263942</td>\n",
       "      <td>0.000113178</td>\n",
       "      <td>0.681098</td>\n",
       "      <td>0.000415603</td>\n",
       "      <td>0.000428428</td>\n",
       "      <td>0.000582853</td>\n",
       "      <td>0.000486962</td>\n",
       "      <td>5.15944e-05</td>\n",
       "      <td>0.00133491</td>\n",
       "      <td>8.23866e-05</td>\n",
       "      <td>8.98581e-05</td>\n",
       "      <td>0.0036999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.00294374</td>\n",
       "      <td>6.69573e-05</td>\n",
       "      <td>7.46923e-05</td>\n",
       "      <td>5.3471e-05</td>\n",
       "      <td>0.00499978</td>\n",
       "      <td>9.58905e-05</td>\n",
       "      <td>3.80159e-05</td>\n",
       "      <td>0.962819</td>\n",
       "      <td>2.21051e-05</td>\n",
       "      <td>0.0256613</td>\n",
       "      <td>1.01601e-05</td>\n",
       "      <td>4.74599e-05</td>\n",
       "      <td>0.00108154</td>\n",
       "      <td>0.00061431</td>\n",
       "      <td>9.63466e-05</td>\n",
       "      <td>0.0011993</td>\n",
       "      <td>4.37645e-05</td>\n",
       "      <td>3.92444e-05</td>\n",
       "      <td>9.34594e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102247</th>\n",
       "      <td>0.000303</td>\n",
       "      <td>8.45208e-06</td>\n",
       "      <td>0.000135488</td>\n",
       "      <td>4.21968e-06</td>\n",
       "      <td>3.18105e-05</td>\n",
       "      <td>0.00024966</td>\n",
       "      <td>2.84787e-05</td>\n",
       "      <td>1.27315e-05</td>\n",
       "      <td>0.995521</td>\n",
       "      <td>3.98284e-05</td>\n",
       "      <td>0.000644456</td>\n",
       "      <td>0.000355095</td>\n",
       "      <td>0.00102517</td>\n",
       "      <td>6.0194e-05</td>\n",
       "      <td>0.000778199</td>\n",
       "      <td>1.37145e-05</td>\n",
       "      <td>0.000121591</td>\n",
       "      <td>3.47139e-06</td>\n",
       "      <td>0.00066306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102248</th>\n",
       "      <td>8.63423e-05</td>\n",
       "      <td>2.37721e-05</td>\n",
       "      <td>6.42473e-06</td>\n",
       "      <td>1.35912e-06</td>\n",
       "      <td>1.55606e-06</td>\n",
       "      <td>1.0858e-07</td>\n",
       "      <td>2.04257e-06</td>\n",
       "      <td>1.80428e-06</td>\n",
       "      <td>1.77761e-06</td>\n",
       "      <td>1.04665e-06</td>\n",
       "      <td>4.76815e-05</td>\n",
       "      <td>1.93328e-05</td>\n",
       "      <td>1.16854e-05</td>\n",
       "      <td>1.38187e-05</td>\n",
       "      <td>7.97265e-06</td>\n",
       "      <td>5.35638e-06</td>\n",
       "      <td>5.25012e-06</td>\n",
       "      <td>0.990106</td>\n",
       "      <td>0.00965717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102249</th>\n",
       "      <td>0.123378</td>\n",
       "      <td>0.000485822</td>\n",
       "      <td>0.000971128</td>\n",
       "      <td>0.00382188</td>\n",
       "      <td>0.00321262</td>\n",
       "      <td>0.000435243</td>\n",
       "      <td>0.0018487</td>\n",
       "      <td>0.069267</td>\n",
       "      <td>0.000326047</td>\n",
       "      <td>0.0110576</td>\n",
       "      <td>0.0399957</td>\n",
       "      <td>0.00373253</td>\n",
       "      <td>0.733918</td>\n",
       "      <td>0.00197786</td>\n",
       "      <td>0.000228845</td>\n",
       "      <td>0.000350132</td>\n",
       "      <td>0.00161045</td>\n",
       "      <td>0.000921419</td>\n",
       "      <td>0.00246071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102250</th>\n",
       "      <td>6.71728e-06</td>\n",
       "      <td>2.71476e-06</td>\n",
       "      <td>5.74645e-05</td>\n",
       "      <td>3.20606e-07</td>\n",
       "      <td>1.09837e-05</td>\n",
       "      <td>1.47502e-06</td>\n",
       "      <td>0.000421547</td>\n",
       "      <td>6.14074e-07</td>\n",
       "      <td>0.999402</td>\n",
       "      <td>5.56328e-07</td>\n",
       "      <td>3.9554e-06</td>\n",
       "      <td>1.77641e-06</td>\n",
       "      <td>8.53685e-06</td>\n",
       "      <td>5.0286e-06</td>\n",
       "      <td>4.1558e-05</td>\n",
       "      <td>1.71159e-06</td>\n",
       "      <td>6.97629e-07</td>\n",
       "      <td>1.59663e-06</td>\n",
       "      <td>3.03657e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102251</th>\n",
       "      <td>0.0760799</td>\n",
       "      <td>0.0389723</td>\n",
       "      <td>0.00696684</td>\n",
       "      <td>0.0051315</td>\n",
       "      <td>0.00146292</td>\n",
       "      <td>0.000998421</td>\n",
       "      <td>0.000896374</td>\n",
       "      <td>0.0689576</td>\n",
       "      <td>0.000482556</td>\n",
       "      <td>0.00171457</td>\n",
       "      <td>0.000684186</td>\n",
       "      <td>0.0304214</td>\n",
       "      <td>0.0692814</td>\n",
       "      <td>0.000508483</td>\n",
       "      <td>9.76958e-05</td>\n",
       "      <td>0.00202266</td>\n",
       "      <td>0.0018141</td>\n",
       "      <td>0.0178637</td>\n",
       "      <td>0.675644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102252</th>\n",
       "      <td>0.00391892</td>\n",
       "      <td>6.6257e-06</td>\n",
       "      <td>2.31133e-05</td>\n",
       "      <td>0.000150983</td>\n",
       "      <td>0.000131365</td>\n",
       "      <td>2.26388e-05</td>\n",
       "      <td>0.000121658</td>\n",
       "      <td>6.21453e-05</td>\n",
       "      <td>2.36002e-05</td>\n",
       "      <td>0.00348796</td>\n",
       "      <td>0.000151401</td>\n",
       "      <td>1.04186e-05</td>\n",
       "      <td>0.000288602</td>\n",
       "      <td>0.990504</td>\n",
       "      <td>0.000812819</td>\n",
       "      <td>0.000144361</td>\n",
       "      <td>8.74027e-06</td>\n",
       "      <td>5.1715e-05</td>\n",
       "      <td>7.84772e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102253</th>\n",
       "      <td>0.0466006</td>\n",
       "      <td>0.00134623</td>\n",
       "      <td>0.115866</td>\n",
       "      <td>0.0008188</td>\n",
       "      <td>0.000904528</td>\n",
       "      <td>0.002782</td>\n",
       "      <td>0.205775</td>\n",
       "      <td>0.00650209</td>\n",
       "      <td>0.0274959</td>\n",
       "      <td>0.00341001</td>\n",
       "      <td>0.207298</td>\n",
       "      <td>0.268468</td>\n",
       "      <td>0.0383202</td>\n",
       "      <td>0.00356205</td>\n",
       "      <td>0.0075336</td>\n",
       "      <td>0.000190143</td>\n",
       "      <td>0.0134913</td>\n",
       "      <td>0.00573906</td>\n",
       "      <td>0.0438972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102254</th>\n",
       "      <td>8.04842e-05</td>\n",
       "      <td>5.67562e-09</td>\n",
       "      <td>0.000269579</td>\n",
       "      <td>3.12708e-07</td>\n",
       "      <td>1.59972e-06</td>\n",
       "      <td>7.09532e-07</td>\n",
       "      <td>2.24525e-06</td>\n",
       "      <td>1.03133e-06</td>\n",
       "      <td>1.50067e-05</td>\n",
       "      <td>1.05379e-06</td>\n",
       "      <td>0.999402</td>\n",
       "      <td>1.01868e-05</td>\n",
       "      <td>2.04447e-05</td>\n",
       "      <td>5.17691e-05</td>\n",
       "      <td>9.34977e-05</td>\n",
       "      <td>2.40233e-06</td>\n",
       "      <td>1.325e-06</td>\n",
       "      <td>3.29056e-06</td>\n",
       "      <td>4.26152e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102255</th>\n",
       "      <td>0.0394911</td>\n",
       "      <td>8.15297e-05</td>\n",
       "      <td>0.00748279</td>\n",
       "      <td>0.000354984</td>\n",
       "      <td>0.000174055</td>\n",
       "      <td>0.00806223</td>\n",
       "      <td>0.0790045</td>\n",
       "      <td>0.0125874</td>\n",
       "      <td>0.0473351</td>\n",
       "      <td>0.0102037</td>\n",
       "      <td>0.620014</td>\n",
       "      <td>0.0469633</td>\n",
       "      <td>0.054466</td>\n",
       "      <td>0.0258708</td>\n",
       "      <td>0.011341</td>\n",
       "      <td>0.000170519</td>\n",
       "      <td>0.00235939</td>\n",
       "      <td>0.00230182</td>\n",
       "      <td>0.0317354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102256</th>\n",
       "      <td>0.0157134</td>\n",
       "      <td>0.0126582</td>\n",
       "      <td>0.00526806</td>\n",
       "      <td>0.000777837</td>\n",
       "      <td>0.000626021</td>\n",
       "      <td>0.0245747</td>\n",
       "      <td>0.00167468</td>\n",
       "      <td>0.00349117</td>\n",
       "      <td>0.000668989</td>\n",
       "      <td>0.00651027</td>\n",
       "      <td>0.000833621</td>\n",
       "      <td>0.00276006</td>\n",
       "      <td>0.000527099</td>\n",
       "      <td>0.000347862</td>\n",
       "      <td>0.000147049</td>\n",
       "      <td>0.00206062</td>\n",
       "      <td>0.00283011</td>\n",
       "      <td>0.904256</td>\n",
       "      <td>0.0142745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102257</th>\n",
       "      <td>0.000244773</td>\n",
       "      <td>4.15952e-05</td>\n",
       "      <td>3.4641e-05</td>\n",
       "      <td>0.99757</td>\n",
       "      <td>4.05204e-05</td>\n",
       "      <td>0.000112756</td>\n",
       "      <td>0.000109296</td>\n",
       "      <td>2.81369e-05</td>\n",
       "      <td>2.93075e-06</td>\n",
       "      <td>0.00024516</td>\n",
       "      <td>6.36362e-05</td>\n",
       "      <td>7.96587e-05</td>\n",
       "      <td>0.00019582</td>\n",
       "      <td>0.00112467</td>\n",
       "      <td>5.72402e-05</td>\n",
       "      <td>8.58875e-07</td>\n",
       "      <td>1.43095e-05</td>\n",
       "      <td>6.45764e-06</td>\n",
       "      <td>2.75215e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102258</th>\n",
       "      <td>0.00454429</td>\n",
       "      <td>0.000108705</td>\n",
       "      <td>0.405941</td>\n",
       "      <td>0.000788294</td>\n",
       "      <td>4.56615e-05</td>\n",
       "      <td>0.00026906</td>\n",
       "      <td>0.0366898</td>\n",
       "      <td>0.000622506</td>\n",
       "      <td>0.00630952</td>\n",
       "      <td>0.000473002</td>\n",
       "      <td>0.18945</td>\n",
       "      <td>0.0843363</td>\n",
       "      <td>0.0292849</td>\n",
       "      <td>0.0055727</td>\n",
       "      <td>0.21383</td>\n",
       "      <td>0.000112433</td>\n",
       "      <td>0.0165206</td>\n",
       "      <td>3.66765e-05</td>\n",
       "      <td>0.00506464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102259</th>\n",
       "      <td>0.00014901</td>\n",
       "      <td>7.94139e-06</td>\n",
       "      <td>0.00739143</td>\n",
       "      <td>4.38855e-06</td>\n",
       "      <td>9.21629e-05</td>\n",
       "      <td>0.990179</td>\n",
       "      <td>3.90698e-06</td>\n",
       "      <td>7.34594e-05</td>\n",
       "      <td>2.4411e-06</td>\n",
       "      <td>0.000622014</td>\n",
       "      <td>5.57196e-05</td>\n",
       "      <td>0.0010009</td>\n",
       "      <td>0.000111806</td>\n",
       "      <td>7.36255e-07</td>\n",
       "      <td>5.20817e-06</td>\n",
       "      <td>6.66596e-05</td>\n",
       "      <td>1.19284e-05</td>\n",
       "      <td>0.000203472</td>\n",
       "      <td>1.73422e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102260</th>\n",
       "      <td>0.00113527</td>\n",
       "      <td>1.39972e-05</td>\n",
       "      <td>0.000141728</td>\n",
       "      <td>2.13555e-05</td>\n",
       "      <td>2.66655e-05</td>\n",
       "      <td>4.58749e-06</td>\n",
       "      <td>0.00078526</td>\n",
       "      <td>0.000151412</td>\n",
       "      <td>0.974311</td>\n",
       "      <td>7.03018e-05</td>\n",
       "      <td>0.000646139</td>\n",
       "      <td>0.00111378</td>\n",
       "      <td>0.000151593</td>\n",
       "      <td>0.00368962</td>\n",
       "      <td>0.000144254</td>\n",
       "      <td>2.45196e-05</td>\n",
       "      <td>8.8429e-05</td>\n",
       "      <td>0.00567503</td>\n",
       "      <td>0.0118051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102261</th>\n",
       "      <td>0.683837</td>\n",
       "      <td>0.000141723</td>\n",
       "      <td>0.00194693</td>\n",
       "      <td>0.000188045</td>\n",
       "      <td>0.00199645</td>\n",
       "      <td>6.5436e-05</td>\n",
       "      <td>0.000797848</td>\n",
       "      <td>0.243308</td>\n",
       "      <td>0.000854508</td>\n",
       "      <td>0.00609167</td>\n",
       "      <td>0.000531022</td>\n",
       "      <td>0.0476625</td>\n",
       "      <td>0.00824738</td>\n",
       "      <td>0.000245635</td>\n",
       "      <td>0.000111529</td>\n",
       "      <td>0.000484734</td>\n",
       "      <td>7.3405e-05</td>\n",
       "      <td>0.00050721</td>\n",
       "      <td>0.00290828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102262</th>\n",
       "      <td>0.000109944</td>\n",
       "      <td>3.332e-05</td>\n",
       "      <td>3.0468e-05</td>\n",
       "      <td>1.7207e-06</td>\n",
       "      <td>2.47155e-06</td>\n",
       "      <td>5.22633e-07</td>\n",
       "      <td>3.6819e-06</td>\n",
       "      <td>1.72507e-06</td>\n",
       "      <td>0.000168352</td>\n",
       "      <td>3.11476e-06</td>\n",
       "      <td>8.14792e-05</td>\n",
       "      <td>0.000176247</td>\n",
       "      <td>3.10884e-05</td>\n",
       "      <td>3.86714e-05</td>\n",
       "      <td>7.58127e-05</td>\n",
       "      <td>4.48334e-05</td>\n",
       "      <td>6.80507e-06</td>\n",
       "      <td>0.988239</td>\n",
       "      <td>0.0109505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102263</th>\n",
       "      <td>6.28283e-06</td>\n",
       "      <td>4.20199e-06</td>\n",
       "      <td>0.000184396</td>\n",
       "      <td>5.73613e-06</td>\n",
       "      <td>3.09138e-06</td>\n",
       "      <td>0.999325</td>\n",
       "      <td>8.66481e-06</td>\n",
       "      <td>1.08899e-05</td>\n",
       "      <td>7.6289e-07</td>\n",
       "      <td>0.000275288</td>\n",
       "      <td>1.0036e-06</td>\n",
       "      <td>8.30788e-05</td>\n",
       "      <td>1.7961e-05</td>\n",
       "      <td>3.85241e-06</td>\n",
       "      <td>8.13799e-07</td>\n",
       "      <td>1.1514e-07</td>\n",
       "      <td>3.67521e-07</td>\n",
       "      <td>5.96271e-05</td>\n",
       "      <td>8.49487e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102264</th>\n",
       "      <td>0.0487496</td>\n",
       "      <td>0.000624602</td>\n",
       "      <td>0.000641267</td>\n",
       "      <td>0.0125842</td>\n",
       "      <td>0.0104204</td>\n",
       "      <td>0.000686893</td>\n",
       "      <td>0.0134876</td>\n",
       "      <td>0.614656</td>\n",
       "      <td>0.007409</td>\n",
       "      <td>0.00739651</td>\n",
       "      <td>0.0100568</td>\n",
       "      <td>0.0328516</td>\n",
       "      <td>0.177786</td>\n",
       "      <td>0.0185808</td>\n",
       "      <td>0.00826856</td>\n",
       "      <td>0.00114798</td>\n",
       "      <td>0.00587539</td>\n",
       "      <td>0.0129456</td>\n",
       "      <td>0.0158311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102265</th>\n",
       "      <td>0.0385674</td>\n",
       "      <td>4.72379e-05</td>\n",
       "      <td>0.0028942</td>\n",
       "      <td>0.00334549</td>\n",
       "      <td>9.08212e-05</td>\n",
       "      <td>0.000315388</td>\n",
       "      <td>1.62542e-05</td>\n",
       "      <td>0.0033741</td>\n",
       "      <td>5.95641e-05</td>\n",
       "      <td>0.000593056</td>\n",
       "      <td>0.00085696</td>\n",
       "      <td>0.000834653</td>\n",
       "      <td>0.0112368</td>\n",
       "      <td>8.53575e-05</td>\n",
       "      <td>0.000203825</td>\n",
       "      <td>0.932603</td>\n",
       "      <td>4.14239e-05</td>\n",
       "      <td>0.00142955</td>\n",
       "      <td>0.00340517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102266</th>\n",
       "      <td>0.0014749</td>\n",
       "      <td>9.83518e-05</td>\n",
       "      <td>1.00349e-05</td>\n",
       "      <td>0.000135424</td>\n",
       "      <td>1.71209e-05</td>\n",
       "      <td>1.08435e-05</td>\n",
       "      <td>1.94668e-05</td>\n",
       "      <td>8.97833e-05</td>\n",
       "      <td>3.29875e-05</td>\n",
       "      <td>0.000138403</td>\n",
       "      <td>8.17569e-05</td>\n",
       "      <td>0.000316201</td>\n",
       "      <td>6.19415e-05</td>\n",
       "      <td>8.10838e-05</td>\n",
       "      <td>3.62379e-06</td>\n",
       "      <td>0.000260635</td>\n",
       "      <td>1.26171e-05</td>\n",
       "      <td>0.994966</td>\n",
       "      <td>0.00218903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102267</th>\n",
       "      <td>0.0539901</td>\n",
       "      <td>0.000125713</td>\n",
       "      <td>8.95791e-05</td>\n",
       "      <td>0.000274375</td>\n",
       "      <td>0.00601792</td>\n",
       "      <td>0.000122236</td>\n",
       "      <td>0.000938989</td>\n",
       "      <td>0.798291</td>\n",
       "      <td>0.000528398</td>\n",
       "      <td>0.0464337</td>\n",
       "      <td>0.00226854</td>\n",
       "      <td>0.000251753</td>\n",
       "      <td>0.0636187</td>\n",
       "      <td>0.0129121</td>\n",
       "      <td>0.00434243</td>\n",
       "      <td>0.003249</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.000861517</td>\n",
       "      <td>0.00449328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102268</th>\n",
       "      <td>0.000186087</td>\n",
       "      <td>1.7821e-05</td>\n",
       "      <td>0.585113</td>\n",
       "      <td>1.05851e-05</td>\n",
       "      <td>0.000127854</td>\n",
       "      <td>6.53375e-05</td>\n",
       "      <td>0.00239556</td>\n",
       "      <td>8.80146e-05</td>\n",
       "      <td>0.405289</td>\n",
       "      <td>2.62212e-05</td>\n",
       "      <td>0.000860727</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.000411313</td>\n",
       "      <td>0.000216889</td>\n",
       "      <td>0.00205781</td>\n",
       "      <td>1.89262e-06</td>\n",
       "      <td>7.09371e-05</td>\n",
       "      <td>2.60265e-05</td>\n",
       "      <td>0.000330251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102269</th>\n",
       "      <td>0.0170557</td>\n",
       "      <td>0.00320195</td>\n",
       "      <td>0.119341</td>\n",
       "      <td>0.000600033</td>\n",
       "      <td>3.96352e-05</td>\n",
       "      <td>0.000327016</td>\n",
       "      <td>0.0344254</td>\n",
       "      <td>0.00288996</td>\n",
       "      <td>0.0245626</td>\n",
       "      <td>0.0012475</td>\n",
       "      <td>0.00576923</td>\n",
       "      <td>0.0192896</td>\n",
       "      <td>0.0319256</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.0184327</td>\n",
       "      <td>0.000407894</td>\n",
       "      <td>0.0231501</td>\n",
       "      <td>0.00904987</td>\n",
       "      <td>0.668622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102270</th>\n",
       "      <td>1.11781e-05</td>\n",
       "      <td>5.54276e-06</td>\n",
       "      <td>0.789941</td>\n",
       "      <td>1.7496e-06</td>\n",
       "      <td>1.35134e-05</td>\n",
       "      <td>0.209519</td>\n",
       "      <td>2.27081e-06</td>\n",
       "      <td>0.000121321</td>\n",
       "      <td>9.03761e-06</td>\n",
       "      <td>6.90369e-05</td>\n",
       "      <td>3.79515e-06</td>\n",
       "      <td>0.000235522</td>\n",
       "      <td>2.32743e-06</td>\n",
       "      <td>9.49593e-06</td>\n",
       "      <td>1.35514e-05</td>\n",
       "      <td>1.70875e-05</td>\n",
       "      <td>2.47944e-06</td>\n",
       "      <td>1.01584e-05</td>\n",
       "      <td>1.132e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102271</th>\n",
       "      <td>0.0402091</td>\n",
       "      <td>0.0623237</td>\n",
       "      <td>0.00372502</td>\n",
       "      <td>0.000425641</td>\n",
       "      <td>0.00517096</td>\n",
       "      <td>0.00324587</td>\n",
       "      <td>0.000659475</td>\n",
       "      <td>0.147535</td>\n",
       "      <td>0.00168929</td>\n",
       "      <td>0.0667847</td>\n",
       "      <td>0.000593747</td>\n",
       "      <td>0.00369526</td>\n",
       "      <td>0.650523</td>\n",
       "      <td>0.0044267</td>\n",
       "      <td>0.000840453</td>\n",
       "      <td>0.000534509</td>\n",
       "      <td>0.00129991</td>\n",
       "      <td>0.00179582</td>\n",
       "      <td>0.00452197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102272</th>\n",
       "      <td>0.0333547</td>\n",
       "      <td>0.000564265</td>\n",
       "      <td>0.000786972</td>\n",
       "      <td>0.0847916</td>\n",
       "      <td>0.00146128</td>\n",
       "      <td>0.000754781</td>\n",
       "      <td>0.0688386</td>\n",
       "      <td>0.00829482</td>\n",
       "      <td>0.00109163</td>\n",
       "      <td>0.0432314</td>\n",
       "      <td>0.00996772</td>\n",
       "      <td>0.00485371</td>\n",
       "      <td>0.0261101</td>\n",
       "      <td>0.634576</td>\n",
       "      <td>0.0224724</td>\n",
       "      <td>0.000714572</td>\n",
       "      <td>0.0201461</td>\n",
       "      <td>0.0174259</td>\n",
       "      <td>0.020564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102273</th>\n",
       "      <td>0.121665</td>\n",
       "      <td>0.00466516</td>\n",
       "      <td>0.00090294</td>\n",
       "      <td>0.00042312</td>\n",
       "      <td>0.00228152</td>\n",
       "      <td>0.000200171</td>\n",
       "      <td>0.000774034</td>\n",
       "      <td>0.553873</td>\n",
       "      <td>0.00250684</td>\n",
       "      <td>0.00675544</td>\n",
       "      <td>0.0014338</td>\n",
       "      <td>0.00395336</td>\n",
       "      <td>0.0186877</td>\n",
       "      <td>0.000266956</td>\n",
       "      <td>0.00012494</td>\n",
       "      <td>0.272419</td>\n",
       "      <td>0.00266441</td>\n",
       "      <td>0.0011789</td>\n",
       "      <td>0.00522371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102274</th>\n",
       "      <td>0.0084595</td>\n",
       "      <td>9.58623e-05</td>\n",
       "      <td>0.000863617</td>\n",
       "      <td>0.00368397</td>\n",
       "      <td>0.000231259</td>\n",
       "      <td>0.00105487</td>\n",
       "      <td>0.00768246</td>\n",
       "      <td>0.00170157</td>\n",
       "      <td>0.00387787</td>\n",
       "      <td>0.0012175</td>\n",
       "      <td>0.587107</td>\n",
       "      <td>0.339281</td>\n",
       "      <td>0.00921187</td>\n",
       "      <td>0.000459942</td>\n",
       "      <td>0.00266342</td>\n",
       "      <td>2.64839e-05</td>\n",
       "      <td>0.00029573</td>\n",
       "      <td>0.00134575</td>\n",
       "      <td>0.030741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102275</th>\n",
       "      <td>0.00263039</td>\n",
       "      <td>9.58191e-06</td>\n",
       "      <td>0.000372931</td>\n",
       "      <td>0.923104</td>\n",
       "      <td>0.000161408</td>\n",
       "      <td>0.000550125</td>\n",
       "      <td>0.00205858</td>\n",
       "      <td>0.000948885</td>\n",
       "      <td>0.00236867</td>\n",
       "      <td>0.00160019</td>\n",
       "      <td>0.00903152</td>\n",
       "      <td>0.00350209</td>\n",
       "      <td>0.0248077</td>\n",
       "      <td>0.0263932</td>\n",
       "      <td>0.00043253</td>\n",
       "      <td>3.77664e-05</td>\n",
       "      <td>0.00011263</td>\n",
       "      <td>5.38598e-05</td>\n",
       "      <td>0.0018237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102276</th>\n",
       "      <td>0.00624561</td>\n",
       "      <td>0.277867</td>\n",
       "      <td>0.00164726</td>\n",
       "      <td>4.1025e-05</td>\n",
       "      <td>0.000132991</td>\n",
       "      <td>0.000839733</td>\n",
       "      <td>0.000854707</td>\n",
       "      <td>0.00187035</td>\n",
       "      <td>0.000624258</td>\n",
       "      <td>8.47149e-05</td>\n",
       "      <td>0.699481</td>\n",
       "      <td>0.00279478</td>\n",
       "      <td>0.0015021</td>\n",
       "      <td>3.08612e-06</td>\n",
       "      <td>0.00301185</td>\n",
       "      <td>7.8369e-06</td>\n",
       "      <td>8.86975e-05</td>\n",
       "      <td>0.000610628</td>\n",
       "      <td>0.00229218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102277 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       meta_base_rnn_wrod_v1_300dim_prob0 meta_base_rnn_wrod_v1_300dim_prob1  \\\n",
       "0                                0.843309                         0.00385805   \n",
       "1                               0.0151885                         0.00205575   \n",
       "2                              0.00957131                        0.000227768   \n",
       "3                               0.0123792                         0.00113402   \n",
       "4                              0.00254471                         6.4346e-05   \n",
       "5                               0.0184902                        0.000422167   \n",
       "6                                0.788332                        0.000334602   \n",
       "7                             7.48919e-05                        2.31249e-05   \n",
       "8                               0.0014666                        0.000106759   \n",
       "9                               0.0673959                        0.000113092   \n",
       "10                            0.000354028                        0.000608471   \n",
       "11                              0.0134909                          0.0399564   \n",
       "12                            1.33394e-06                        3.97908e-06   \n",
       "13                             0.00985081                         0.00017597   \n",
       "14                             0.00102038                        7.58863e-05   \n",
       "15                               0.421142                        5.61804e-05   \n",
       "16                             0.00210349                        7.44414e-07   \n",
       "17                             0.00915159                         0.00783279   \n",
       "18                              0.0483168                         0.00482423   \n",
       "19                             0.00316796                        1.51752e-05   \n",
       "20                             0.00807058                        6.70895e-05   \n",
       "21                            0.000136551                        6.94133e-06   \n",
       "22                               0.459499                        0.000835216   \n",
       "23                              0.0357584                           0.609898   \n",
       "24                               0.327348                         0.00341869   \n",
       "25                               0.029729                        0.000410013   \n",
       "26                             0.00138048                        0.000333689   \n",
       "27                             0.00196733                        9.96504e-05   \n",
       "28                              0.0451425                        0.000263728   \n",
       "29                             0.00294374                        6.69573e-05   \n",
       "...                                   ...                                ...   \n",
       "102247                           0.000303                        8.45208e-06   \n",
       "102248                        8.63423e-05                        2.37721e-05   \n",
       "102249                           0.123378                        0.000485822   \n",
       "102250                        6.71728e-06                        2.71476e-06   \n",
       "102251                          0.0760799                          0.0389723   \n",
       "102252                         0.00391892                         6.6257e-06   \n",
       "102253                          0.0466006                         0.00134623   \n",
       "102254                        8.04842e-05                        5.67562e-09   \n",
       "102255                          0.0394911                        8.15297e-05   \n",
       "102256                          0.0157134                          0.0126582   \n",
       "102257                        0.000244773                        4.15952e-05   \n",
       "102258                         0.00454429                        0.000108705   \n",
       "102259                         0.00014901                        7.94139e-06   \n",
       "102260                         0.00113527                        1.39972e-05   \n",
       "102261                           0.683837                        0.000141723   \n",
       "102262                        0.000109944                          3.332e-05   \n",
       "102263                        6.28283e-06                        4.20199e-06   \n",
       "102264                          0.0487496                        0.000624602   \n",
       "102265                          0.0385674                        4.72379e-05   \n",
       "102266                          0.0014749                        9.83518e-05   \n",
       "102267                          0.0539901                        0.000125713   \n",
       "102268                        0.000186087                         1.7821e-05   \n",
       "102269                          0.0170557                         0.00320195   \n",
       "102270                        1.11781e-05                        5.54276e-06   \n",
       "102271                          0.0402091                          0.0623237   \n",
       "102272                          0.0333547                        0.000564265   \n",
       "102273                           0.121665                         0.00466516   \n",
       "102274                          0.0084595                        9.58623e-05   \n",
       "102275                         0.00263039                        9.58191e-06   \n",
       "102276                         0.00624561                           0.277867   \n",
       "\n",
       "       meta_base_rnn_wrod_v1_300dim_prob2 meta_base_rnn_wrod_v1_300dim_prob3  \\\n",
       "0                             0.000102125                        0.000280596   \n",
       "1                                0.205176                          0.0150657   \n",
       "2                             0.000799553                          0.0167076   \n",
       "3                             5.87818e-05                         0.00197263   \n",
       "4                             0.000528202                           0.318447   \n",
       "5                               0.0100747                        0.000687973   \n",
       "6                               0.0185646                        0.000574465   \n",
       "7                             0.000202785                        1.63342e-07   \n",
       "8                             3.04273e-05                        2.22436e-05   \n",
       "9                              0.00371484                        3.35084e-05   \n",
       "10                            8.71532e-05                        1.95913e-06   \n",
       "11                              0.0054856                         0.00638989   \n",
       "12                            3.45291e-06                        2.69874e-07   \n",
       "13                            0.000283023                           0.683768   \n",
       "14                            0.000195554                        4.80269e-05   \n",
       "15                               0.030726                        0.000465937   \n",
       "16                            1.15846e-05                        3.82187e-05   \n",
       "17                            1.67576e-06                         7.1805e-07   \n",
       "18                            0.000653438                        0.000184868   \n",
       "19                            4.10857e-05                        0.000151936   \n",
       "20                            0.000690838                         3.3289e-05   \n",
       "21                            0.000375048                        2.63349e-07   \n",
       "22                            0.000194947                         0.00715274   \n",
       "23                             0.00529043                          0.0347792   \n",
       "24                             0.00042174                         0.00117918   \n",
       "25                            0.000575811                        4.11418e-05   \n",
       "26                            0.000594414                           0.453065   \n",
       "27                            2.43339e-05                         0.00274834   \n",
       "28                            3.03899e-05                         3.2559e-05   \n",
       "29                            7.46923e-05                         5.3471e-05   \n",
       "...                                   ...                                ...   \n",
       "102247                        0.000135488                        4.21968e-06   \n",
       "102248                        6.42473e-06                        1.35912e-06   \n",
       "102249                        0.000971128                         0.00382188   \n",
       "102250                        5.74645e-05                        3.20606e-07   \n",
       "102251                         0.00696684                          0.0051315   \n",
       "102252                        2.31133e-05                        0.000150983   \n",
       "102253                           0.115866                          0.0008188   \n",
       "102254                        0.000269579                        3.12708e-07   \n",
       "102255                         0.00748279                        0.000354984   \n",
       "102256                         0.00526806                        0.000777837   \n",
       "102257                         3.4641e-05                            0.99757   \n",
       "102258                           0.405941                        0.000788294   \n",
       "102259                         0.00739143                        4.38855e-06   \n",
       "102260                        0.000141728                        2.13555e-05   \n",
       "102261                         0.00194693                        0.000188045   \n",
       "102262                         3.0468e-05                         1.7207e-06   \n",
       "102263                        0.000184396                        5.73613e-06   \n",
       "102264                        0.000641267                          0.0125842   \n",
       "102265                          0.0028942                         0.00334549   \n",
       "102266                        1.00349e-05                        0.000135424   \n",
       "102267                        8.95791e-05                        0.000274375   \n",
       "102268                           0.585113                        1.05851e-05   \n",
       "102269                           0.119341                        0.000600033   \n",
       "102270                           0.789941                         1.7496e-06   \n",
       "102271                         0.00372502                        0.000425641   \n",
       "102272                        0.000786972                          0.0847916   \n",
       "102273                         0.00090294                         0.00042312   \n",
       "102274                        0.000863617                         0.00368397   \n",
       "102275                        0.000372931                           0.923104   \n",
       "102276                         0.00164726                         4.1025e-05   \n",
       "\n",
       "       meta_base_rnn_wrod_v1_300dim_prob4 meta_base_rnn_wrod_v1_300dim_prob5  \\\n",
       "0                              0.00113676                        0.000297191   \n",
       "1                              0.00104939                          0.0114522   \n",
       "2                                0.000784                        0.000451077   \n",
       "3                             0.000478276                         6.4085e-05   \n",
       "4                             0.000129541                        0.000277433   \n",
       "5                             0.000124711                         7.7079e-05   \n",
       "6                                0.004053                        0.000226971   \n",
       "7                             8.26145e-06                            0.10912   \n",
       "8                             0.000194526                        3.09678e-05   \n",
       "9                             0.000685924                        7.06593e-05   \n",
       "10                            2.76006e-05                        4.04659e-06   \n",
       "11                             0.00467861                         0.00028018   \n",
       "12                            1.45341e-06                        3.53258e-05   \n",
       "13                            0.000752208                         0.00132947   \n",
       "14                             9.6694e-05                        1.09512e-05   \n",
       "15                             0.00198172                          0.0626795   \n",
       "16                            1.43915e-05                        6.39374e-05   \n",
       "17                            3.12129e-05                        0.000128984   \n",
       "18                             0.00118365                         0.00489552   \n",
       "19                            4.56394e-05                        7.48119e-06   \n",
       "20                                 0.0175                        6.20337e-06   \n",
       "21                            0.000128928                        6.83934e-06   \n",
       "22                             0.00837022                        0.000832582   \n",
       "23                             0.00487074                        0.000309201   \n",
       "24                             0.00315594                          2.741e-05   \n",
       "25                             0.00101643                         0.00146932   \n",
       "26                            0.000142077                        1.86598e-05   \n",
       "27                            2.61035e-05                          0.0012226   \n",
       "28                             0.00114426                        0.000291917   \n",
       "29                             0.00499978                        9.58905e-05   \n",
       "...                                   ...                                ...   \n",
       "102247                        3.18105e-05                         0.00024966   \n",
       "102248                        1.55606e-06                         1.0858e-07   \n",
       "102249                         0.00321262                        0.000435243   \n",
       "102250                        1.09837e-05                        1.47502e-06   \n",
       "102251                         0.00146292                        0.000998421   \n",
       "102252                        0.000131365                        2.26388e-05   \n",
       "102253                        0.000904528                           0.002782   \n",
       "102254                        1.59972e-06                        7.09532e-07   \n",
       "102255                        0.000174055                         0.00806223   \n",
       "102256                        0.000626021                          0.0245747   \n",
       "102257                        4.05204e-05                        0.000112756   \n",
       "102258                        4.56615e-05                         0.00026906   \n",
       "102259                        9.21629e-05                           0.990179   \n",
       "102260                        2.66655e-05                        4.58749e-06   \n",
       "102261                         0.00199645                         6.5436e-05   \n",
       "102262                        2.47155e-06                        5.22633e-07   \n",
       "102263                        3.09138e-06                           0.999325   \n",
       "102264                          0.0104204                        0.000686893   \n",
       "102265                        9.08212e-05                        0.000315388   \n",
       "102266                        1.71209e-05                        1.08435e-05   \n",
       "102267                         0.00601792                        0.000122236   \n",
       "102268                        0.000127854                        6.53375e-05   \n",
       "102269                        3.96352e-05                        0.000327016   \n",
       "102270                        1.35134e-05                           0.209519   \n",
       "102271                         0.00517096                         0.00324587   \n",
       "102272                         0.00146128                        0.000754781   \n",
       "102273                         0.00228152                        0.000200171   \n",
       "102274                        0.000231259                         0.00105487   \n",
       "102275                        0.000161408                        0.000550125   \n",
       "102276                        0.000132991                        0.000839733   \n",
       "\n",
       "       meta_base_rnn_wrod_v1_300dim_prob6 meta_base_rnn_wrod_v1_300dim_prob7  \\\n",
       "0                              0.00201609                           0.011277   \n",
       "1                               0.0107847                          0.0178365   \n",
       "2                                0.263447                        0.000484068   \n",
       "3                             0.000301501                        0.000593545   \n",
       "4                               0.0834307                        0.000307498   \n",
       "5                             0.000932308                        0.000122889   \n",
       "6                               0.0038321                          0.0124038   \n",
       "7                             8.14359e-06                        0.000471771   \n",
       "8                             8.60485e-05                          0.0013385   \n",
       "9                                0.184815                        0.000716093   \n",
       "10                            1.55278e-05                        1.34479e-05   \n",
       "11                              0.0359807                         0.00700673   \n",
       "12                            5.56732e-06                        4.35078e-08   \n",
       "13                              0.0186075                         0.00581888   \n",
       "14                            0.000543621                          0.0012241   \n",
       "15                               0.107584                          0.0144016   \n",
       "16                            6.17736e-06                        7.52984e-06   \n",
       "17                            7.77209e-05                         0.00358537   \n",
       "18                             0.00127151                         0.00394004   \n",
       "19                            0.000165341                         0.00232355   \n",
       "20                            0.000658452                         0.00119209   \n",
       "21                            7.83433e-05                        8.34597e-06   \n",
       "22                            0.000259338                           0.422135   \n",
       "23                             0.00447058                           0.021606   \n",
       "24                             0.00552979                           0.120705   \n",
       "25                              0.0560131                         0.00995776   \n",
       "26                               0.260151                        0.000143211   \n",
       "27                            0.000471775                          0.0463422   \n",
       "28                            0.000769414                           0.263942   \n",
       "29                            3.80159e-05                           0.962819   \n",
       "...                                   ...                                ...   \n",
       "102247                        2.84787e-05                        1.27315e-05   \n",
       "102248                        2.04257e-06                        1.80428e-06   \n",
       "102249                          0.0018487                           0.069267   \n",
       "102250                        0.000421547                        6.14074e-07   \n",
       "102251                        0.000896374                          0.0689576   \n",
       "102252                        0.000121658                        6.21453e-05   \n",
       "102253                           0.205775                         0.00650209   \n",
       "102254                        2.24525e-06                        1.03133e-06   \n",
       "102255                          0.0790045                          0.0125874   \n",
       "102256                         0.00167468                         0.00349117   \n",
       "102257                        0.000109296                        2.81369e-05   \n",
       "102258                          0.0366898                        0.000622506   \n",
       "102259                        3.90698e-06                        7.34594e-05   \n",
       "102260                         0.00078526                        0.000151412   \n",
       "102261                        0.000797848                           0.243308   \n",
       "102262                         3.6819e-06                        1.72507e-06   \n",
       "102263                        8.66481e-06                        1.08899e-05   \n",
       "102264                          0.0134876                           0.614656   \n",
       "102265                        1.62542e-05                          0.0033741   \n",
       "102266                        1.94668e-05                        8.97833e-05   \n",
       "102267                        0.000938989                           0.798291   \n",
       "102268                         0.00239556                        8.80146e-05   \n",
       "102269                          0.0344254                         0.00288996   \n",
       "102270                        2.27081e-06                        0.000121321   \n",
       "102271                        0.000659475                           0.147535   \n",
       "102272                          0.0688386                         0.00829482   \n",
       "102273                        0.000774034                           0.553873   \n",
       "102274                         0.00768246                         0.00170157   \n",
       "102275                         0.00205858                        0.000948885   \n",
       "102276                        0.000854707                         0.00187035   \n",
       "\n",
       "       meta_base_rnn_wrod_v1_300dim_prob8 meta_base_rnn_wrod_v1_300dim_prob9  \\\n",
       "0                             0.000228425                          0.0279983   \n",
       "1                              0.00477458                          0.0044505   \n",
       "2                              0.00258696                         0.00594361   \n",
       "3                              0.00014242                        0.000111584   \n",
       "4                               0.0101595                        5.16035e-05   \n",
       "5                             8.12786e-05                        0.000491437   \n",
       "6                              0.00227509                          0.0856138   \n",
       "7                             5.30261e-06                           0.889522   \n",
       "8                             2.27349e-05                           0.996469   \n",
       "9                               0.0247319                        0.000611504   \n",
       "10                             0.00121565                        0.000143187   \n",
       "11                               0.104248                         0.00295146   \n",
       "12                               0.999942                        9.49475e-08   \n",
       "13                            0.000263542                         0.00474559   \n",
       "14                             0.00361258                         0.00015123   \n",
       "15                               0.109521                         0.00303408   \n",
       "16                            1.23052e-06                        2.31627e-05   \n",
       "17                            4.40863e-06                           0.977246   \n",
       "18                            0.000183467                           0.664278   \n",
       "19                            7.12966e-05                         0.00140398   \n",
       "20                             0.00023702                           0.967911   \n",
       "21                               0.999003                        8.89361e-06   \n",
       "22                            0.000498094                         0.00663255   \n",
       "23                            0.000778857                         0.00216915   \n",
       "24                            0.000262624                          0.0706039   \n",
       "25                               0.794888                          0.0184709   \n",
       "26                             0.00331241                         0.00041573   \n",
       "27                            2.48006e-05                         0.00412433   \n",
       "28                            0.000113178                           0.681098   \n",
       "29                            2.21051e-05                          0.0256613   \n",
       "...                                   ...                                ...   \n",
       "102247                           0.995521                        3.98284e-05   \n",
       "102248                        1.77761e-06                        1.04665e-06   \n",
       "102249                        0.000326047                          0.0110576   \n",
       "102250                           0.999402                        5.56328e-07   \n",
       "102251                        0.000482556                         0.00171457   \n",
       "102252                        2.36002e-05                         0.00348796   \n",
       "102253                          0.0274959                         0.00341001   \n",
       "102254                        1.50067e-05                        1.05379e-06   \n",
       "102255                          0.0473351                          0.0102037   \n",
       "102256                        0.000668989                         0.00651027   \n",
       "102257                        2.93075e-06                         0.00024516   \n",
       "102258                         0.00630952                        0.000473002   \n",
       "102259                         2.4411e-06                        0.000622014   \n",
       "102260                           0.974311                        7.03018e-05   \n",
       "102261                        0.000854508                         0.00609167   \n",
       "102262                        0.000168352                        3.11476e-06   \n",
       "102263                         7.6289e-07                        0.000275288   \n",
       "102264                           0.007409                         0.00739651   \n",
       "102265                        5.95641e-05                        0.000593056   \n",
       "102266                        3.29875e-05                        0.000138403   \n",
       "102267                        0.000528398                          0.0464337   \n",
       "102268                           0.405289                        2.62212e-05   \n",
       "102269                          0.0245626                          0.0012475   \n",
       "102270                        9.03761e-06                        6.90369e-05   \n",
       "102271                         0.00168929                          0.0667847   \n",
       "102272                         0.00109163                          0.0432314   \n",
       "102273                         0.00250684                         0.00675544   \n",
       "102274                         0.00387787                          0.0012175   \n",
       "102275                         0.00236867                         0.00160019   \n",
       "102276                        0.000624258                        8.47149e-05   \n",
       "\n",
       "       meta_base_rnn_wrod_v1_300dim_prob10  \\\n",
       "0                              0.000527547   \n",
       "1                                0.0104938   \n",
       "2                               0.00607816   \n",
       "3                               5.6173e-05   \n",
       "4                               0.00849455   \n",
       "5                                0.0296258   \n",
       "6                                0.0059898   \n",
       "7                              2.40785e-05   \n",
       "8                               1.7496e-05   \n",
       "9                                0.0190444   \n",
       "10                             4.99837e-05   \n",
       "11                              0.00937905   \n",
       "12                             1.65568e-06   \n",
       "13                             4.40061e-05   \n",
       "14                             9.30802e-05   \n",
       "15                               0.0712067   \n",
       "16                             4.06073e-05   \n",
       "17                             4.59887e-05   \n",
       "18                              0.00127097   \n",
       "19                             0.000155878   \n",
       "20                             0.000259943   \n",
       "21                             8.65836e-06   \n",
       "22                             0.000327445   \n",
       "23                              0.00110259   \n",
       "24                             0.000416691   \n",
       "25                             0.000357167   \n",
       "26                              0.00805845   \n",
       "27                             1.01023e-05   \n",
       "28                             0.000415603   \n",
       "29                             1.01601e-05   \n",
       "...                                    ...   \n",
       "102247                         0.000644456   \n",
       "102248                         4.76815e-05   \n",
       "102249                           0.0399957   \n",
       "102250                          3.9554e-06   \n",
       "102251                         0.000684186   \n",
       "102252                         0.000151401   \n",
       "102253                            0.207298   \n",
       "102254                            0.999402   \n",
       "102255                            0.620014   \n",
       "102256                         0.000833621   \n",
       "102257                         6.36362e-05   \n",
       "102258                             0.18945   \n",
       "102259                         5.57196e-05   \n",
       "102260                         0.000646139   \n",
       "102261                         0.000531022   \n",
       "102262                         8.14792e-05   \n",
       "102263                          1.0036e-06   \n",
       "102264                           0.0100568   \n",
       "102265                          0.00085696   \n",
       "102266                         8.17569e-05   \n",
       "102267                          0.00226854   \n",
       "102268                         0.000860727   \n",
       "102269                          0.00576923   \n",
       "102270                         3.79515e-06   \n",
       "102271                         0.000593747   \n",
       "102272                          0.00996772   \n",
       "102273                           0.0014338   \n",
       "102274                            0.587107   \n",
       "102275                          0.00903152   \n",
       "102276                            0.699481   \n",
       "\n",
       "       meta_base_rnn_wrod_v1_300dim_prob11  \\\n",
       "0                              0.000457277   \n",
       "1                               0.00337448   \n",
       "2                               0.00684792   \n",
       "3                               0.00204868   \n",
       "4                                  0.55004   \n",
       "5                               0.00278336   \n",
       "6                                0.0026417   \n",
       "7                              0.000184653   \n",
       "8                              2.93614e-05   \n",
       "9                                 0.156417   \n",
       "10                             3.62057e-05   \n",
       "11                               0.0507639   \n",
       "12                             3.08469e-06   \n",
       "13                               0.0878157   \n",
       "14                              0.00073933   \n",
       "15                               0.0547523   \n",
       "16                             0.000434232   \n",
       "17                             0.000178509   \n",
       "18                              0.00126869   \n",
       "19                             0.000517777   \n",
       "20                             7.14571e-06   \n",
       "21                             0.000119524   \n",
       "22                              0.00515536   \n",
       "23                              0.00476823   \n",
       "24                               0.0223186   \n",
       "25                             0.000485464   \n",
       "26                               0.0886101   \n",
       "27                             0.000162386   \n",
       "28                             0.000428428   \n",
       "29                             4.74599e-05   \n",
       "...                                    ...   \n",
       "102247                         0.000355095   \n",
       "102248                         1.93328e-05   \n",
       "102249                          0.00373253   \n",
       "102250                         1.77641e-06   \n",
       "102251                           0.0304214   \n",
       "102252                         1.04186e-05   \n",
       "102253                            0.268468   \n",
       "102254                         1.01868e-05   \n",
       "102255                           0.0469633   \n",
       "102256                          0.00276006   \n",
       "102257                         7.96587e-05   \n",
       "102258                           0.0843363   \n",
       "102259                           0.0010009   \n",
       "102260                          0.00111378   \n",
       "102261                           0.0476625   \n",
       "102262                         0.000176247   \n",
       "102263                         8.30788e-05   \n",
       "102264                           0.0328516   \n",
       "102265                         0.000834653   \n",
       "102266                         0.000316201   \n",
       "102267                         0.000251753   \n",
       "102268                            0.002705   \n",
       "102269                           0.0192896   \n",
       "102270                         0.000235522   \n",
       "102271                          0.00369526   \n",
       "102272                          0.00485371   \n",
       "102273                          0.00395336   \n",
       "102274                            0.339281   \n",
       "102275                          0.00350209   \n",
       "102276                          0.00279478   \n",
       "\n",
       "       meta_base_rnn_wrod_v1_300dim_prob12  \\\n",
       "0                              0.000981998   \n",
       "1                                0.0326517   \n",
       "2                               0.00824489   \n",
       "3                                 0.680166   \n",
       "4                               0.00080656   \n",
       "5                                 0.922409   \n",
       "6                              0.000129325   \n",
       "7                               0.00018651   \n",
       "8                              4.91412e-05   \n",
       "9                                0.0320471   \n",
       "10                             0.000186148   \n",
       "11                               0.0219055   \n",
       "12                             2.74073e-07   \n",
       "13                                0.015197   \n",
       "14                             0.000286019   \n",
       "15                              0.00508214   \n",
       "16                                0.996855   \n",
       "17                             0.000993204   \n",
       "18                                0.201189   \n",
       "19                               0.0003786   \n",
       "20                             0.000823718   \n",
       "21                             7.20069e-05   \n",
       "22                               0.0713012   \n",
       "23                              0.00635844   \n",
       "24                                 0.40513   \n",
       "25                              0.00244877   \n",
       "26                               0.0203095   \n",
       "27                              0.00143425   \n",
       "28                             0.000582853   \n",
       "29                              0.00108154   \n",
       "...                                    ...   \n",
       "102247                          0.00102517   \n",
       "102248                         1.16854e-05   \n",
       "102249                            0.733918   \n",
       "102250                         8.53685e-06   \n",
       "102251                           0.0692814   \n",
       "102252                         0.000288602   \n",
       "102253                           0.0383202   \n",
       "102254                         2.04447e-05   \n",
       "102255                            0.054466   \n",
       "102256                         0.000527099   \n",
       "102257                          0.00019582   \n",
       "102258                           0.0292849   \n",
       "102259                         0.000111806   \n",
       "102260                         0.000151593   \n",
       "102261                          0.00824738   \n",
       "102262                         3.10884e-05   \n",
       "102263                          1.7961e-05   \n",
       "102264                            0.177786   \n",
       "102265                           0.0112368   \n",
       "102266                         6.19415e-05   \n",
       "102267                           0.0636187   \n",
       "102268                         0.000411313   \n",
       "102269                           0.0319256   \n",
       "102270                         2.32743e-06   \n",
       "102271                            0.650523   \n",
       "102272                           0.0261101   \n",
       "102273                           0.0186877   \n",
       "102274                          0.00921187   \n",
       "102275                           0.0248077   \n",
       "102276                           0.0015021   \n",
       "\n",
       "       meta_base_rnn_wrod_v1_300dim_prob13  \\\n",
       "0                                0.0844372   \n",
       "1                              0.000376994   \n",
       "2                                 0.673102   \n",
       "3                                 0.280089   \n",
       "4                                0.0181468   \n",
       "5                              0.000342142   \n",
       "6                                0.0362188   \n",
       "7                              1.16147e-06   \n",
       "8                              9.46555e-06   \n",
       "9                                0.0458184   \n",
       "10                              0.00165968   \n",
       "11                               0.0137073   \n",
       "12                             1.67233e-07   \n",
       "13                                0.001573   \n",
       "14                                0.208645   \n",
       "15                              0.00181246   \n",
       "16                             2.34857e-05   \n",
       "17                             0.000425074   \n",
       "18                               0.0373893   \n",
       "19                                0.979199   \n",
       "20                              0.00135199   \n",
       "21                             2.50557e-06   \n",
       "22                              0.00499426   \n",
       "23                               0.0353753   \n",
       "24                             0.000800969   \n",
       "25                              0.00184609   \n",
       "26                                0.152873   \n",
       "27                                0.920396   \n",
       "28                             0.000486962   \n",
       "29                              0.00061431   \n",
       "...                                    ...   \n",
       "102247                          6.0194e-05   \n",
       "102248                         1.38187e-05   \n",
       "102249                          0.00197786   \n",
       "102250                          5.0286e-06   \n",
       "102251                         0.000508483   \n",
       "102252                            0.990504   \n",
       "102253                          0.00356205   \n",
       "102254                         5.17691e-05   \n",
       "102255                           0.0258708   \n",
       "102256                         0.000347862   \n",
       "102257                          0.00112467   \n",
       "102258                           0.0055727   \n",
       "102259                         7.36255e-07   \n",
       "102260                          0.00368962   \n",
       "102261                         0.000245635   \n",
       "102262                         3.86714e-05   \n",
       "102263                         3.85241e-06   \n",
       "102264                           0.0185808   \n",
       "102265                         8.53575e-05   \n",
       "102266                         8.10838e-05   \n",
       "102267                           0.0129121   \n",
       "102268                         0.000216889   \n",
       "102269                            0.019662   \n",
       "102270                         9.49593e-06   \n",
       "102271                           0.0044267   \n",
       "102272                            0.634576   \n",
       "102273                         0.000266956   \n",
       "102274                         0.000459942   \n",
       "102275                           0.0263932   \n",
       "102276                         3.08612e-06   \n",
       "\n",
       "       meta_base_rnn_wrod_v1_300dim_prob14  \\\n",
       "0                               0.00138605   \n",
       "1                                 0.561974   \n",
       "2                               0.00141714   \n",
       "3                              9.91731e-05   \n",
       "4                               0.00503975   \n",
       "5                               0.00279051   \n",
       "6                               0.00121623   \n",
       "7                              4.15998e-07   \n",
       "8                              2.37934e-06   \n",
       "9                              0.000870905   \n",
       "10                             0.000256742   \n",
       "11                              0.00862758   \n",
       "12                             4.57625e-07   \n",
       "13                              0.00470924   \n",
       "14                             0.000323229   \n",
       "15                              0.00383306   \n",
       "16                             5.37127e-07   \n",
       "17                              1.2448e-05   \n",
       "18                             0.000619593   \n",
       "19                              0.00142464   \n",
       "20                              2.8236e-05   \n",
       "21                             1.54169e-07   \n",
       "22                             6.25562e-05   \n",
       "23                             0.000603963   \n",
       "24                              0.00339516   \n",
       "25                               0.0215438   \n",
       "26                              0.00307336   \n",
       "27                               0.0022569   \n",
       "28                             5.15944e-05   \n",
       "29                             9.63466e-05   \n",
       "...                                    ...   \n",
       "102247                         0.000778199   \n",
       "102248                         7.97265e-06   \n",
       "102249                         0.000228845   \n",
       "102250                          4.1558e-05   \n",
       "102251                         9.76958e-05   \n",
       "102252                         0.000812819   \n",
       "102253                           0.0075336   \n",
       "102254                         9.34977e-05   \n",
       "102255                            0.011341   \n",
       "102256                         0.000147049   \n",
       "102257                         5.72402e-05   \n",
       "102258                             0.21383   \n",
       "102259                         5.20817e-06   \n",
       "102260                         0.000144254   \n",
       "102261                         0.000111529   \n",
       "102262                         7.58127e-05   \n",
       "102263                         8.13799e-07   \n",
       "102264                          0.00826856   \n",
       "102265                         0.000203825   \n",
       "102266                         3.62379e-06   \n",
       "102267                          0.00434243   \n",
       "102268                          0.00205781   \n",
       "102269                           0.0184327   \n",
       "102270                         1.35514e-05   \n",
       "102271                         0.000840453   \n",
       "102272                           0.0224724   \n",
       "102273                          0.00012494   \n",
       "102274                          0.00266342   \n",
       "102275                          0.00043253   \n",
       "102276                          0.00301185   \n",
       "\n",
       "       meta_base_rnn_wrod_v1_300dim_prob15  \\\n",
       "0                               0.00620003   \n",
       "1                               0.00221263   \n",
       "2                              3.83659e-05   \n",
       "3                                 0.015348   \n",
       "4                              2.06945e-05   \n",
       "5                              0.000950382   \n",
       "6                              3.37266e-05   \n",
       "7                              8.72374e-05   \n",
       "8                              2.15814e-05   \n",
       "9                               9.7643e-06   \n",
       "10                             0.000359101   \n",
       "11                             7.04401e-05   \n",
       "12                             5.31762e-07   \n",
       "13                             0.000100012   \n",
       "14                             1.02837e-05   \n",
       "15                             4.71164e-05   \n",
       "16                             5.80306e-06   \n",
       "17                             6.63985e-05   \n",
       "18                              0.00334107   \n",
       "19                             0.000576421   \n",
       "20                             1.86421e-05   \n",
       "21                             7.98029e-06   \n",
       "22                             0.000235867   \n",
       "23                              0.00161581   \n",
       "24                               0.0115276   \n",
       "25                             1.18243e-05   \n",
       "26                             3.09563e-05   \n",
       "27                              0.00367671   \n",
       "28                              0.00133491   \n",
       "29                               0.0011993   \n",
       "...                                    ...   \n",
       "102247                         1.37145e-05   \n",
       "102248                         5.35638e-06   \n",
       "102249                         0.000350132   \n",
       "102250                         1.71159e-06   \n",
       "102251                          0.00202266   \n",
       "102252                         0.000144361   \n",
       "102253                         0.000190143   \n",
       "102254                         2.40233e-06   \n",
       "102255                         0.000170519   \n",
       "102256                          0.00206062   \n",
       "102257                         8.58875e-07   \n",
       "102258                         0.000112433   \n",
       "102259                         6.66596e-05   \n",
       "102260                         2.45196e-05   \n",
       "102261                         0.000484734   \n",
       "102262                         4.48334e-05   \n",
       "102263                          1.1514e-07   \n",
       "102264                          0.00114798   \n",
       "102265                            0.932603   \n",
       "102266                         0.000260635   \n",
       "102267                            0.003249   \n",
       "102268                         1.89262e-06   \n",
       "102269                         0.000407894   \n",
       "102270                         1.70875e-05   \n",
       "102271                         0.000534509   \n",
       "102272                         0.000714572   \n",
       "102273                            0.272419   \n",
       "102274                         2.64839e-05   \n",
       "102275                         3.77664e-05   \n",
       "102276                          7.8369e-06   \n",
       "\n",
       "       meta_base_rnn_wrod_v1_300dim_prob16  \\\n",
       "0                               0.00540942   \n",
       "1                               0.00721158   \n",
       "2                              0.000402002   \n",
       "3                                0.0021364   \n",
       "4                               0.00135884   \n",
       "5                              3.25806e-05   \n",
       "6                                0.0239614   \n",
       "7                               1.8724e-05   \n",
       "8                              1.71409e-05   \n",
       "9                              0.000411843   \n",
       "10                             2.04193e-05   \n",
       "11                               0.0191357   \n",
       "12                             3.29898e-07   \n",
       "13                             4.14703e-05   \n",
       "14                                0.781701   \n",
       "15                             0.000236426   \n",
       "16                             1.46535e-05   \n",
       "17                             4.01371e-05   \n",
       "18                              0.00553634   \n",
       "19                              0.00869864   \n",
       "20                             1.70596e-05   \n",
       "21                             1.97872e-05   \n",
       "22                             0.000365782   \n",
       "23                              0.00359976   \n",
       "24                              0.00213462   \n",
       "25                               0.0237253   \n",
       "26                             6.95673e-05   \n",
       "27                               0.0146426   \n",
       "28                             8.23866e-05   \n",
       "29                             4.37645e-05   \n",
       "...                                    ...   \n",
       "102247                         0.000121591   \n",
       "102248                         5.25012e-06   \n",
       "102249                          0.00161045   \n",
       "102250                         6.97629e-07   \n",
       "102251                           0.0018141   \n",
       "102252                         8.74027e-06   \n",
       "102253                           0.0134913   \n",
       "102254                           1.325e-06   \n",
       "102255                          0.00235939   \n",
       "102256                          0.00283011   \n",
       "102257                         1.43095e-05   \n",
       "102258                           0.0165206   \n",
       "102259                         1.19284e-05   \n",
       "102260                          8.8429e-05   \n",
       "102261                          7.3405e-05   \n",
       "102262                         6.80507e-06   \n",
       "102263                         3.67521e-07   \n",
       "102264                          0.00587539   \n",
       "102265                         4.14239e-05   \n",
       "102266                         1.26171e-05   \n",
       "102267                            0.001191   \n",
       "102268                         7.09371e-05   \n",
       "102269                           0.0231501   \n",
       "102270                         2.47944e-06   \n",
       "102271                          0.00129991   \n",
       "102272                           0.0201461   \n",
       "102273                          0.00266441   \n",
       "102274                          0.00029573   \n",
       "102275                          0.00011263   \n",
       "102276                         8.86975e-05   \n",
       "\n",
       "       meta_base_rnn_wrod_v1_300dim_prob17 meta_base_rnn_wrod_v1_300dim_prob18  \n",
       "0                               0.00878303                          0.00131441  \n",
       "1                                 0.057818                           0.0360538  \n",
       "2                              0.000100433                          0.00276673  \n",
       "3                              0.000472363                          0.00234889  \n",
       "4                              1.57882e-05                         0.000136033  \n",
       "5                              0.000489108                          0.00907236  \n",
       "6                               0.00978395                          0.00381497  \n",
       "7                                2.746e-06                         5.79916e-05  \n",
       "8                              7.73404e-06                         7.78797e-05  \n",
       "9                               0.00335388                            0.459139  \n",
       "10                                0.979086                           0.0158745  \n",
       "11                               0.0380617                            0.617881  \n",
       "12                             9.79328e-10                         1.11036e-07  \n",
       "13                             1.77895e-05                            0.164907  \n",
       "14                             0.000216162                          0.00100688  \n",
       "15                               0.0122447                            0.099193  \n",
       "16                             0.000175646                         0.000179796  \n",
       "17                             7.53856e-05                         0.000102749  \n",
       "18                               0.0124714                          0.00718234  \n",
       "19                             0.000175397                          0.00148026  \n",
       "20                             0.000222627                          0.00090435  \n",
       "21                               4.993e-07                         1.53357e-05  \n",
       "22                              0.00606732                          0.00508052  \n",
       "23                                 0.17978                           0.0468655  \n",
       "24                              0.00173542                           0.0198887  \n",
       "25                             0.000446713                           0.0365636  \n",
       "26                             3.12848e-05                           0.0073875  \n",
       "27                             1.30112e-05                         0.000356258  \n",
       "28                             8.98581e-05                           0.0036999  \n",
       "29                             3.92444e-05                         9.34594e-05  \n",
       "...                                    ...                                 ...  \n",
       "102247                         3.47139e-06                          0.00066306  \n",
       "102248                            0.990106                          0.00965717  \n",
       "102249                         0.000921419                          0.00246071  \n",
       "102250                         1.59663e-06                         3.03657e-05  \n",
       "102251                           0.0178637                            0.675644  \n",
       "102252                          5.1715e-05                         7.84772e-05  \n",
       "102253                          0.00573906                           0.0438972  \n",
       "102254                         3.29056e-06                         4.26152e-05  \n",
       "102255                          0.00230182                           0.0317354  \n",
       "102256                            0.904256                           0.0142745  \n",
       "102257                         6.45764e-06                         2.75215e-05  \n",
       "102258                         3.66765e-05                          0.00506464  \n",
       "102259                         0.000203472                         1.73422e-05  \n",
       "102260                          0.00567503                           0.0118051  \n",
       "102261                          0.00050721                          0.00290828  \n",
       "102262                            0.988239                           0.0109505  \n",
       "102263                         5.96271e-05                         8.49487e-06  \n",
       "102264                           0.0129456                           0.0158311  \n",
       "102265                          0.00142955                          0.00340517  \n",
       "102266                            0.994966                          0.00218903  \n",
       "102267                         0.000861517                          0.00449328  \n",
       "102268                         2.60265e-05                         0.000330251  \n",
       "102269                          0.00904987                            0.668622  \n",
       "102270                         1.01584e-05                           1.132e-05  \n",
       "102271                          0.00179582                          0.00452197  \n",
       "102272                           0.0174259                            0.020564  \n",
       "102273                           0.0011789                          0.00522371  \n",
       "102274                          0.00134575                            0.030741  \n",
       "102275                         5.38598e-05                           0.0018237  \n",
       "102276                         0.000610628                          0.00229218  \n",
       "\n",
       "[102277 rows x 19 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train.columns = ['meta_base_rnn_wrod_v1_300dim_' + x for x in meta_train.columns]\n",
    "\n",
    "p = pd.DataFrame()\n",
    "\n",
    "for i in range(19):\n",
    "    p[i] = meta_test[i].mean(axis = 1)\n",
    "\n",
    "p.columns = meta_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train.to_csv('/home/libo/daguan/stack_new/train_meta_base_rnn_word_v1_300dim.csv', index = None ,encoding = 'utf-8')\n",
    "\n",
    "p.to_csv('/home/libo/daguan/stack_new/test_meta_base_rnn_word_v1_300dim.csv', index = None ,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    \n",
    "    #Input layer\n",
    "    char_input = Input(shape=(max_sequence_length_api,), dtype='int32')\n",
    "    \n",
    "    api_embedding_layer = Embedding(nb_words,\n",
    "        embedding_dim,\n",
    "        weights=[api_embedding_matrix],\n",
    "        input_length=max_sequence_length_api,\n",
    "        trainable=False)\n",
    "    char_embedding = api_embedding_layer(char_input)\n",
    "#     with tf.device('/cpu:0'):\n",
    "#         char_embedding = api_embedding_layer(char_input)\n",
    "\n",
    "    \n",
    "    char_rnn_1= Bidirectional(CuDNNLSTM(100, \n",
    "                                return_sequences=True))(char_embedding)\n",
    "    char_rnn_2= Bidirectional(CuDNNLSTM(100, \n",
    "                                return_sequences=True))(char_rnn_1)\n",
    "    \n",
    "#     char_rnn_1 = Lambda(lambda x:K.permute_dimensions(x, (0, 2, 1)))(char_rnn_1)\n",
    "#     char_rnn_2 = Lambda(lambda x:K.permute_dimensions(x, (0, 2, 1)))(char_rnn_2)\n",
    "\n",
    "    merged = concatenate([char_rnn_1, char_rnn_2], axis = -1)\n",
    "    \n",
    "    #merged = Lambda(lambda x:K.permute_dimensions(x, (0, 2, 1)))(merged)\n",
    "    \n",
    "    merged = Dense(1024, activation='relu')(merged)\n",
    "\n",
    "    preds = Dense(19, activation = 'softmax')(merged)\n",
    "    \n",
    "    model = Model(inputs=[char_input],outputs=preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 1200)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1200, 300)    381438300   input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 1200, 200)    321600      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 1200, 200)    241600      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1200, 400)    0           bidirectional_5[0][0]            \n",
      "                                                                 bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1200, 1024)   410624      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1200, 19)     19475       dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 382,431,599\n",
      "Trainable params: 993,299\n",
      "Non-trainable params: 381,438,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train = pd.DataFrame(np.array([None]*train_FE.shape[0]*19).reshape(train_FE.shape[0], 19))\n",
    "meta_train.columns = ['prob' + str(x) for x in range(19)]\n",
    "meta_test = pd.DataFrame()\n",
    "\n",
    "label = train['class']\n",
    "\n",
    "label = label - 1\n",
    "\n",
    "categorical_labels = keras.utils.np_utils.to_categorical(label, num_classes=19)\n",
    "\n",
    "index = pd.read_csv('fold.csv')\n",
    "index['id'] = train['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    \n",
    "    #Input layer\n",
    "    char_input = Input(shape=(max_sequence_length_api,), dtype='int32')\n",
    "    \n",
    "    api_embedding_layer = Embedding(nb_words,\n",
    "        embedding_dim,\n",
    "        weights=[api_embedding_matrix],\n",
    "        input_length=max_sequence_length_api,\n",
    "        trainable=False)\n",
    "    char_embedding = api_embedding_layer(char_input)\n",
    "#     with tf.device('/cpu:0'):\n",
    "#         char_embedding = api_embedding_layer(char_input)\n",
    "\n",
    "    \n",
    "    char_rnn_1= Bidirectional(CuDNNLSTM(100, \n",
    "                                return_sequences=True))(char_embedding)\n",
    "    char_rnn_2= Bidirectional(CuDNNLSTM(100, \n",
    "                                return_sequences=True))(char_rnn_1)\n",
    "    \n",
    "#     char_rnn_1 = Lambda(lambda x:K.permute_dimensions(x, (0, 2, 1)))(char_rnn_1)\n",
    "#     char_rnn_2 = Lambda(lambda x:K.permute_dimensions(x, (0, 2, 1)))(char_rnn_2)\n",
    "\n",
    "    #merged = concatenate([char_rnn_1, char_rnn_2], axis = -1)\n",
    "    \n",
    "    #merged = Lambda(lambda x:K.permute_dimensions(x, (0, 2, 1)))(merged)\n",
    "    \n",
    "    merged = Dense(1024, activation='relu')(char_rnn_1)\n",
    "\n",
    "    preds = Dense(19, activation = 'softmax')(merged)\n",
    "    \n",
    "    model = Model(inputs=[char_input],outputs=preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    \n",
    "    #Input layer\n",
    "    char_input = Input(shape=(max_sequence_length_api,), dtype='int32')\n",
    "    \n",
    "    api_embedding_layer = Embedding(nb_words,\n",
    "        embedding_dim,\n",
    "        weights=[api_embedding_matrix],\n",
    "        input_length=max_sequence_length_api,\n",
    "        trainable=False)\n",
    "    \n",
    "    #RNN\n",
    "    char_rnn_layer_1 = Bidirectional(CuDNNLSTM(250, \n",
    "                               return_sequences=True))\n",
    "    char_rnn_layer_2 = Bidirectional(CuDNNLSTM(250, \n",
    "                               return_sequences=True))\n",
    "    \n",
    "    char_embedding = api_embedding_layer(char_input)\n",
    "#     with tf.device('/cpu:0'):\n",
    "#         char_embedding = api_embedding_layer(char_input)\n",
    "    #char_embedding = SpatialDropout1D(0.2)(char_embedding)\n",
    "    \n",
    "    #双层BiLSTM\n",
    "    char_rnn_1= char_rnn_layer_1(char_embedding)\n",
    "    char_rnn_2= char_rnn_layer_2(char_rnn_1)\n",
    "    char_rnn_2 = Flatten()(char_rnn_2)\n",
    "    merged = Dropout(0.2)(char_rnn_2)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dense(400, activation='relu')(merged)\n",
    "    merged = Dropout(0.1)(merged)\n",
    "    merged = BatchNormalization()(merged) \n",
    "    preds = Dense(19, activation = 'softmax')(merged)\n",
    "    \n",
    "    model = Model(inputs=[char_input],outputs=preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 1200, 300)         381438300 \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 1200, 500)         1104000   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 1200, 500)         1504000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 600000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 600000)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 600000)            2400000   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 400)               240000400 \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 400)               1600      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 19)                7619      \n",
      "=================================================================\n",
      "Total params: 626,455,919\n",
      "Trainable params: 243,816,819\n",
      "Non-trainable params: 382,639,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******0*******\n",
      "Train on 92040 samples, validate on 10237 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[600000,400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training/Adam/gradients/dense_1/MatMul_grad/MatMul_1 = MatMul[T=DT_FLOAT, _class=[\"loc:@dense_1/MatMul\"], transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_1/cond/Merge, training/Adam/gradients/dense_1/Relu_grad/ReluGrad)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'training/Adam/gradients/dense_1/MatMul_grad/MatMul_1', defined at:\n  File \"/home/libo/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/libo/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2808, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-37-cbb54f935885>\", line 1, in <module>\n    get_ipython().run_cell_magic('time', '', \"#logloss_list = []\\nfor i in range(10):\\n    gc.collect()\\n    K.clear_session()\\n    print('*******%s*******'%i)\\n    idx_train = np.array(index[index['fold'] != i]['id'])\\n    idx_val = np.array(index[index['fold'] == i]['id'])\\n    \\n    \\n    X_train = train_FE.loc[idx_train, :]\\n    y_train = categorical_labels[idx_train]\\n\\n    X_val = train_FE.loc[idx_val, :]\\n    y_val = categorical_labels[idx_val]\\n\\n    #dtest = test.loc[:,user_col]\\n    \\n    X_train = get_input(X_train)\\n    X_val = get_input(X_val)\\n    X_test = get_input(test_FE)\\n    \\n    BATCH_SIZE = 64\\n    bst_model_path = '10folds_' + 'base_rnn_wrod_v1_300dim' + str(i) + '.hdf5'\\n    early_stopping =EarlyStopping(monitor='val_acc', patience=2)\\n    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\\n    callbacks = [\\n            early_stopping,\\n            model_checkpoint\\n        ]\\n    model = RNN()\\n    model.compile(optimizer='adam', loss='categorical_crossentropy',\\\\\\n                 metrics=['accuracy'])\\n    \\n\\n    hist = model.fit(X_train, y_train, \\\\\\n            validation_data=(X_val, y_val), \\\\\\n            epochs=20, batch_size=BATCH_SIZE, shuffle=True, callbacks=callbacks)#callbacks=callbacks\\n    \\n    model = RNN()\\n    model.load_weights(bst_model_path)\\n    \\n    pred = pd.DataFrame(model.predict(X_val,batch_size=64,verbose=1))\\n    \\n    for i in range(19):\\n        meta_train.loc[idx_val, 'prob' + str(i)] = pred[i].values\\n    \\n    prob = model.predict(X_test,batch_size=64,verbose=1)\\n    meta_test = pd.concat([meta_test, pd.DataFrame(prob)], axis = 1)\")\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2103, in run_cell_magic\n    result = fn(magic_arg_s, cell)\n  File \"<decorator-gen-62>\", line 2, in time\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/IPython/core/magic.py\", line 187, in <lambda>\n    call = lambda f, *a, **k: f(*a, **k)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/IPython/core/magics/execution.py\", line 1215, in time\n    exec(code, glob, local_ns)\n  File \"<timed exec>\", line 34, in <module>\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 1682, in fit\n    self._make_train_function()\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 992, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/keras/optimizers.py\", line 445, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/keras/optimizers.py\", line 78, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2519, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 375, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py\", line 963, in _MatMulGrad\n    grad_b = math_ops.matmul(a, grad, transpose_a=True)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 2022, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2516, in _mat_mul\n    name=name)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'dense_1/MatMul', defined at:\n  File \"/home/libo/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 23 identical lines from previous traceback]\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/IPython/core/magics/execution.py\", line 1215, in time\n    exec(code, glob, local_ns)\n  File \"<timed exec>\", line 30, in <module>\n  File \"<ipython-input-33-73f26ce9c823>\", line 29, in RNN\n    merged = Dense(400, activation='relu')(merged)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/keras/layers/core.py\", line 877, in call\n    output = K.dot(inputs, self.kernel)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 1076, in dot\n    out = tf.matmul(x, y)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 2022, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2516, in _mat_mul\n    name=name)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[600000,400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training/Adam/gradients/dense_1/MatMul_grad/MatMul_1 = MatMul[T=DT_FLOAT, _class=[\"loc:@dense_1/MatMul\"], transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_1/cond/Merge, training/Adam/gradients/dense_1/Relu_grad/ReluGrad)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[600000,400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training/Adam/gradients/dense_1/MatMul_grad/MatMul_1 = MatMul[T=DT_FLOAT, _class=[\"loc:@dense_1/MatMul\"], transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_1/cond/Merge, training/Adam/gradients/dense_1/Relu_grad/ReluGrad)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[600000,400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training/Adam/gradients/dense_1/MatMul_grad/MatMul_1 = MatMul[T=DT_FLOAT, _class=[\"loc:@dense_1/MatMul\"], transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_1/cond/Merge, training/Adam/gradients/dense_1/Relu_grad/ReluGrad)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'training/Adam/gradients/dense_1/MatMul_grad/MatMul_1', defined at:\n  File \"/home/libo/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/libo/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2808, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-37-cbb54f935885>\", line 1, in <module>\n    get_ipython().run_cell_magic('time', '', \"#logloss_list = []\\nfor i in range(10):\\n    gc.collect()\\n    K.clear_session()\\n    print('*******%s*******'%i)\\n    idx_train = np.array(index[index['fold'] != i]['id'])\\n    idx_val = np.array(index[index['fold'] == i]['id'])\\n    \\n    \\n    X_train = train_FE.loc[idx_train, :]\\n    y_train = categorical_labels[idx_train]\\n\\n    X_val = train_FE.loc[idx_val, :]\\n    y_val = categorical_labels[idx_val]\\n\\n    #dtest = test.loc[:,user_col]\\n    \\n    X_train = get_input(X_train)\\n    X_val = get_input(X_val)\\n    X_test = get_input(test_FE)\\n    \\n    BATCH_SIZE = 64\\n    bst_model_path = '10folds_' + 'base_rnn_wrod_v1_300dim' + str(i) + '.hdf5'\\n    early_stopping =EarlyStopping(monitor='val_acc', patience=2)\\n    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\\n    callbacks = [\\n            early_stopping,\\n            model_checkpoint\\n        ]\\n    model = RNN()\\n    model.compile(optimizer='adam', loss='categorical_crossentropy',\\\\\\n                 metrics=['accuracy'])\\n    \\n\\n    hist = model.fit(X_train, y_train, \\\\\\n            validation_data=(X_val, y_val), \\\\\\n            epochs=20, batch_size=BATCH_SIZE, shuffle=True, callbacks=callbacks)#callbacks=callbacks\\n    \\n    model = RNN()\\n    model.load_weights(bst_model_path)\\n    \\n    pred = pd.DataFrame(model.predict(X_val,batch_size=64,verbose=1))\\n    \\n    for i in range(19):\\n        meta_train.loc[idx_val, 'prob' + str(i)] = pred[i].values\\n    \\n    prob = model.predict(X_test,batch_size=64,verbose=1)\\n    meta_test = pd.concat([meta_test, pd.DataFrame(prob)], axis = 1)\")\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2103, in run_cell_magic\n    result = fn(magic_arg_s, cell)\n  File \"<decorator-gen-62>\", line 2, in time\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/IPython/core/magic.py\", line 187, in <lambda>\n    call = lambda f, *a, **k: f(*a, **k)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/IPython/core/magics/execution.py\", line 1215, in time\n    exec(code, glob, local_ns)\n  File \"<timed exec>\", line 34, in <module>\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 1682, in fit\n    self._make_train_function()\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 992, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/keras/optimizers.py\", line 445, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/keras/optimizers.py\", line 78, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2519, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 375, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py\", line 963, in _MatMulGrad\n    grad_b = math_ops.matmul(a, grad, transpose_a=True)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 2022, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2516, in _mat_mul\n    name=name)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'dense_1/MatMul', defined at:\n  File \"/home/libo/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 23 identical lines from previous traceback]\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/IPython/core/magics/execution.py\", line 1215, in time\n    exec(code, glob, local_ns)\n  File \"<timed exec>\", line 30, in <module>\n  File \"<ipython-input-33-73f26ce9c823>\", line 29, in RNN\n    merged = Dense(400, activation='relu')(merged)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/keras/layers/core.py\", line 877, in call\n    output = K.dot(inputs, self.kernel)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 1076, in dot\n    out = tf.matmul(x, y)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 2022, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2516, in _mat_mul\n    name=name)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/home/libo/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[600000,400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training/Adam/gradients/dense_1/MatMul_grad/MatMul_1 = MatMul[T=DT_FLOAT, _class=[\"loc:@dense_1/MatMul\"], transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_1/cond/Merge, training/Adam/gradients/dense_1/Relu_grad/ReluGrad)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#logloss_list = []\n",
    "for i in range(10):\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    print('*******%s*******'%i)\n",
    "    idx_train = np.array(index[index['fold'] != i]['id'])\n",
    "    idx_val = np.array(index[index['fold'] == i]['id'])\n",
    "    \n",
    "    \n",
    "    X_train = train_FE.loc[idx_train, :]\n",
    "    y_train = categorical_labels[idx_train]\n",
    "\n",
    "    X_val = train_FE.loc[idx_val, :]\n",
    "    y_val = categorical_labels[idx_val]\n",
    "\n",
    "    #dtest = test.loc[:,user_col]\n",
    "    \n",
    "    X_train = get_input(X_train)\n",
    "    X_val = get_input(X_val)\n",
    "    X_test = get_input(test_FE)\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "    bst_model_path = '10folds_' + 'base_rnn_wrod_v1_300dim' + str(i) + '.hdf5'\n",
    "    early_stopping =EarlyStopping(monitor='val_acc', patience=2)\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "    callbacks = [\n",
    "            early_stopping,\n",
    "            model_checkpoint\n",
    "        ]\n",
    "    model = RNN()\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',\\\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    hist = model.fit(X_train, y_train, \\\n",
    "            validation_data=(X_val, y_val), \\\n",
    "            epochs=20, batch_size=BATCH_SIZE, shuffle=True, callbacks=callbacks)#callbacks=callbacks\n",
    "    \n",
    "    model = RNN()\n",
    "    model.load_weights(bst_model_path)\n",
    "    \n",
    "    pred = pd.DataFrame(model.predict(X_val,batch_size=64,verbose=1))\n",
    "    \n",
    "    for i in range(19):\n",
    "        meta_train.loc[idx_val, 'prob' + str(i)] = pred[i].values\n",
    "    \n",
    "    prob = model.predict(X_test,batch_size=64,verbose=1)\n",
    "    meta_test = pd.concat([meta_test, pd.DataFrame(prob)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1200)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1200, 300)    381438300   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1200, 200)    321600      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1200, 200)    241600      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1200, 400)    0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1200, 1024)   410624      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1200, 19)     19475       dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 382,431,599\n",
      "Trainable params: 993,299\n",
      "Non-trainable params: 381,438,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
