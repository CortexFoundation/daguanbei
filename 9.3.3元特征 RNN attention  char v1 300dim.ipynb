{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "window.load_remote_theme = true\n",
       "var theme_js = \"https://odhk.github.io/hyrule_theme/custom.js\";\n",
       "\n",
       "window.load_local_theme = function(){\n",
       "    var hostname = document.location.hostname\n",
       "    return ((hostname == \"localhost\" || hostname == '127.0.0.1') && !load_remote_theme)\n",
       "}\n",
       "\n",
       "var url = load_local_theme() ? document.location.origin + \"/files/theme/custom.js\" : theme_js\n",
       "\n",
       "$.getScript(url)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "window.load_remote_theme = true\n",
    "var theme_js = \"https://odhk.github.io/hyrule_theme/custom.js\";\n",
    "\n",
    "window.load_local_theme = function(){\n",
    "    var hostname = document.location.hostname\n",
    "    return ((hostname == \"localhost\" || hostname == '127.0.0.1') && !load_remote_theme)\n",
    "}\n",
    "\n",
    "var url = load_local_theme() ? document.location.origin + \"/files/theme/custom.js\" : theme_js\n",
    "\n",
    "$.getScript(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/libo/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "1271461it [01:41, 12520.13it/s]\n",
      "  2%|▏         | 31325/1271460 [00:00<00:03, 313180.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1271460 unique tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1271460/1271460 [00:04<00:00, 303195.73it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import keras\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Lambda, Embedding,constraints,\\\n",
    "Dropout, Activation,GRU,Bidirectional,Subtract, Permute, TimeDistributed, Reshape\n",
    "from keras.layers import Conv1D,Conv2D,MaxPooling2D,GlobalAveragePooling1D,GlobalMaxPooling1D, MaxPooling1D, Flatten\n",
    "from keras.layers import CuDNNGRU, CuDNNLSTM, SpatialDropout1D,Layer, initializers, regularizers\n",
    "from keras.layers.merge import concatenate, Concatenate, Average, Dot, Maximum, Multiply, Subtract\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from keras.activations import softmax\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import *\n",
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy.special import erfinv\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train = pd.read_csv('train_set.csv')\n",
    "test = pd.read_csv('test_set.csv')\n",
    "\n",
    "label = train['class']\n",
    "\n",
    "char_embedding_index = {}\n",
    "f = open('word_embedding_300dim_new2.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0].lower()\n",
    "    embeddings = np.asarray(values[1:], dtype = 'float32')\n",
    "    char_embedding_index[word] = embeddings\n",
    "f.close()\n",
    "\n",
    "len(char_embedding_index)\n",
    "\n",
    "api_embedding_file_path = 'word_embedding_300dim_new2.txt'\n",
    "embedding_dim = 300\n",
    "max_nb_api = len(char_embedding_index) + 1\n",
    "max_sequence_length_api =1000\n",
    "\n",
    "all_word = list(train['word_seg'].values) + list(test['word_seg'].values)\n",
    "\n",
    "all_word_list = []\n",
    "for i in list(all_word):\n",
    "    all_word_list.append(i.split(' '))\n",
    "\n",
    "len(all_word_list)\n",
    "\n",
    "train['word_seg'].apply(lambda x:len(x.split(' '))).describe()\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "tokenizer_api = Tokenizer()\n",
    "\n",
    "tokenizer_api.fit_on_texts(all_word_list)\n",
    "\n",
    "api_seq = tokenizer_api.texts_to_sequences(all_word_list)\n",
    "\n",
    "data['word_seq'] = pad_sequences(api_seq, maxlen = max_sequence_length_api, padding='post').tolist()\n",
    "\n",
    "api_index = tokenizer_api.word_index\n",
    "print('Found %s unique tokens' % len(api_index))\n",
    "nb_words = min(max_nb_api, len(char_embedding_index)+1)\n",
    "api_embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "\n",
    "for word, i in tqdm(api_index.items()):\n",
    "    embedding_vector = char_embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        api_embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        print('no char%s'%word)\n",
    "\n",
    "data['id'] = train['id']\n",
    "\n",
    "train_FE = data.iloc[:len(label),:]\n",
    "test_FE = data.iloc[len(label):,:]\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "def get_input(df):\n",
    "    \n",
    "    _input = [np.array(df.word_seq.values.tolist())]\n",
    "    \n",
    "    \n",
    "    return _input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention(x):\n",
    "    \n",
    "    q1, q2 = x[0], x[1]\n",
    "    \n",
    "    #compute match matrix euclidean match score\n",
    "    match_matrix = 1. + K.sqrt(\n",
    "        -2 * K.batch_dot(q1, q2, axes=[2, 2]) +\n",
    "        K.expand_dims(K.sum(K.square(q1), axis=2), 2) +\n",
    "        K.expand_dims(K.sum(K.square(q2), axis=2), 1)\n",
    "    )\n",
    "    \n",
    "    match_matrix = K.maximum(match_matrix, K.epsilon())\n",
    "    match_matrix = 1. / match_matrix\n",
    "    #compute attention output\n",
    "    q1_new = K.batch_dot(K.softmax(match_matrix, axis = 1), q2)\n",
    "    match_matrix_T = Permute((2, 1))(match_matrix)\n",
    "    q2_new = K.batch_dot(K.softmax(match_matrix_T, axis = 1), q1)\n",
    "    \n",
    "    return [q1_new, q2_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Globel_attention_DR_Stack_RNN():\n",
    "    \n",
    "    #Input layer\n",
    "    char_input = Input(shape=(max_sequence_length_api,), dtype='int32')\n",
    "    \n",
    "    api_embedding_layer = Embedding(nb_words,\n",
    "        embedding_dim,\n",
    "        weights=[api_embedding_matrix],\n",
    "        input_length=max_sequence_length_api,\n",
    "        trainable=False)\n",
    "    \n",
    "    #RNN\n",
    "    char_rnn_layer_1 = Bidirectional(CuDNNLSTM(150, \n",
    "                               return_sequences=True))\n",
    "    char_rnn_layer_2 = Bidirectional(CuDNNLSTM(150, \n",
    "                               return_sequences=True))\n",
    "    \n",
    "    char_embedding = api_embedding_layer(char_input)\n",
    "    char_embedding = SpatialDropout1D(0.2)(char_embedding)\n",
    "    \n",
    "    #双层BiLSTM\n",
    "    char_rnn_1= char_rnn_layer_1(char_embedding)\n",
    "    char_rnn_2= char_rnn_layer_2(char_rnn_1)\n",
    "    \n",
    "    #max avg \n",
    "    char_att_maxpooling_1 = GlobalMaxPooling1D()(char_rnn_1)\n",
    "    char_att_avgpooling_1 = GlobalAveragePooling1D()(char_rnn_1)\n",
    "    \n",
    "    char_att_maxpooling_2 = GlobalMaxPooling1D()(char_rnn_2)\n",
    "    char_att_avgpooling_2 = GlobalAveragePooling1D()(char_rnn_2)\n",
    "    \n",
    "    #batchnormalization\n",
    "    char_embedding = BatchNormalization()(char_embedding)\n",
    "    char_rnn_2 = BatchNormalization()(char_rnn_2)\n",
    "    \n",
    "    #globel attention information\n",
    "    r = Lambda(attention, name = 'attention')([char_embedding, char_rnn_2])\n",
    "    \n",
    "    attention_map_left = r[0]\n",
    "    attention_map_right = r[1]\n",
    "    \n",
    "    att_maxpooling_left = GlobalMaxPooling1D()(attention_map_left)\n",
    "    att_maxpooling_right = GlobalMaxPooling1D()(attention_map_right)\n",
    "    \n",
    "    att_avgpooling_left = GlobalAveragePooling1D()(attention_map_left)\n",
    "    att_avgpooling_right = GlobalAveragePooling1D()(attention_map_right)\n",
    "   \n",
    "    max_sub_char_abs_attention = Lambda(lambda x:K.abs(x[0] - x[1]))([att_maxpooling_left, att_maxpooling_right])\n",
    "    max_multi_char_attention = Lambda(lambda x: x[0] * x[1])([att_maxpooling_left, att_maxpooling_right])\n",
    "\n",
    "    avg_sub_char_abs_attention = Lambda(lambda x:K.abs(x[0] - x[1]))([att_avgpooling_left, att_avgpooling_right])\n",
    "    avg_multi_char_attention = Lambda(lambda x: x[0] * x[1])([att_avgpooling_left, att_avgpooling_right])\n",
    "\n",
    "    merged = concatenate([char_att_maxpooling_1, char_att_avgpooling_1, \\\n",
    "                          char_att_maxpooling_2, char_att_avgpooling_2,\\\n",
    "                         max_sub_char_abs_attention, max_multi_char_attention, \\\n",
    "                         avg_sub_char_abs_attention, avg_multi_char_attention, \\\n",
    "                         att_maxpooling_left, att_maxpooling_right, \\\n",
    "                         att_avgpooling_left, att_avgpooling_right])\n",
    "    merged = Dropout(0.4)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dense(512, activation='relu')(merged)\n",
    "    merged = Dropout(0.1)(merged)\n",
    "    merged = BatchNormalization()(merged) \n",
    "    preds = Dense(19, activation = 'softmax')(merged)\n",
    "    \n",
    "    model = Model(inputs=[char_input],outputs=preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train = pd.DataFrame(np.array([None]*train_FE.shape[0]*19).reshape(train_FE.shape[0], 19))\n",
    "meta_train.columns = ['prob' + str(x) for x in range(19)]\n",
    "meta_test = pd.DataFrame()\n",
    "\n",
    "label = train['class']\n",
    "\n",
    "label = label - 1\n",
    "\n",
    "categorical_labels = keras.utils.np_utils.to_categorical(label, num_classes=19)\n",
    "\n",
    "index = pd.read_csv('fold.csv')\n",
    "index['id'] = train['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train, test, all_word\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******0*******\n",
      "Train on 92040 samples, validate on 10237 samples\n",
      "Epoch 1/13\n",
      "92040/92040 [==============================] - 568s 6ms/step - loss: 1.1911 - acc: 0.6562 - val_loss: 0.8888 - val_acc: 0.7348\n",
      "Epoch 2/13\n",
      "92040/92040 [==============================] - 557s 6ms/step - loss: 0.9251 - acc: 0.7281 - val_loss: 0.8385 - val_acc: 0.7514\n",
      "Epoch 3/13\n",
      "92040/92040 [==============================] - 556s 6ms/step - loss: 0.8498 - acc: 0.7488 - val_loss: 0.8050 - val_acc: 0.7594\n",
      "Epoch 4/13\n",
      "92040/92040 [==============================] - 556s 6ms/step - loss: 0.8060 - acc: 0.7592 - val_loss: 0.7763 - val_acc: 0.7716\n",
      "Epoch 5/13\n",
      "92040/92040 [==============================] - 555s 6ms/step - loss: 0.7754 - acc: 0.7671 - val_loss: 0.7492 - val_acc: 0.7755\n",
      "Epoch 6/13\n",
      "92040/92040 [==============================] - 554s 6ms/step - loss: 0.7402 - acc: 0.7787 - val_loss: 0.7490 - val_acc: 0.7760\n",
      "Epoch 7/13\n",
      "92040/92040 [==============================] - 555s 6ms/step - loss: 0.7145 - acc: 0.7832 - val_loss: 0.7324 - val_acc: 0.7811\n",
      "Epoch 8/13\n",
      "92040/92040 [==============================] - 555s 6ms/step - loss: 0.6867 - acc: 0.7917 - val_loss: 0.7446 - val_acc: 0.7781\n",
      "Epoch 9/13\n",
      "92040/92040 [==============================] - 584s 6ms/step - loss: 0.6619 - acc: 0.7958 - val_loss: 0.7310 - val_acc: 0.7826\n",
      "Epoch 10/13\n",
      "92040/92040 [==============================] - 584s 6ms/step - loss: 0.6407 - acc: 0.8026 - val_loss: 0.7275 - val_acc: 0.7820\n",
      "Epoch 11/13\n",
      "92040/92040 [==============================] - 585s 6ms/step - loss: 0.6140 - acc: 0.8106 - val_loss: 0.7236 - val_acc: 0.7864\n",
      "Epoch 12/13\n",
      "92040/92040 [==============================] - 581s 6ms/step - loss: 0.5955 - acc: 0.8158 - val_loss: 0.7235 - val_acc: 0.7827\n",
      "Epoch 13/13\n",
      "92040/92040 [==============================] - 583s 6ms/step - loss: 0.5697 - acc: 0.8222 - val_loss: 0.7341 - val_acc: 0.7875\n",
      "10237/10237 [==============================] - 19s 2ms/step\n",
      "102277/102277 [==============================] - 184s 2ms/step\n",
      "*******1*******\n",
      "Train on 92043 samples, validate on 10234 samples\n",
      "Epoch 1/13\n",
      "92043/92043 [==============================] - 593s 6ms/step - loss: 1.1866 - acc: 0.6561 - val_loss: 0.9264 - val_acc: 0.7301\n",
      "Epoch 2/13\n",
      "92043/92043 [==============================] - 582s 6ms/step - loss: 0.9214 - acc: 0.7287 - val_loss: 0.8621 - val_acc: 0.7509\n",
      "Epoch 3/13\n",
      "92043/92043 [==============================] - 583s 6ms/step - loss: 0.8461 - acc: 0.7490 - val_loss: 0.8271 - val_acc: 0.7543\n",
      "Epoch 4/13\n",
      "92043/92043 [==============================] - 582s 6ms/step - loss: 0.8011 - acc: 0.7622 - val_loss: 0.8083 - val_acc: 0.7635\n",
      "Epoch 5/13\n",
      "92043/92043 [==============================] - 584s 6ms/step - loss: 0.7687 - acc: 0.7699 - val_loss: 0.7969 - val_acc: 0.7618\n",
      "Epoch 6/13\n",
      "92043/92043 [==============================] - 584s 6ms/step - loss: 0.7365 - acc: 0.7761 - val_loss: 0.7766 - val_acc: 0.7662\n",
      "Epoch 7/13\n",
      "92043/92043 [==============================] - 584s 6ms/step - loss: 0.7108 - acc: 0.7847 - val_loss: 0.7695 - val_acc: 0.7719\n",
      "Epoch 8/13\n",
      "92043/92043 [==============================] - 588s 6ms/step - loss: 0.6848 - acc: 0.7925 - val_loss: 0.7824 - val_acc: 0.7636\n",
      "Epoch 9/13\n",
      "92043/92043 [==============================] - 571s 6ms/step - loss: 0.6593 - acc: 0.7988 - val_loss: 0.7660 - val_acc: 0.7702\n",
      "Epoch 10/13\n",
      "92043/92043 [==============================] - 557s 6ms/step - loss: 0.6390 - acc: 0.8038 - val_loss: 0.7629 - val_acc: 0.7774\n",
      "Epoch 11/13\n",
      "92043/92043 [==============================] - 557s 6ms/step - loss: 0.6143 - acc: 0.8093 - val_loss: 0.7778 - val_acc: 0.7781\n",
      "Epoch 12/13\n",
      "92043/92043 [==============================] - 556s 6ms/step - loss: 0.5935 - acc: 0.8154 - val_loss: 0.7775 - val_acc: 0.7753\n",
      "Epoch 13/13\n",
      "92043/92043 [==============================] - 556s 6ms/step - loss: 0.5712 - acc: 0.8219 - val_loss: 0.7684 - val_acc: 0.7773\n",
      "10234/10234 [==============================] - 18s 2ms/step\n",
      "102277/102277 [==============================] - 177s 2ms/step\n",
      "*******2*******\n",
      "Train on 92044 samples, validate on 10233 samples\n",
      "Epoch 1/13\n",
      "92044/92044 [==============================] - 566s 6ms/step - loss: 1.2019 - acc: 0.6538 - val_loss: 0.9234 - val_acc: 0.7318\n",
      "Epoch 2/13\n",
      "92044/92044 [==============================] - 555s 6ms/step - loss: 0.9280 - acc: 0.7249 - val_loss: 0.8798 - val_acc: 0.7415\n",
      "Epoch 3/13\n",
      "92044/92044 [==============================] - 556s 6ms/step - loss: 0.8569 - acc: 0.7460 - val_loss: 0.8134 - val_acc: 0.7594\n",
      "Epoch 4/13\n",
      "92044/92044 [==============================] - 556s 6ms/step - loss: 0.8103 - acc: 0.7599 - val_loss: 0.7748 - val_acc: 0.7662\n",
      "Epoch 5/13\n",
      "92044/92044 [==============================] - 556s 6ms/step - loss: 0.7748 - acc: 0.7676 - val_loss: 0.7453 - val_acc: 0.7783\n",
      "Epoch 6/13\n",
      "92044/92044 [==============================] - 555s 6ms/step - loss: 0.7448 - acc: 0.7770 - val_loss: 0.7462 - val_acc: 0.7785\n",
      "Epoch 7/13\n",
      "92044/92044 [==============================] - 555s 6ms/step - loss: 0.7200 - acc: 0.7829 - val_loss: 0.7398 - val_acc: 0.7773\n",
      "Epoch 8/13\n",
      "92044/92044 [==============================] - 556s 6ms/step - loss: 0.6925 - acc: 0.7898 - val_loss: 0.7313 - val_acc: 0.7831\n",
      "Epoch 9/13\n",
      "92044/92044 [==============================] - 555s 6ms/step - loss: 0.6707 - acc: 0.7943 - val_loss: 0.7271 - val_acc: 0.7807\n",
      "Epoch 10/13\n",
      "92044/92044 [==============================] - 557s 6ms/step - loss: 0.6441 - acc: 0.8021 - val_loss: 0.7349 - val_acc: 0.7829\n",
      "Epoch 11/13\n",
      "92044/92044 [==============================] - 572s 6ms/step - loss: 0.6227 - acc: 0.8074 - val_loss: 0.7033 - val_acc: 0.7946\n",
      "Epoch 12/13\n",
      "92044/92044 [==============================] - 578s 6ms/step - loss: 0.5998 - acc: 0.8142 - val_loss: 0.7182 - val_acc: 0.7948\n",
      "Epoch 13/13\n",
      "92044/92044 [==============================] - 579s 6ms/step - loss: 0.5803 - acc: 0.8180 - val_loss: 0.7136 - val_acc: 0.7906\n",
      "10233/10233 [==============================] - 19s 2ms/step\n",
      "102277/102277 [==============================] - 184s 2ms/step\n",
      "92046/92046 [==============================] - 577s 6ms/step - loss: 0.7446 - acc: 0.7764 - val_loss: 0.7478 - val_acc: 0.7778\n",
      "Epoch 7/13\n",
      "92046/92046 [==============================] - 577s 6ms/step - loss: 0.7157 - acc: 0.7840 - val_loss: 0.7376 - val_acc: 0.7801\n",
      "Epoch 8/13\n",
      "92046/92046 [==============================] - 581s 6ms/step - loss: 0.6905 - acc: 0.7907 - val_loss: 0.7503 - val_acc: 0.7783\n",
      "Epoch 9/13\n",
      "92046/92046 [==============================] - 576s 6ms/step - loss: 0.6660 - acc: 0.7972 - val_loss: 0.7214 - val_acc: 0.7856\n",
      "Epoch 10/13\n",
      "92046/92046 [==============================] - 585s 6ms/step - loss: 0.6416 - acc: 0.8028 - val_loss: 0.7216 - val_acc: 0.7873\n",
      "Epoch 11/13\n",
      "92046/92046 [==============================] - 586s 6ms/step - loss: 0.6173 - acc: 0.8094 - val_loss: 0.7249 - val_acc: 0.7843\n",
      "Epoch 12/13\n",
      "92046/92046 [==============================] - 582s 6ms/step - loss: 0.5941 - acc: 0.8147 - val_loss: 0.7224 - val_acc: 0.7887\n",
      "Epoch 13/13\n",
      "92046/92046 [==============================] - 578s 6ms/step - loss: 0.5732 - acc: 0.8208 - val_loss: 0.7373 - val_acc: 0.7866\n",
      "10231/10231 [==============================] - 18s 2ms/step\n",
      "102277/102277 [==============================] - 182s 2ms/step\n",
      "*******4*******\n",
      "Train on 92049 samples, validate on 10228 samples\n",
      "Epoch 1/13\n",
      "92049/92049 [==============================] - 589s 6ms/step - loss: 1.1912 - acc: 0.6548 - val_loss: 0.9364 - val_acc: 0.7215\n",
      "Epoch 2/13\n",
      "92049/92049 [==============================] - 578s 6ms/step - loss: 0.9246 - acc: 0.7268 - val_loss: 0.8386 - val_acc: 0.7498\n",
      "Epoch 3/13\n",
      "92049/92049 [==============================] - 579s 6ms/step - loss: 0.8518 - acc: 0.7484 - val_loss: 0.8370 - val_acc: 0.7457\n",
      "Epoch 4/13\n",
      "92049/92049 [==============================] - 579s 6ms/step - loss: 0.8034 - acc: 0.7611 - val_loss: 0.7641 - val_acc: 0.7748\n",
      "Epoch 5/13\n",
      "92049/92049 [==============================] - 578s 6ms/step - loss: 0.7700 - acc: 0.7687 - val_loss: 0.7458 - val_acc: 0.7760\n",
      "Epoch 6/13\n",
      "92049/92049 [==============================] - 578s 6ms/step - loss: 0.7386 - acc: 0.7778 - val_loss: 0.7659 - val_acc: 0.7725\n",
      "Epoch 7/13\n",
      "92049/92049 [==============================] - 583s 6ms/step - loss: 0.7138 - acc: 0.7844 - val_loss: 0.7302 - val_acc: 0.7780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/13\n",
      "92049/92049 [==============================] - 576s 6ms/step - loss: 0.6888 - acc: 0.7905 - val_loss: 0.7249 - val_acc: 0.7825\n",
      "Epoch 9/13\n",
      "92049/92049 [==============================] - 577s 6ms/step - loss: 0.6635 - acc: 0.7974 - val_loss: 0.7248 - val_acc: 0.7806\n",
      "Epoch 10/13\n",
      "92049/92049 [==============================] - 578s 6ms/step - loss: 0.6398 - acc: 0.8029 - val_loss: 0.7102 - val_acc: 0.7852\n",
      "Epoch 11/13\n",
      "92049/92049 [==============================] - 578s 6ms/step - loss: 0.6185 - acc: 0.8085 - val_loss: 0.7180 - val_acc: 0.7855\n",
      "Epoch 12/13\n",
      "92049/92049 [==============================] - 578s 6ms/step - loss: 0.5952 - acc: 0.8157 - val_loss: 0.7168 - val_acc: 0.7852\n",
      "Epoch 13/13\n",
      "92049/92049 [==============================] - 578s 6ms/step - loss: 0.5755 - acc: 0.8195 - val_loss: 0.7246 - val_acc: 0.7871\n",
      "10228/10228 [==============================] - 18s 2ms/step\n",
      "102277/102277 [==============================] - 182s 2ms/step\n",
      "*******5*******\n",
      "Train on 92051 samples, validate on 10226 samples\n",
      "Epoch 1/13\n",
      "92051/92051 [==============================] - 588s 6ms/step - loss: 1.1887 - acc: 0.6564 - val_loss: 0.9422 - val_acc: 0.7224\n",
      "Epoch 2/13\n",
      "92051/92051 [==============================] - 578s 6ms/step - loss: 0.9260 - acc: 0.7272 - val_loss: 0.8412 - val_acc: 0.7547\n",
      "Epoch 3/13\n",
      "92051/92051 [==============================] - 578s 6ms/step - loss: 0.8492 - acc: 0.7479 - val_loss: 0.7942 - val_acc: 0.7641\n",
      "Epoch 4/13\n",
      "92051/92051 [==============================] - 577s 6ms/step - loss: 0.8040 - acc: 0.7609 - val_loss: 0.8003 - val_acc: 0.7590\n",
      "Epoch 5/13\n",
      "92051/92051 [==============================] - 582s 6ms/step - loss: 0.7710 - acc: 0.7686 - val_loss: 0.7582 - val_acc: 0.7745\n",
      "Epoch 6/13\n",
      "92051/92051 [==============================] - 577s 6ms/step - loss: 0.7394 - acc: 0.7775 - val_loss: 0.7592 - val_acc: 0.7724\n",
      "Epoch 7/13\n",
      "92051/92051 [==============================] - 578s 6ms/step - loss: 0.7116 - acc: 0.7842 - val_loss: 0.7531 - val_acc: 0.7751\n",
      "Epoch 8/13\n",
      "92051/92051 [==============================] - 577s 6ms/step - loss: 0.6847 - acc: 0.7911 - val_loss: 0.7463 - val_acc: 0.7777\n",
      "Epoch 9/13\n",
      "92051/92051 [==============================] - 578s 6ms/step - loss: 0.6622 - acc: 0.7966 - val_loss: 0.7544 - val_acc: 0.7763\n",
      "Epoch 10/13\n",
      "92051/92051 [==============================] - 578s 6ms/step - loss: 0.6359 - acc: 0.8030 - val_loss: 0.7469 - val_acc: 0.7811\n",
      "Epoch 11/13\n",
      "92051/92051 [==============================] - 578s 6ms/step - loss: 0.6140 - acc: 0.8097 - val_loss: 0.7512 - val_acc: 0.7813\n",
      "Epoch 12/13\n",
      "92051/92051 [==============================] - 577s 6ms/step - loss: 0.5906 - acc: 0.8160 - val_loss: 0.7277 - val_acc: 0.7890\n",
      "Epoch 13/13\n",
      "92051/92051 [==============================] - 578s 6ms/step - loss: 0.5696 - acc: 0.8223 - val_loss: 0.7468 - val_acc: 0.7826\n",
      "10226/10226 [==============================] - 18s 2ms/step\n",
      "102277/102277 [==============================] - 181s 2ms/step\n",
      "*******6*******\n",
      "Train on 92053 samples, validate on 10224 samples\n",
      "Epoch 1/13\n",
      "92053/92053 [==============================] - 591s 6ms/step - loss: 1.1990 - acc: 0.6536 - val_loss: 0.8918 - val_acc: 0.7377\n",
      "Epoch 2/13\n",
      "92053/92053 [==============================] - 577s 6ms/step - loss: 0.9301 - acc: 0.7259 - val_loss: 0.8584 - val_acc: 0.7539\n",
      "Epoch 3/13\n",
      "92053/92053 [==============================] - 577s 6ms/step - loss: 0.8588 - acc: 0.7453 - val_loss: 0.8005 - val_acc: 0.7652\n",
      "Epoch 4/13\n",
      "92053/92053 [==============================] - 577s 6ms/step - loss: 0.8150 - acc: 0.7574 - val_loss: 0.7552 - val_acc: 0.7779\n",
      "Epoch 5/13\n",
      "92053/92053 [==============================] - 578s 6ms/step - loss: 0.7818 - acc: 0.7661 - val_loss: 0.7330 - val_acc: 0.7835\n",
      "Epoch 6/13\n",
      "92053/92053 [==============================] - 578s 6ms/step - loss: 0.7474 - acc: 0.7772 - val_loss: 0.7265 - val_acc: 0.7844\n",
      "Epoch 7/13\n",
      "92053/92053 [==============================] - 578s 6ms/step - loss: 0.7225 - acc: 0.7806 - val_loss: 0.7197 - val_acc: 0.7884\n",
      "Epoch 8/13\n",
      "92053/92053 [==============================] - 578s 6ms/step - loss: 0.6916 - acc: 0.7899 - val_loss: 0.7297 - val_acc: 0.7850\n",
      "Epoch 9/13\n",
      "92053/92053 [==============================] - 579s 6ms/step - loss: 0.6702 - acc: 0.7949 - val_loss: 0.7026 - val_acc: 0.7933\n",
      "Epoch 10/13\n",
      "92053/92053 [==============================] - 578s 6ms/step - loss: 0.6433 - acc: 0.8006 - val_loss: 0.7045 - val_acc: 0.7945\n",
      "Epoch 11/13\n",
      "92053/92053 [==============================] - 583s 6ms/step - loss: 0.6197 - acc: 0.8074 - val_loss: 0.7039 - val_acc: 0.7943\n",
      "Epoch 12/13\n",
      "92053/92053 [==============================] - 576s 6ms/step - loss: 0.5983 - acc: 0.8139 - val_loss: 0.7084 - val_acc: 0.7933\n",
      "Epoch 13/13\n",
      "92053/92053 [==============================] - 577s 6ms/step - loss: 0.5771 - acc: 0.8204 - val_loss: 0.6990 - val_acc: 0.7986\n",
      "10224/10224 [==============================] - 18s 2ms/step\n",
      "102277/102277 [==============================] - 182s 2ms/step\n",
      "*******7*******\n",
      "Train on 92054 samples, validate on 10223 samples\n",
      "Epoch 1/13\n",
      "92054/92054 [==============================] - 588s 6ms/step - loss: 1.1929 - acc: 0.6548 - val_loss: 0.9920 - val_acc: 0.6988\n",
      "Epoch 2/13\n",
      "92054/92054 [==============================] - 577s 6ms/step - loss: 0.9241 - acc: 0.7274 - val_loss: 0.8652 - val_acc: 0.7511\n",
      "Epoch 3/13\n",
      "92054/92054 [==============================] - 577s 6ms/step - loss: 0.8525 - acc: 0.7477 - val_loss: 0.8255 - val_acc: 0.7599\n",
      "Epoch 4/13\n",
      "92054/92054 [==============================] - 578s 6ms/step - loss: 0.8050 - acc: 0.7585 - val_loss: 0.7874 - val_acc: 0.7699\n",
      "Epoch 5/13\n",
      "92054/92054 [==============================] - 578s 6ms/step - loss: 0.7680 - acc: 0.7697 - val_loss: 0.7851 - val_acc: 0.7714\n",
      "Epoch 6/13\n",
      "92054/92054 [==============================] - 578s 6ms/step - loss: 0.7420 - acc: 0.7764 - val_loss: 0.7540 - val_acc: 0.7794\n",
      "Epoch 7/13\n",
      "92054/92054 [==============================] - 579s 6ms/step - loss: 0.7131 - acc: 0.7829 - val_loss: 0.7388 - val_acc: 0.7847\n",
      "Epoch 8/13\n",
      "92054/92054 [==============================] - 579s 6ms/step - loss: 0.6883 - acc: 0.7900 - val_loss: 0.7316 - val_acc: 0.7842\n",
      "Epoch 9/13\n",
      "92054/92054 [==============================] - 577s 6ms/step - loss: 0.6655 - acc: 0.7962 - val_loss: 0.7368 - val_acc: 0.7883\n",
      "Epoch 10/13\n",
      "92054/92054 [==============================] - 577s 6ms/step - loss: 0.6391 - acc: 0.8030 - val_loss: 0.7292 - val_acc: 0.7876\n",
      "Epoch 11/13\n",
      "92054/92054 [==============================] - 577s 6ms/step - loss: 0.6147 - acc: 0.8090 - val_loss: 0.7305 - val_acc: 0.7875\n",
      "Epoch 12/13\n",
      "92054/92054 [==============================] - 577s 6ms/step - loss: 0.5964 - acc: 0.8140 - val_loss: 0.7379 - val_acc: 0.7878\n",
      "Epoch 13/13\n",
      "92054/92054 [==============================] - 577s 6ms/step - loss: 0.5723 - acc: 0.8206 - val_loss: 0.7383 - val_acc: 0.7912\n",
      "10223/10223 [==============================] - 18s 2ms/step\n",
      "102277/102277 [==============================] - 181s 2ms/step\n",
      "*******8*******\n",
      "Train on 92056 samples, validate on 10221 samples\n",
      "Epoch 1/13\n",
      "92056/92056 [==============================] - 587s 6ms/step - loss: 1.1955 - acc: 0.6538 - val_loss: 0.8803 - val_acc: 0.7402\n",
      "Epoch 2/13\n",
      "92056/92056 [==============================] - 578s 6ms/step - loss: 0.9279 - acc: 0.7280 - val_loss: 0.8283 - val_acc: 0.7502\n",
      "Epoch 3/13\n",
      "92056/92056 [==============================] - 578s 6ms/step - loss: 0.8556 - acc: 0.7462 - val_loss: 0.7730 - val_acc: 0.7662\n",
      "Epoch 4/13\n",
      "92056/92056 [==============================] - 576s 6ms/step - loss: 0.8142 - acc: 0.7590 - val_loss: 0.7538 - val_acc: 0.7689\n",
      "Epoch 5/13\n",
      "92056/92056 [==============================] - 581s 6ms/step - loss: 0.7776 - acc: 0.7685 - val_loss: 0.7398 - val_acc: 0.7782\n",
      "Epoch 6/13\n",
      "92056/92056 [==============================] - 575s 6ms/step - loss: 0.7465 - acc: 0.7750 - val_loss: 0.7337 - val_acc: 0.7786\n",
      "Epoch 7/13\n",
      "92056/92056 [==============================] - 577s 6ms/step - loss: 0.7217 - acc: 0.7819 - val_loss: 0.7086 - val_acc: 0.7849\n",
      "Epoch 8/13\n",
      "92056/92056 [==============================] - 577s 6ms/step - loss: 0.6955 - acc: 0.7893 - val_loss: 0.7108 - val_acc: 0.7848\n",
      "Epoch 9/13\n",
      "92056/92056 [==============================] - 578s 6ms/step - loss: 0.6718 - acc: 0.7941 - val_loss: 0.7085 - val_acc: 0.7880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/13\n",
      "92056/92056 [==============================] - 578s 6ms/step - loss: 0.6475 - acc: 0.8019 - val_loss: 0.6985 - val_acc: 0.7882\n",
      "Epoch 11/13\n",
      "92056/92056 [==============================] - 577s 6ms/step - loss: 0.6240 - acc: 0.8083 - val_loss: 0.7067 - val_acc: 0.7862\n",
      "Epoch 12/13\n",
      "92056/92056 [==============================] - 577s 6ms/step - loss: 0.6043 - acc: 0.8136 - val_loss: 0.6977 - val_acc: 0.7931\n",
      "Epoch 13/13\n",
      "92056/92056 [==============================] - 578s 6ms/step - loss: 0.5821 - acc: 0.8179 - val_loss: 0.7127 - val_acc: 0.7925\n",
      "10221/10221 [==============================] - 19s 2ms/step\n",
      "102277/102277 [==============================] - 186s 2ms/step\n",
      "*******9*******\n",
      "Train on 92057 samples, validate on 10220 samples\n",
      "Epoch 1/13\n",
      "92057/92057 [==============================] - 586s 6ms/step - loss: 1.2205 - acc: 0.6486 - val_loss: 1.1783 - val_acc: 0.6704\n",
      "Epoch 2/13\n",
      "92057/92057 [==============================] - 576s 6ms/step - loss: 0.9399 - acc: 0.7238 - val_loss: 0.8672 - val_acc: 0.7455\n",
      "Epoch 3/13\n",
      "92057/92057 [==============================] - 577s 6ms/step - loss: 0.8608 - acc: 0.7457 - val_loss: 0.7979 - val_acc: 0.7601\n",
      "Epoch 4/13\n",
      "92057/92057 [==============================] - 578s 6ms/step - loss: 0.8153 - acc: 0.7581 - val_loss: 0.7772 - val_acc: 0.7663\n",
      "Epoch 5/13\n",
      "92057/92057 [==============================] - 577s 6ms/step - loss: 0.7782 - acc: 0.7684 - val_loss: 0.7798 - val_acc: 0.7644\n",
      "Epoch 6/13\n",
      "92057/92057 [==============================] - 577s 6ms/step - loss: 0.7488 - acc: 0.7744 - val_loss: 0.7545 - val_acc: 0.7760\n",
      "Epoch 7/13\n",
      "92057/92057 [==============================] - 577s 6ms/step - loss: 0.7250 - acc: 0.7819 - val_loss: 0.7339 - val_acc: 0.7846\n",
      "Epoch 8/13\n",
      "92057/92057 [==============================] - 577s 6ms/step - loss: 0.7010 - acc: 0.7869 - val_loss: 0.7235 - val_acc: 0.7831\n",
      "Epoch 9/13\n",
      "92032/92057 [============================>.] - ETA: 0s - loss: 0.6776 - acc: 0.7928"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#logloss_list = []\n",
    "for i in range(10):\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    print('*******%s*******'%i)\n",
    "    idx_train = np.array(index[index['fold'] != i]['id'])\n",
    "    idx_val = np.array(index[index['fold'] == i]['id'])\n",
    "    \n",
    "    \n",
    "    X_train = train_FE.loc[idx_train, :]\n",
    "    y_train = categorical_labels[idx_train]\n",
    "\n",
    "    X_val = train_FE.loc[idx_val, :]\n",
    "    y_val = categorical_labels[idx_val]\n",
    "\n",
    "    #dtest = test.loc[:,user_col]\n",
    "    \n",
    "    X_train = get_input(X_train)\n",
    "    X_val = get_input(X_val)\n",
    "    X_test = get_input(test_FE)\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    model = Globel_attention_DR_Stack_RNN()\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',\\\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    hist = model.fit(X_train, y_train, \\\n",
    "            validation_data=(X_val, y_val), \\\n",
    "            epochs=13, batch_size=BATCH_SIZE, shuffle=True)#callbacks=callbacks\n",
    "\n",
    "    \n",
    "    pred = pd.DataFrame(model.predict(X_val,batch_size=128,verbose=1))\n",
    "    \n",
    "    for i in range(19):\n",
    "        meta_train.loc[idx_val, 'prob' + str(i)] = pred[i].values\n",
    "    \n",
    "    prob = model.predict(X_test,batch_size=128,verbose=1)\n",
    "    meta_test = pd.concat([meta_test, pd.DataFrame(prob)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train.columns = ['meta_rnn_att_wrod_v2_dim_' + x for x in meta_train.columns]\n",
    "\n",
    "p = pd.DataFrame()\n",
    "\n",
    "for i in range(19):\n",
    "    p[i] = meta_test[i].mean(axis = 1)\n",
    "\n",
    "p.columns = meta_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train.to_csv('/home/libo/daguan/stack/train_meta_rnn_att_wrod_v2_300dim.csv', index = None ,encoding = 'utf-8')\n",
    "\n",
    "p.to_csv('/home/libo/daguan/stack/test_meta_rnn_att_wrod_v2_300dim.csv', index = None ,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******0*******\n",
      "Train on 92040 samples, validate on 10237 samples\n",
      "Epoch 1/20\n",
      "92040/92040 [==============================] - 575s 6ms/step - loss: 1.1914 - acc: 0.6556 - val_loss: 0.9431 - val_acc: 0.7201\n",
      "Epoch 2/20\n",
      "92040/92040 [==============================] - 559s 6ms/step - loss: 0.9247 - acc: 0.7285 - val_loss: 0.8576 - val_acc: 0.7453\n",
      "Epoch 3/20\n",
      "92040/92040 [==============================] - 555s 6ms/step - loss: 0.8525 - acc: 0.7476 - val_loss: 0.8103 - val_acc: 0.7564\n",
      "Epoch 4/20\n",
      "92040/92040 [==============================] - 556s 6ms/step - loss: 0.8074 - acc: 0.7589 - val_loss: 0.7801 - val_acc: 0.7655\n",
      "Epoch 5/20\n",
      "92040/92040 [==============================] - 556s 6ms/step - loss: 0.7756 - acc: 0.7690 - val_loss: 0.7749 - val_acc: 0.7655\n",
      "Epoch 6/20\n",
      "92040/92040 [==============================] - 553s 6ms/step - loss: 0.7442 - acc: 0.7767 - val_loss: 0.7687 - val_acc: 0.7701\n",
      "Epoch 7/20\n",
      "92040/92040 [==============================] - 554s 6ms/step - loss: 0.7147 - acc: 0.7834 - val_loss: 0.7330 - val_acc: 0.7807\n",
      "Epoch 8/20\n",
      "92040/92040 [==============================] - 554s 6ms/step - loss: 0.6934 - acc: 0.7892 - val_loss: 0.7346 - val_acc: 0.7807\n",
      "Epoch 9/20\n",
      "92040/92040 [==============================] - 554s 6ms/step - loss: 0.6674 - acc: 0.7952 - val_loss: 0.7275 - val_acc: 0.7832\n",
      "Epoch 10/20\n",
      "92040/92040 [==============================] - 554s 6ms/step - loss: 0.6417 - acc: 0.8037 - val_loss: 0.7278 - val_acc: 0.7833\n",
      "Epoch 11/20\n",
      "92040/92040 [==============================] - 554s 6ms/step - loss: 0.6191 - acc: 0.8089 - val_loss: 0.7212 - val_acc: 0.7884\n",
      "Epoch 12/20\n",
      "92040/92040 [==============================] - 555s 6ms/step - loss: 0.5971 - acc: 0.8148 - val_loss: 0.7165 - val_acc: 0.7914\n",
      "Epoch 13/20\n",
      "92040/92040 [==============================] - 554s 6ms/step - loss: 0.5736 - acc: 0.8203 - val_loss: 0.7321 - val_acc: 0.7862\n",
      "Epoch 14/20\n",
      "92040/92040 [==============================] - 554s 6ms/step - loss: 0.5575 - acc: 0.8251 - val_loss: 0.7279 - val_acc: 0.7879\n",
      "10237/10237 [==============================] - 18s 2ms/step\n",
      "102277/102277 [==============================] - 178s 2ms/step\n",
      "*******1*******\n",
      "Train on 92043 samples, validate on 10234 samples\n",
      "Epoch 1/20\n",
      "92043/92043 [==============================] - 555s 6ms/step - loss: 1.1985 - acc: 0.6555 - val_loss: 0.9759 - val_acc: 0.7162\n",
      "Epoch 2/20\n",
      "92043/92043 [==============================] - 554s 6ms/step - loss: 0.9232 - acc: 0.7285 - val_loss: 0.8893 - val_acc: 0.7415\n",
      "Epoch 3/20\n",
      "92043/92043 [==============================] - 555s 6ms/step - loss: 0.8677 - acc: 0.7436 - val_loss: 0.8656 - val_acc: 0.7489\n",
      "Epoch 4/20\n",
      "92043/92043 [==============================] - 554s 6ms/step - loss: 0.8147 - acc: 0.7561 - val_loss: 0.8040 - val_acc: 0.7614\n",
      "Epoch 5/20\n",
      "92043/92043 [==============================] - 554s 6ms/step - loss: 0.7774 - acc: 0.7679 - val_loss: 0.8116 - val_acc: 0.7598\n",
      "Epoch 6/20\n",
      "92043/92043 [==============================] - 554s 6ms/step - loss: 0.7502 - acc: 0.7751 - val_loss: 0.7850 - val_acc: 0.7697\n",
      "Epoch 7/20\n",
      "92043/92043 [==============================] - 554s 6ms/step - loss: 0.7259 - acc: 0.7819 - val_loss: 0.7614 - val_acc: 0.7709\n",
      "Epoch 8/20\n",
      "92043/92043 [==============================] - 554s 6ms/step - loss: 0.6997 - acc: 0.7866 - val_loss: 0.7839 - val_acc: 0.7687\n",
      "Epoch 9/20\n",
      "92043/92043 [==============================] - 554s 6ms/step - loss: 0.6747 - acc: 0.7918 - val_loss: 0.7676 - val_acc: 0.7728\n",
      "Epoch 10/20\n",
      "92043/92043 [==============================] - 554s 6ms/step - loss: 0.6511 - acc: 0.8009 - val_loss: 0.7673 - val_acc: 0.7754\n",
      "Epoch 11/20\n",
      "92043/92043 [==============================] - 554s 6ms/step - loss: 0.6285 - acc: 0.8065 - val_loss: 0.7713 - val_acc: 0.7725\n",
      "Epoch 12/20\n",
      "92043/92043 [==============================] - 554s 6ms/step - loss: 0.6059 - acc: 0.8126 - val_loss: 0.7619 - val_acc: 0.7771\n",
      "Epoch 13/20\n",
      "92043/92043 [==============================] - 553s 6ms/step - loss: 0.5873 - acc: 0.8163 - val_loss: 0.7666 - val_acc: 0.7820\n",
      "Epoch 14/20\n",
      "92043/92043 [==============================] - 554s 6ms/step - loss: 0.5641 - acc: 0.8230 - val_loss: 0.7607 - val_acc: 0.7859\n",
      "Epoch 15/20\n",
      "92043/92043 [==============================] - 554s 6ms/step - loss: 0.5424 - acc: 0.8294 - val_loss: 0.7700 - val_acc: 0.7818\n",
      "Epoch 16/20\n",
      "92043/92043 [==============================] - 554s 6ms/step - loss: 0.5252 - acc: 0.8333 - val_loss: 0.7838 - val_acc: 0.7810\n",
      "10234/10234 [==============================] - 18s 2ms/step\n",
      "102277/102277 [==============================] - 178s 2ms/step\n",
      "*******2*******\n",
      "Train on 92044 samples, validate on 10233 samples\n",
      "Epoch 1/20\n",
      "92044/92044 [==============================] - 557s 6ms/step - loss: 1.1908 - acc: 0.6560 - val_loss: 0.9328 - val_acc: 0.7296\n",
      "Epoch 2/20\n",
      "92044/92044 [==============================] - 554s 6ms/step - loss: 0.9256 - acc: 0.7274 - val_loss: 0.8557 - val_acc: 0.7463\n",
      "Epoch 3/20\n",
      "92044/92044 [==============================] - 554s 6ms/step - loss: 0.8525 - acc: 0.7466 - val_loss: 0.7926 - val_acc: 0.7634\n",
      "Epoch 4/20\n",
      "92044/92044 [==============================] - 554s 6ms/step - loss: 0.8057 - acc: 0.7602 - val_loss: 0.7827 - val_acc: 0.7663\n",
      "Epoch 5/20\n",
      "92044/92044 [==============================] - 573s 6ms/step - loss: 0.7701 - acc: 0.7693 - val_loss: 0.7503 - val_acc: 0.7789\n",
      "Epoch 6/20\n",
      "92044/92044 [==============================] - 562s 6ms/step - loss: 0.7399 - acc: 0.7769 - val_loss: 0.7423 - val_acc: 0.7799\n",
      "Epoch 7/20\n",
      "92044/92044 [==============================] - 563s 6ms/step - loss: 0.7165 - acc: 0.7823 - val_loss: 0.7371 - val_acc: 0.7827\n",
      "Epoch 8/20\n",
      "92044/92044 [==============================] - 562s 6ms/step - loss: 0.6869 - acc: 0.7906 - val_loss: 0.7227 - val_acc: 0.7867\n",
      "Epoch 9/20\n",
      "92044/92044 [==============================] - 564s 6ms/step - loss: 0.6662 - acc: 0.7956 - val_loss: 0.7283 - val_acc: 0.7806\n",
      "Epoch 10/20\n",
      "92044/92044 [==============================] - 564s 6ms/step - loss: 0.6406 - acc: 0.8037 - val_loss: 0.7167 - val_acc: 0.7886\n",
      "Epoch 11/20\n",
      "92044/92044 [==============================] - 563s 6ms/step - loss: 0.6201 - acc: 0.8099 - val_loss: 0.7263 - val_acc: 0.7910\n",
      "Epoch 12/20\n",
      "92044/92044 [==============================] - 563s 6ms/step - loss: 0.5960 - acc: 0.8139 - val_loss: 0.7162 - val_acc: 0.7882\n",
      "Epoch 13/20\n",
      "92044/92044 [==============================] - 563s 6ms/step - loss: 0.5740 - acc: 0.8196 - val_loss: 0.7333 - val_acc: 0.7912\n",
      "Epoch 14/20\n",
      "92044/92044 [==============================] - 564s 6ms/step - loss: 0.5562 - acc: 0.8255 - val_loss: 0.7339 - val_acc: 0.7895\n",
      "Epoch 15/20\n",
      "92044/92044 [==============================] - 557s 6ms/step - loss: 0.5356 - acc: 0.8306 - val_loss: 0.7275 - val_acc: 0.7946\n",
      "Epoch 16/20\n",
      "92044/92044 [==============================] - 555s 6ms/step - loss: 0.5151 - acc: 0.8373 - val_loss: 0.7234 - val_acc: 0.7924\n",
      "Epoch 17/20\n",
      "92044/92044 [==============================] - 555s 6ms/step - loss: 0.4977 - acc: 0.8420 - val_loss: 0.7345 - val_acc: 0.7941\n",
      "10233/10233 [==============================] - 18s 2ms/step\n",
      "102277/102277 [==============================] - 178s 2ms/step\n",
      "*******3*******\n",
      "Train on 92046 samples, validate on 10231 samples\n",
      "Epoch 1/20\n",
      "92046/92046 [==============================] - 557s 6ms/step - loss: 1.1956 - acc: 0.6555 - val_loss: 0.8950 - val_acc: 0.7414\n",
      "Epoch 2/20\n",
      "92046/92046 [==============================] - 554s 6ms/step - loss: 0.9242 - acc: 0.7284 - val_loss: 0.8215 - val_acc: 0.7598\n",
      "Epoch 3/20\n",
      "92046/92046 [==============================] - 554s 6ms/step - loss: 0.8535 - acc: 0.7479 - val_loss: 0.8030 - val_acc: 0.7614\n",
      "Epoch 4/20\n",
      "92046/92046 [==============================] - 555s 6ms/step - loss: 0.8064 - acc: 0.7599 - val_loss: 0.7934 - val_acc: 0.7638\n",
      "Epoch 5/20\n",
      "92046/92046 [==============================] - 554s 6ms/step - loss: 0.7709 - acc: 0.7687 - val_loss: 0.7665 - val_acc: 0.7724\n",
      "Epoch 6/20\n",
      "92046/92046 [==============================] - 554s 6ms/step - loss: 0.7384 - acc: 0.7785 - val_loss: 0.7517 - val_acc: 0.7738\n",
      "Epoch 7/20\n",
      "92046/92046 [==============================] - 554s 6ms/step - loss: 0.7115 - acc: 0.7854 - val_loss: 0.7469 - val_acc: 0.7817\n",
      "Epoch 8/20\n",
      "92046/92046 [==============================] - 554s 6ms/step - loss: 0.6851 - acc: 0.7929 - val_loss: 0.7348 - val_acc: 0.7800\n",
      "Epoch 9/20\n",
      "92046/92046 [==============================] - 555s 6ms/step - loss: 0.6630 - acc: 0.7979 - val_loss: 0.7376 - val_acc: 0.7796\n",
      "10231/10231 [==============================] - 18s 2ms/step\n",
      "102277/102277 [==============================] - 178s 2ms/step\n",
      "*******4*******\n",
      "Train on 92049 samples, validate on 10228 samples\n",
      "Epoch 1/20\n",
      "92049/92049 [==============================] - 557s 6ms/step - loss: 1.2049 - acc: 0.6517 - val_loss: 0.9091 - val_acc: 0.7363\n",
      "Epoch 2/20\n",
      "92049/92049 [==============================] - 554s 6ms/step - loss: 0.9252 - acc: 0.7271 - val_loss: 0.9410 - val_acc: 0.7257\n",
      "Epoch 3/20\n",
      "92049/92049 [==============================] - 554s 6ms/step - loss: 0.8534 - acc: 0.7468 - val_loss: 0.7991 - val_acc: 0.7597\n",
      "Epoch 4/20\n",
      "92049/92049 [==============================] - 555s 6ms/step - loss: 0.8067 - acc: 0.7594 - val_loss: 0.7578 - val_acc: 0.7733\n",
      "Epoch 5/20\n",
      "92049/92049 [==============================] - 556s 6ms/step - loss: 0.7731 - acc: 0.7691 - val_loss: 0.7622 - val_acc: 0.7739\n",
      "Epoch 6/20\n",
      "92049/92049 [==============================] - 553s 6ms/step - loss: 0.5997 - acc: 0.8151 - val_loss: 0.7133 - val_acc: 0.7885\n",
      "Epoch 13/20\n",
      "92049/92049 [==============================] - 552s 6ms/step - loss: 0.5802 - acc: 0.8199 - val_loss: 0.7128 - val_acc: 0.7873\n",
      "Epoch 14/20\n",
      "92049/92049 [==============================] - 553s 6ms/step - loss: 0.5582 - acc: 0.8255 - val_loss: 0.7208 - val_acc: 0.7876\n",
      "10228/10228 [==============================] - 18s 2ms/step\n",
      "102277/102277 [==============================] - 177s 2ms/step\n",
      "*******5*******\n",
      "Train on 92051 samples, validate on 10226 samples\n",
      "Epoch 1/20\n",
      "92051/92051 [==============================] - 554s 6ms/step - loss: 1.2071 - acc: 0.6506 - val_loss: 0.9290 - val_acc: 0.7253\n",
      "Epoch 2/20\n",
      "92051/92051 [==============================] - 570s 6ms/step - loss: 0.9271 - acc: 0.7258 - val_loss: 0.8716 - val_acc: 0.7433\n",
      "Epoch 3/20\n",
      "92051/92051 [==============================] - 580s 6ms/step - loss: 0.8518 - acc: 0.7473 - val_loss: 0.8107 - val_acc: 0.7562\n",
      "Epoch 4/20\n",
      "92051/92051 [==============================] - 579s 6ms/step - loss: 0.8078 - acc: 0.7595 - val_loss: 0.7983 - val_acc: 0.7591\n",
      "Epoch 5/20\n",
      "92051/92051 [==============================] - 579s 6ms/step - loss: 0.7708 - acc: 0.7684 - val_loss: 0.7891 - val_acc: 0.7653\n",
      "Epoch 6/20\n",
      "92051/92051 [==============================] - 580s 6ms/step - loss: 0.7420 - acc: 0.7767 - val_loss: 0.7489 - val_acc: 0.7791\n",
      "Epoch 7/20\n",
      "92051/92051 [==============================] - 580s 6ms/step - loss: 0.7143 - acc: 0.7848 - val_loss: 0.7446 - val_acc: 0.7789\n",
      "Epoch 8/20\n",
      "92051/92051 [==============================] - 580s 6ms/step - loss: 0.6896 - acc: 0.7908 - val_loss: 0.7460 - val_acc: 0.7789\n",
      "10226/10226 [==============================] - 19s 2ms/step\n",
      "102277/102277 [==============================] - 184s 2ms/step\n",
      "*******6*******\n",
      "Train on 92053 samples, validate on 10224 samples\n",
      "Epoch 1/20\n",
      "92053/92053 [==============================] - 579s 6ms/step - loss: 1.2007 - acc: 0.6533 - val_loss: 0.9393 - val_acc: 0.7336\n",
      "Epoch 2/20\n",
      "92053/92053 [==============================] - 581s 6ms/step - loss: 0.9312 - acc: 0.7262 - val_loss: 0.8617 - val_acc: 0.7523\n",
      "Epoch 3/20\n",
      "92053/92053 [==============================] - 580s 6ms/step - loss: 0.8563 - acc: 0.7452 - val_loss: 0.7884 - val_acc: 0.7680\n",
      "Epoch 4/20\n",
      "92053/92053 [==============================] - 581s 6ms/step - loss: 0.8111 - acc: 0.7591 - val_loss: 0.7911 - val_acc: 0.7698\n",
      "Epoch 5/20\n",
      "92053/92053 [==============================] - 580s 6ms/step - loss: 0.7793 - acc: 0.7684 - val_loss: 0.7345 - val_acc: 0.7819\n",
      "Epoch 6/20\n",
      "92053/92053 [==============================] - 580s 6ms/step - loss: 0.7485 - acc: 0.7745 - val_loss: 0.7249 - val_acc: 0.7867\n",
      "Epoch 7/20\n",
      "92053/92053 [==============================] - 581s 6ms/step - loss: 0.7191 - acc: 0.7825 - val_loss: 0.7267 - val_acc: 0.7866\n",
      "Epoch 8/20\n",
      "92053/92053 [==============================] - 579s 6ms/step - loss: 0.6972 - acc: 0.7886 - val_loss: 0.7068 - val_acc: 0.7901\n",
      "Epoch 9/20\n",
      "92053/92053 [==============================] - 580s 6ms/step - loss: 0.6720 - acc: 0.7938 - val_loss: 0.7147 - val_acc: 0.7865\n",
      "Epoch 10/20\n",
      "92053/92053 [==============================] - 581s 6ms/step - loss: 0.6458 - acc: 0.8018 - val_loss: 0.6935 - val_acc: 0.7948\n",
      "Epoch 11/20\n",
      "92053/92053 [==============================] - 580s 6ms/step - loss: 0.6224 - acc: 0.8077 - val_loss: 0.7040 - val_acc: 0.7941\n",
      "Epoch 12/20\n",
      "92053/92053 [==============================] - 580s 6ms/step - loss: 0.6092 - acc: 0.8105 - val_loss: 0.7005 - val_acc: 0.7951\n",
      "Epoch 13/20\n",
      "92053/92053 [==============================] - 578s 6ms/step - loss: 0.5775 - acc: 0.8182 - val_loss: 0.6950 - val_acc: 0.7969\n",
      "Epoch 14/20\n",
      "92053/92053 [==============================] - 580s 6ms/step - loss: 0.5586 - acc: 0.8240 - val_loss: 0.7029 - val_acc: 0.7975\n",
      "Epoch 15/20\n",
      "92053/92053 [==============================] - 581s 6ms/step - loss: 0.5428 - acc: 0.8274 - val_loss: 0.7094 - val_acc: 0.7977\n",
      "Epoch 16/20\n",
      "92053/92053 [==============================] - 581s 6ms/step - loss: 0.5260 - acc: 0.8316 - val_loss: 0.7045 - val_acc: 0.8008\n",
      "Epoch 17/20\n",
      "92053/92053 [==============================] - 580s 6ms/step - loss: 0.5056 - acc: 0.8390 - val_loss: 0.7240 - val_acc: 0.7930\n",
      "Epoch 18/20\n",
      "92053/92053 [==============================] - 581s 6ms/step - loss: 0.4920 - acc: 0.8417 - val_loss: 0.7353 - val_acc: 0.7939\n",
      "10224/10224 [==============================] - 19s 2ms/step\n",
      "102277/102277 [==============================] - 183s 2ms/step\n",
      "*******7*******\n",
      "Train on 92054 samples, validate on 10223 samples\n",
      "Epoch 1/20\n",
      "92054/92054 [==============================] - 582s 6ms/step - loss: 1.2069 - acc: 0.6498 - val_loss: 0.9367 - val_acc: 0.7229\n",
      "Epoch 2/20\n",
      "92054/92054 [==============================] - 580s 6ms/step - loss: 0.9325 - acc: 0.7259 - val_loss: 0.8441 - val_acc: 0.7533\n",
      "Epoch 3/20\n",
      "92054/92054 [==============================] - 579s 6ms/step - loss: 0.8605 - acc: 0.7447 - val_loss: 0.8224 - val_acc: 0.7607\n",
      "Epoch 4/20\n",
      "92054/92054 [==============================] - 579s 6ms/step - loss: 0.8120 - acc: 0.7578 - val_loss: 0.7965 - val_acc: 0.7689\n",
      "Epoch 5/20\n",
      "92054/92054 [==============================] - 578s 6ms/step - loss: 0.7806 - acc: 0.7665 - val_loss: 0.7792 - val_acc: 0.7736\n",
      "Epoch 6/20\n",
      "92054/92054 [==============================] - 580s 6ms/step - loss: 0.7498 - acc: 0.7745 - val_loss: 0.7674 - val_acc: 0.7772\n",
      "Epoch 7/20\n",
      "92054/92054 [==============================] - 580s 6ms/step - loss: 0.7217 - acc: 0.7813 - val_loss: 0.7550 - val_acc: 0.7807\n",
      "Epoch 8/20\n",
      "92054/92054 [==============================] - 577s 6ms/step - loss: 0.6940 - acc: 0.7881 - val_loss: 0.7511 - val_acc: 0.7808\n",
      "Epoch 9/20\n",
      "92054/92054 [==============================] - 579s 6ms/step - loss: 0.6714 - acc: 0.7936 - val_loss: 0.7344 - val_acc: 0.7893\n",
      "Epoch 10/20\n",
      "92054/92054 [==============================] - 580s 6ms/step - loss: 0.6486 - acc: 0.7993 - val_loss: 0.7450 - val_acc: 0.7870\n",
      "Epoch 11/20\n",
      "92054/92054 [==============================] - 580s 6ms/step - loss: 0.6256 - acc: 0.8059 - val_loss: 0.7350 - val_acc: 0.7868\n",
      "10223/10223 [==============================] - 18s 2ms/step\n",
      "102277/102277 [==============================] - 183s 2ms/step\n",
      "*******8*******\n",
      "Train on 92056 samples, validate on 10221 samples\n",
      "Epoch 1/20\n",
      "92056/92056 [==============================] - 581s 6ms/step - loss: 1.1947 - acc: 0.6538 - val_loss: 0.8797 - val_acc: 0.7381\n",
      "Epoch 2/20\n",
      "92056/92056 [==============================] - 580s 6ms/step - loss: 0.9273 - acc: 0.7276 - val_loss: 0.8435 - val_acc: 0.7441\n",
      "Epoch 3/20\n",
      "92056/92056 [==============================] - 579s 6ms/step - loss: 0.8582 - acc: 0.7468 - val_loss: 0.7752 - val_acc: 0.7650\n",
      "Epoch 4/20\n",
      "92056/92056 [==============================] - 581s 6ms/step - loss: 0.8085 - acc: 0.7591 - val_loss: 0.7506 - val_acc: 0.7743\n",
      "Epoch 5/20\n",
      "92056/92056 [==============================] - 580s 6ms/step - loss: 0.7750 - acc: 0.7683 - val_loss: 0.7593 - val_acc: 0.7701\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92056/92056 [==============================] - 581s 6ms/step - loss: 0.7445 - acc: 0.7770 - val_loss: 0.7268 - val_acc: 0.7822\n",
      "Epoch 7/20\n",
      "92056/92056 [==============================] - 581s 6ms/step - loss: 0.7184 - acc: 0.7832 - val_loss: 0.7106 - val_acc: 0.7833\n",
      "Epoch 8/20\n",
      "92056/92056 [==============================] - 581s 6ms/step - loss: 0.6884 - acc: 0.7916 - val_loss: 0.7058 - val_acc: 0.7887\n",
      "Epoch 9/20\n",
      "92056/92056 [==============================] - 580s 6ms/step - loss: 0.6646 - acc: 0.7971 - val_loss: 0.6965 - val_acc: 0.7882\n",
      "Epoch 10/20\n",
      "92056/92056 [==============================] - 580s 6ms/step - loss: 0.6448 - acc: 0.8025 - val_loss: 0.6926 - val_acc: 0.7921\n",
      "Epoch 11/20\n",
      "92056/92056 [==============================] - 580s 6ms/step - loss: 0.6192 - acc: 0.8097 - val_loss: 0.6952 - val_acc: 0.7909\n",
      "Epoch 12/20\n",
      "92056/92056 [==============================] - 581s 6ms/step - loss: 0.5988 - acc: 0.8148 - val_loss: 0.7050 - val_acc: 0.7896\n",
      "10221/10221 [==============================] - 18s 2ms/step\n",
      "102277/102277 [==============================] - 185s 2ms/step\n",
      "*******9*******\n",
      "Train on 92057 samples, validate on 10220 samples\n",
      "Epoch 1/20\n",
      "92057/92057 [==============================] - 582s 6ms/step - loss: 1.1813 - acc: 0.6595 - val_loss: 0.9295 - val_acc: 0.7220\n",
      "Epoch 2/20\n",
      "92057/92057 [==============================] - 578s 6ms/step - loss: 0.9215 - acc: 0.7283 - val_loss: 0.8295 - val_acc: 0.7504\n",
      "Epoch 3/20\n",
      "92057/92057 [==============================] - 580s 6ms/step - loss: 0.8521 - acc: 0.7484 - val_loss: 0.8236 - val_acc: 0.7550\n",
      "Epoch 4/20\n",
      "43456/92057 [=============>................] - ETA: 4:53 - loss: 0.8050 - acc: 0.7621"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#logloss_list = []\n",
    "for i in range(10):\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    print('*******%s*******'%i)\n",
    "    idx_train = np.array(index[index['fold'] != i]['id'])\n",
    "    idx_val = np.array(index[index['fold'] == i]['id'])\n",
    "    \n",
    "    \n",
    "    X_train = train_FE.loc[idx_train, :]\n",
    "    y_train = categorical_labels[idx_train]\n",
    "\n",
    "    X_val = train_FE.loc[idx_val, :]\n",
    "    y_val = categorical_labels[idx_val]\n",
    "\n",
    "    #dtest = test.loc[:,user_col]\n",
    "    \n",
    "    X_train = get_input(X_train)\n",
    "    X_val = get_input(X_val)\n",
    "    X_test = get_input(test_FE)\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "    bst_model_path = '10folds_' + 'RNN_attention_char_v1_300dim' + str(i) + '.hdf5'\n",
    "    early_stopping =EarlyStopping(monitor='val_acc', patience=2)\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "    callbacks = [\n",
    "            early_stopping,\n",
    "            model_checkpoint\n",
    "        ]\n",
    "    model = Globel_attention_DR_Stack_RNN()\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',\\\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    hist = model.fit(X_train, y_train, \\\n",
    "            validation_data=(X_val, y_val), \\\n",
    "            epochs=20, batch_size=BATCH_SIZE, shuffle=True, callbacks=callbacks)#callbacks=callbacks\n",
    "    \n",
    "    model = Globel_attention_DR_Stack_RNN()\n",
    "    model.load_weights(bst_model_path)\n",
    "    \n",
    "    pred = pd.DataFrame(model.predict(X_val,batch_size=128,verbose=1))\n",
    "    \n",
    "    for i in range(19):\n",
    "        meta_train.loc[idx_val, 'prob' + str(i)] = pred[i].values\n",
    "    \n",
    "    prob = model.predict(X_test,batch_size=128,verbose=1)\n",
    "    meta_test = pd.concat([meta_test, pd.DataFrame(prob)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102277, 19)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102277, 190)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train.columns = ['meta_base_rnn_att_wrod_v1_300dim_' + x for x in meta_train.columns]\n",
    "\n",
    "p = pd.DataFrame()\n",
    "\n",
    "for i in range(19):\n",
    "    p[i] = meta_test[i].mean(axis = 1)\n",
    "\n",
    "p.columns = meta_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((102277, 19), (102277, 19))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape, meta_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train.to_csv('/home/libo/daguan/stack_new/train_meta_base_att_rnn_word_v2_300dim.csv', index = None ,encoding = 'utf-8')\n",
    "\n",
    "p.to_csv('/home/libo/daguan/stack_new/test_meta_base_att_rnn_word_v2_300dim.csv', index = None ,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_set.csv')\n",
    "\n",
    "pred = pd.DataFrame(test['id'])\n",
    "\n",
    "pred['class'] = np.argmax(p.values, axis=1) + 1\n",
    "\n",
    "pred.to_csv('/home/libo/daguan/stack_new/9.4.2.csv', index = None ,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Globel_attention_DR_Stack_RNN():\n",
    "    \n",
    "    #Input layer\n",
    "    char_input = Input(shape=(max_sequence_length_api,), dtype='int32')\n",
    "    \n",
    "    api_embedding_layer = Embedding(nb_words,\n",
    "        embedding_dim,\n",
    "        weights=[api_embedding_matrix],\n",
    "        input_length=max_sequence_length_api,\n",
    "        trainable=False)\n",
    "    \n",
    "    #RNN\n",
    "    char_rnn_layer_1 = Bidirectional(CuDNNLSTM(150, \n",
    "                               return_sequences=True))\n",
    "    char_rnn_layer_2 = Bidirectional(CuDNNLSTM(150, \n",
    "                               return_sequences=True))\n",
    "    \n",
    "    char_embedding = api_embedding_layer(char_input)\n",
    "    #char_embedding = SpatialDropout1D(0.2)(char_embedding)\n",
    "    \n",
    "    #双层BiLSTM\n",
    "    char_rnn_1= char_rnn_layer_1(char_embedding)\n",
    "    char_rnn_2= char_rnn_layer_2(char_rnn_1)\n",
    "    \n",
    "    #max avg \n",
    "    char_att_maxpooling_1 = GlobalMaxPooling1D()(char_rnn_1)\n",
    "    char_att_avgpooling_1 = GlobalAveragePooling1D()(char_rnn_1)\n",
    "    \n",
    "    char_att_maxpooling_2 = GlobalMaxPooling1D()(char_rnn_2)\n",
    "    char_att_avgpooling_2 = GlobalAveragePooling1D()(char_rnn_2)\n",
    "    \n",
    "    #batchnormalization\n",
    "    char_embedding = BatchNormalization()(char_embedding)\n",
    "    char_rnn_2 = BatchNormalization()(char_rnn_2)\n",
    "    \n",
    "    #globel attention information\n",
    "    r = Lambda(attention, name = 'attention')([char_embedding, char_rnn_2])\n",
    "    \n",
    "    attention_map_left = r[0]\n",
    "    attention_map_right = r[1]\n",
    "    \n",
    "    att_maxpooling_left = GlobalMaxPooling1D()(attention_map_left)\n",
    "    att_maxpooling_right = GlobalMaxPooling1D()(attention_map_right)\n",
    "    \n",
    "    att_avgpooling_left = GlobalAveragePooling1D()(attention_map_left)\n",
    "    att_avgpooling_right = GlobalAveragePooling1D()(attention_map_right)\n",
    "   \n",
    "    max_sub_char_abs_attention = Lambda(lambda x:K.abs(x[0] - x[1]))([att_maxpooling_left, att_maxpooling_right])\n",
    "    max_multi_char_attention = Lambda(lambda x: x[0] * x[1])([att_maxpooling_left, att_maxpooling_right])\n",
    "\n",
    "    avg_sub_char_abs_attention = Lambda(lambda x:K.abs(x[0] - x[1]))([att_avgpooling_left, att_avgpooling_right])\n",
    "    avg_multi_char_attention = Lambda(lambda x: x[0] * x[1])([att_avgpooling_left, att_avgpooling_right])\n",
    "\n",
    "    merged = concatenate([char_att_maxpooling_1, char_att_avgpooling_1, \\\n",
    "                          char_att_maxpooling_2, char_att_avgpooling_2,\\\n",
    "                         max_sub_char_abs_attention, max_multi_char_attention, \\\n",
    "                         avg_sub_char_abs_attention, avg_multi_char_attention, \\\n",
    "                         att_maxpooling_left, att_maxpooling_right, \\\n",
    "                         att_avgpooling_left, att_avgpooling_right])\n",
    "    merged = Dropout(0.3)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dense(400, activation='relu')(merged)\n",
    "    merged = Dropout(0.2)(merged)\n",
    "    merged = BatchNormalization()(merged) \n",
    "    preds = Dense(19, activation = 'softmax')(merged)\n",
    "    \n",
    "    model = Model(inputs=[char_input],outputs=preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train = pd.DataFrame(np.array([None]*train_FE.shape[0]*19).reshape(train_FE.shape[0], 19))\n",
    "meta_train.columns = ['prob' + str(x) for x in range(19)]\n",
    "meta_test = pd.DataFrame()\n",
    "\n",
    "label = train['class']\n",
    "\n",
    "label = label - 1\n",
    "\n",
    "categorical_labels = keras.utils.np_utils.to_categorical(label, num_classes=19)\n",
    "\n",
    "index = pd.read_csv('fold.csv')\n",
    "index['id'] = train['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******0*******\n",
      "Train on 92040 samples, validate on 10237 samples\n",
      "Epoch 1/13\n",
      "92040/92040 [==============================] - 558s 6ms/step - loss: 1.0630 - acc: 0.6951 - val_loss: 1.0005 - val_acc: 0.7154\n",
      "Epoch 2/13\n",
      "92040/92040 [==============================] - 557s 6ms/step - loss: 0.8400 - acc: 0.7532 - val_loss: 0.8633 - val_acc: 0.7480\n",
      "Epoch 3/13\n",
      "92040/92040 [==============================] - 557s 6ms/step - loss: 0.7764 - acc: 0.7708 - val_loss: 0.9347 - val_acc: 0.7425\n",
      "Epoch 4/13\n",
      "92040/92040 [==============================] - 556s 6ms/step - loss: 0.7270 - acc: 0.7835 - val_loss: 0.7821 - val_acc: 0.7723\n",
      "Epoch 5/13\n",
      "92040/92040 [==============================] - 556s 6ms/step - loss: 0.6890 - acc: 0.7942 - val_loss: 0.8370 - val_acc: 0.7575\n",
      "Epoch 6/13\n",
      "92040/92040 [==============================] - 558s 6ms/step - loss: 0.6499 - acc: 0.8043 - val_loss: 0.7719 - val_acc: 0.7740\n",
      "Epoch 7/13\n",
      "92040/92040 [==============================] - 556s 6ms/step - loss: 0.6127 - acc: 0.8151 - val_loss: 0.8217 - val_acc: 0.7711\n",
      "Epoch 8/13\n",
      "92040/92040 [==============================] - 555s 6ms/step - loss: 0.5759 - acc: 0.8260 - val_loss: 0.8084 - val_acc: 0.7698\n",
      "Epoch 9/13\n",
      "92040/92040 [==============================] - 555s 6ms/step - loss: 0.5391 - acc: 0.8344 - val_loss: 0.8045 - val_acc: 0.7775\n",
      "Epoch 10/13\n",
      "92040/92040 [==============================] - 557s 6ms/step - loss: 0.5047 - acc: 0.8435 - val_loss: 0.8290 - val_acc: 0.7697\n",
      "Epoch 11/13\n",
      "92040/92040 [==============================] - 558s 6ms/step - loss: 0.4692 - acc: 0.8538 - val_loss: 0.8447 - val_acc: 0.7752\n",
      "Epoch 12/13\n",
      "92040/92040 [==============================] - 559s 6ms/step - loss: 0.4377 - acc: 0.8629 - val_loss: 0.8588 - val_acc: 0.7693\n",
      "Epoch 13/13\n",
      "92040/92040 [==============================] - 558s 6ms/step - loss: 0.4081 - acc: 0.8707 - val_loss: 0.8418 - val_acc: 0.7788\n",
      "10237/10237 [==============================] - 18s 2ms/step\n",
      "102277/102277 [==============================] - 177s 2ms/step\n",
      "*******1*******\n",
      "Train on 92043 samples, validate on 10234 samples\n",
      "Epoch 1/13\n",
      "92043/92043 [==============================] - 560s 6ms/step - loss: 1.0665 - acc: 0.6953 - val_loss: 1.2388 - val_acc: 0.6530\n",
      "Epoch 2/13\n",
      "92043/92043 [==============================] - 557s 6ms/step - loss: 0.5405 - acc: 0.8334 - val_loss: 0.8064 - val_acc: 0.7710\n",
      "Epoch 10/13\n",
      "92043/92043 [==============================] - 557s 6ms/step - loss: 0.5067 - acc: 0.8434 - val_loss: 0.8576 - val_acc: 0.7663\n",
      "Epoch 11/13\n",
      "92043/92043 [==============================] - 558s 6ms/step - loss: 0.4717 - acc: 0.8523 - val_loss: 0.8553 - val_acc: 0.7693\n",
      "Epoch 12/13\n",
      "92043/92043 [==============================] - 557s 6ms/step - loss: 0.4383 - acc: 0.8622 - val_loss: 0.8956 - val_acc: 0.7711\n",
      "Epoch 13/13\n",
      "92043/92043 [==============================] - 557s 6ms/step - loss: 0.4051 - acc: 0.8717 - val_loss: 0.9249 - val_acc: 0.7666\n",
      "10234/10234 [==============================] - 18s 2ms/step\n",
      "102277/102277 [==============================] - 177s 2ms/step\n",
      "*******2*******\n",
      "Train on 92044 samples, validate on 10233 samples\n",
      "Epoch 1/13\n",
      "92044/92044 [==============================] - 558s 6ms/step - loss: 1.0681 - acc: 0.6939 - val_loss: 0.9253 - val_acc: 0.7255\n",
      "Epoch 2/13\n",
      "92044/92044 [==============================] - 557s 6ms/step - loss: 0.8414 - acc: 0.7536 - val_loss: 0.8362 - val_acc: 0.7575\n",
      "Epoch 3/13\n",
      "92044/92044 [==============================] - 555s 6ms/step - loss: 0.7767 - acc: 0.7701 - val_loss: 0.7872 - val_acc: 0.7721\n",
      "Epoch 4/13\n",
      "92044/92044 [==============================] - 557s 6ms/step - loss: 0.7305 - acc: 0.7813 - val_loss: 0.8011 - val_acc: 0.7674\n",
      "Epoch 5/13\n",
      "92044/92044 [==============================] - 557s 6ms/step - loss: 0.6915 - acc: 0.7919 - val_loss: 0.7748 - val_acc: 0.7758\n",
      "Epoch 6/13\n",
      "92044/92044 [==============================] - 558s 6ms/step - loss: 0.6502 - acc: 0.8051 - val_loss: 0.8233 - val_acc: 0.7729\n",
      "Epoch 7/13\n",
      "92044/92044 [==============================] - 557s 6ms/step - loss: 0.6143 - acc: 0.8130 - val_loss: 0.7808 - val_acc: 0.7713\n",
      "Epoch 8/13\n",
      "92044/92044 [==============================] - 557s 6ms/step - loss: 0.5765 - acc: 0.8245 - val_loss: 0.7694 - val_acc: 0.7782\n",
      "Epoch 9/13\n",
      "92044/92044 [==============================] - 557s 6ms/step - loss: 0.5408 - acc: 0.8346 - val_loss: 0.7784 - val_acc: 0.7835\n",
      "Epoch 10/13\n",
      "92044/92044 [==============================] - 558s 6ms/step - loss: 0.5054 - acc: 0.8430 - val_loss: 0.8195 - val_acc: 0.7772\n",
      "Epoch 11/13\n",
      "92044/92044 [==============================] - 558s 6ms/step - loss: 0.4733 - acc: 0.8523 - val_loss: 0.8281 - val_acc: 0.7817\n",
      "Epoch 12/13\n",
      "92044/92044 [==============================] - 558s 6ms/step - loss: 0.4351 - acc: 0.8637 - val_loss: 0.9706 - val_acc: 0.7678\n",
      "Epoch 13/13\n",
      "92044/92044 [==============================] - 558s 6ms/step - loss: 0.4076 - acc: 0.8708 - val_loss: 0.8930 - val_acc: 0.7797\n",
      "10233/10233 [==============================] - 18s 2ms/step\n",
      "102277/102277 [==============================] - 176s 2ms/step\n",
      "*******3*******\n",
      "Train on 92046 samples, validate on 10231 samples\n",
      "Epoch 1/13\n",
      "92046/92046 [==============================] - 558s 6ms/step - loss: 1.0686 - acc: 0.6930 - val_loss: 1.0108 - val_acc: 0.7007\n",
      "Epoch 2/13\n",
      "92046/92046 [==============================] - 558s 6ms/step - loss: 0.8426 - acc: 0.7518 - val_loss: 0.8935 - val_acc: 0.7304\n",
      "Epoch 3/13\n",
      "92046/92046 [==============================] - 558s 6ms/step - loss: 0.7762 - acc: 0.7703 - val_loss: 0.8230 - val_acc: 0.7616\n",
      "Epoch 4/13\n",
      "92046/92046 [==============================] - 558s 6ms/step - loss: 0.7281 - acc: 0.7835 - val_loss: 0.8176 - val_acc: 0.7586\n",
      "Epoch 5/13\n",
      "92046/92046 [==============================] - 558s 6ms/step - loss: 0.6892 - acc: 0.7940 - val_loss: 0.7840 - val_acc: 0.7727\n",
      "Epoch 6/13\n",
      "92046/92046 [==============================] - 559s 6ms/step - loss: 0.6496 - acc: 0.8044 - val_loss: 0.8017 - val_acc: 0.7761\n",
      "Epoch 7/13\n",
      "92046/92046 [==============================] - 559s 6ms/step - loss: 0.6164 - acc: 0.8142 - val_loss: 0.8275 - val_acc: 0.7752\n",
      "Epoch 8/13\n",
      "92046/92046 [==============================] - 559s 6ms/step - loss: 0.5773 - acc: 0.8230 - val_loss: 0.7735 - val_acc: 0.7794\n",
      "Epoch 9/13\n",
      "92046/92046 [==============================] - 559s 6ms/step - loss: 0.5410 - acc: 0.8339 - val_loss: 0.8542 - val_acc: 0.7747\n",
      "Epoch 10/13\n",
      "92046/92046 [==============================] - 558s 6ms/step - loss: 0.5072 - acc: 0.8428 - val_loss: 0.8101 - val_acc: 0.7765\n",
      "Epoch 11/13\n",
      "92046/92046 [==============================] - 559s 6ms/step - loss: 0.4691 - acc: 0.8529 - val_loss: 0.8491 - val_acc: 0.7787\n",
      "Epoch 12/13\n",
      "92046/92046 [==============================] - 560s 6ms/step - loss: 0.4382 - acc: 0.8627 - val_loss: 0.8982 - val_acc: 0.7716\n",
      "Epoch 13/13\n",
      "92046/92046 [==============================] - 559s 6ms/step - loss: 0.4083 - acc: 0.8711 - val_loss: 0.9369 - val_acc: 0.7726\n",
      "10231/10231 [==============================] - 18s 2ms/step\n",
      "102277/102277 [==============================] - 177s 2ms/step\n",
      "*******4*******\n",
      "Train on 92049 samples, validate on 10228 samples\n",
      "Epoch 1/13\n",
      "92049/92049 [==============================] - 560s 6ms/step - loss: 1.0659 - acc: 0.6936 - val_loss: 1.0721 - val_acc: 0.6798\n",
      "Epoch 2/13\n",
      "92049/92049 [==============================] - 559s 6ms/step - loss: 0.8445 - acc: 0.7527 - val_loss: 0.9610 - val_acc: 0.7186\n",
      "Epoch 3/13\n",
      "92049/92049 [==============================] - 558s 6ms/step - loss: 0.7771 - acc: 0.7699 - val_loss: 0.8721 - val_acc: 0.7384\n",
      "Epoch 4/13\n",
      "92049/92049 [==============================] - 559s 6ms/step - loss: 0.7302 - acc: 0.7827 - val_loss: 0.8853 - val_acc: 0.7403\n",
      "Epoch 5/13\n",
      "92049/92049 [==============================] - 559s 6ms/step - loss: 0.6889 - acc: 0.7944 - val_loss: 0.8012 - val_acc: 0.7663\n",
      "Epoch 6/13\n",
      "92049/92049 [==============================] - 559s 6ms/step - loss: 0.6488 - acc: 0.8053 - val_loss: 0.7812 - val_acc: 0.7721\n",
      "Epoch 7/13\n",
      "92049/92049 [==============================] - 559s 6ms/step - loss: 0.6143 - acc: 0.8139 - val_loss: 0.7718 - val_acc: 0.7767\n",
      "Epoch 8/13\n",
      "92049/92049 [==============================] - 558s 6ms/step - loss: 0.5740 - acc: 0.8244 - val_loss: 0.8284 - val_acc: 0.7719\n",
      "Epoch 9/13\n",
      "92049/92049 [==============================] - 559s 6ms/step - loss: 0.5361 - acc: 0.8338 - val_loss: 0.8120 - val_acc: 0.7711\n",
      "Epoch 10/13\n",
      "92049/92049 [==============================] - 559s 6ms/step - loss: 0.5033 - acc: 0.8441 - val_loss: 0.8484 - val_acc: 0.7604\n",
      "Epoch 11/13\n",
      "92049/92049 [==============================] - 559s 6ms/step - loss: 0.4670 - acc: 0.8533 - val_loss: 0.8050 - val_acc: 0.7700\n",
      "Epoch 12/13\n",
      "92049/92049 [==============================] - 559s 6ms/step - loss: 0.4338 - acc: 0.8642 - val_loss: 0.8464 - val_acc: 0.7615\n",
      "Epoch 13/13\n",
      "92049/92049 [==============================] - 559s 6ms/step - loss: 0.4023 - acc: 0.8722 - val_loss: 0.9626 - val_acc: 0.7795\n",
      "10228/10228 [==============================] - 18s 2ms/step\n",
      "102277/102277 [==============================] - 177s 2ms/step\n",
      "*******5*******\n",
      "Train on 92051 samples, validate on 10226 samples\n",
      "Epoch 1/13\n",
      "92051/92051 [==============================] - 559s 6ms/step - loss: 1.0645 - acc: 0.6943 - val_loss: 0.9158 - val_acc: 0.7283\n",
      "Epoch 2/13\n",
      "92051/92051 [==============================] - 558s 6ms/step - loss: 0.8416 - acc: 0.7533 - val_loss: 0.8712 - val_acc: 0.7497\n",
      "Epoch 3/13\n",
      "92051/92051 [==============================] - 556s 6ms/step - loss: 0.7741 - acc: 0.7706 - val_loss: 0.8445 - val_acc: 0.7515\n",
      "Epoch 4/13\n",
      "92051/92051 [==============================] - 558s 6ms/step - loss: 0.7306 - acc: 0.7837 - val_loss: 0.7889 - val_acc: 0.7639\n",
      "Epoch 5/13\n",
      "92051/92051 [==============================] - 558s 6ms/step - loss: 0.6873 - acc: 0.7941 - val_loss: 0.7854 - val_acc: 0.7650\n",
      "Epoch 6/13\n",
      "92051/92051 [==============================] - 558s 6ms/step - loss: 0.6483 - acc: 0.8051 - val_loss: 0.7901 - val_acc: 0.7707\n",
      "Epoch 7/13\n",
      "92051/92051 [==============================] - 558s 6ms/step - loss: 0.6090 - acc: 0.8150 - val_loss: 0.7737 - val_acc: 0.7756\n",
      "Epoch 8/13\n",
      "92051/92051 [==============================] - 558s 6ms/step - loss: 0.5748 - acc: 0.8237 - val_loss: 0.8208 - val_acc: 0.7677\n",
      "Epoch 9/13\n",
      "92051/92051 [==============================] - 558s 6ms/step - loss: 0.5362 - acc: 0.8352 - val_loss: 0.8016 - val_acc: 0.7700\n",
      "Epoch 10/13\n",
      "92051/92051 [==============================] - 558s 6ms/step - loss: 0.5021 - acc: 0.8446 - val_loss: 0.8215 - val_acc: 0.7774\n",
      "Epoch 11/13\n",
      "92051/92051 [==============================] - 558s 6ms/step - loss: 0.4674 - acc: 0.8531 - val_loss: 0.8620 - val_acc: 0.7700\n",
      "Epoch 12/13\n",
      "92051/92051 [==============================] - 559s 6ms/step - loss: 0.4358 - acc: 0.8641 - val_loss: 0.9236 - val_acc: 0.7824\n",
      "Epoch 13/13\n",
      "92051/92051 [==============================] - 558s 6ms/step - loss: 0.4030 - acc: 0.8729 - val_loss: 0.8888 - val_acc: 0.7742\n",
      "10226/10226 [==============================] - 18s 2ms/step\n",
      "102277/102277 [==============================] - 177s 2ms/step\n",
      "*******6*******\n",
      "Train on 92053 samples, validate on 10224 samples\n",
      "Epoch 1/13\n",
      "92053/92053 [==============================] - 559s 6ms/step - loss: 1.0657 - acc: 0.6942 - val_loss: 0.9489 - val_acc: 0.7210\n",
      "Epoch 2/13\n",
      "92053/92053 [==============================] - 558s 6ms/step - loss: 0.8438 - acc: 0.7520 - val_loss: 0.8610 - val_acc: 0.7482\n",
      "Epoch 3/13\n",
      "92053/92053 [==============================] - 558s 6ms/step - loss: 0.7785 - acc: 0.7686 - val_loss: 0.8151 - val_acc: 0.7640\n",
      "Epoch 4/13\n",
      "92053/92053 [==============================] - 558s 6ms/step - loss: 0.7325 - acc: 0.7825 - val_loss: 0.7864 - val_acc: 0.7730\n",
      "Epoch 5/13\n",
      "92053/92053 [==============================] - 558s 6ms/step - loss: 0.6926 - acc: 0.7934 - val_loss: 0.7264 - val_acc: 0.7903\n",
      "Epoch 6/13\n",
      "92053/92053 [==============================] - 558s 6ms/step - loss: 0.6524 - acc: 0.8038 - val_loss: 0.7418 - val_acc: 0.7895\n",
      "Epoch 7/13\n",
      "92053/92053 [==============================] - 558s 6ms/step - loss: 0.6170 - acc: 0.8124 - val_loss: 0.7664 - val_acc: 0.7841\n",
      "Epoch 8/13\n",
      "92053/92053 [==============================] - 559s 6ms/step - loss: 0.5796 - acc: 0.8235 - val_loss: 0.7494 - val_acc: 0.7864\n",
      "Epoch 9/13\n",
      "92053/92053 [==============================] - 559s 6ms/step - loss: 0.5432 - acc: 0.8334 - val_loss: 0.7591 - val_acc: 0.7927\n",
      "Epoch 10/13\n",
      "92053/92053 [==============================] - 556s 6ms/step - loss: 0.5081 - acc: 0.8426 - val_loss: 0.7678 - val_acc: 0.7867\n",
      "Epoch 11/13\n",
      "92053/92053 [==============================] - 557s 6ms/step - loss: 0.4746 - acc: 0.8522 - val_loss: 0.8095 - val_acc: 0.7808\n",
      "Epoch 12/13\n",
      "92053/92053 [==============================] - 559s 6ms/step - loss: 0.4375 - acc: 0.8623 - val_loss: 0.8411 - val_acc: 0.7848\n",
      "Epoch 13/13\n",
      "92053/92053 [==============================] - 558s 6ms/step - loss: 0.4093 - acc: 0.8705 - val_loss: 0.8101 - val_acc: 0.7824\n",
      "10224/10224 [==============================] - 18s 2ms/step\n",
      "102277/102277 [==============================] - 177s 2ms/step\n",
      "*******7*******\n",
      "Train on 92054 samples, validate on 10223 samples\n",
      "Epoch 1/13\n",
      "92054/92054 [==============================] - 559s 6ms/step - loss: 1.0680 - acc: 0.6932 - val_loss: 1.0195 - val_acc: 0.7093\n",
      "Epoch 2/13\n",
      "92054/92054 [==============================] - 558s 6ms/step - loss: 0.8408 - acc: 0.7515 - val_loss: 0.8784 - val_acc: 0.7527\n",
      "Epoch 3/13\n",
      "92054/92054 [==============================] - 558s 6ms/step - loss: 0.7752 - acc: 0.7704 - val_loss: 0.8221 - val_acc: 0.7602\n",
      "Epoch 4/13\n",
      "92054/92054 [==============================] - 558s 6ms/step - loss: 0.7289 - acc: 0.7826 - val_loss: 0.8265 - val_acc: 0.7584\n",
      "Epoch 5/13\n",
      "92054/92054 [==============================] - 558s 6ms/step - loss: 0.6854 - acc: 0.7951 - val_loss: 0.7868 - val_acc: 0.7726\n",
      "Epoch 6/13\n",
      "92054/92054 [==============================] - 558s 6ms/step - loss: 0.6474 - acc: 0.8058 - val_loss: 0.7858 - val_acc: 0.7738\n",
      "Epoch 7/13\n",
      "92054/92054 [==============================] - 559s 6ms/step - loss: 0.6088 - acc: 0.8153 - val_loss: 0.8261 - val_acc: 0.7762\n",
      "Epoch 8/13\n",
      "92054/92054 [==============================] - 559s 6ms/step - loss: 0.5741 - acc: 0.8242 - val_loss: 0.7757 - val_acc: 0.7842\n",
      "Epoch 9/13\n",
      "92056/92056 [==============================] - 557s 6ms/step - loss: 1.0704 - acc: 0.6913 - val_loss: 0.9036 - val_acc: 0.7292\n",
      "Epoch 2/13\n",
      "92056/92056 [==============================] - 557s 6ms/step - loss: 0.8419 - acc: 0.7518 - val_loss: 0.8213 - val_acc: 0.7643\n",
      "Epoch 3/13\n",
      " 5824/92056 [>.............................] - ETA: 8:21 - loss: 0.7844 - acc: 0.7701"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#logloss_list = []\n",
    "for i in range(10):\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    print('*******%s*******'%i)\n",
    "    idx_train = np.array(index[index['fold'] != i]['id'])\n",
    "    idx_val = np.array(index[index['fold'] == i]['id'])\n",
    "    \n",
    "    \n",
    "    X_train = train_FE.loc[idx_train, :]\n",
    "    y_train = categorical_labels[idx_train]\n",
    "\n",
    "    X_val = train_FE.loc[idx_val, :]\n",
    "    y_val = categorical_labels[idx_val]\n",
    "\n",
    "    #dtest = test.loc[:,user_col]\n",
    "    \n",
    "    X_train = get_input(X_train)\n",
    "    X_val = get_input(X_val)\n",
    "    X_test = get_input(test_FE)\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    model = Globel_attention_DR_Stack_RNN()\n",
    "    model.compile(optimizer='RMSprop', loss='categorical_crossentropy',\\\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    hist = model.fit(X_train, y_train, \\\n",
    "            validation_data=(X_val, y_val), \\\n",
    "            epochs=13, batch_size=BATCH_SIZE, shuffle=True)#callbacks=callbacks\n",
    "\n",
    "    \n",
    "    pred = pd.DataFrame(model.predict(X_val,batch_size=128,verbose=1))\n",
    "    \n",
    "    for i in range(19):\n",
    "        meta_train.loc[idx_val, 'prob' + str(i)] = pred[i].values\n",
    "    \n",
    "    prob = model.predict(X_test,batch_size=128,verbose=1)\n",
    "    meta_test = pd.concat([meta_test, pd.DataFrame(prob)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((102277, 19), (102277, 190))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_train.shape, meta_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train.columns = ['meta_base_rnn_att_wrod_v4_300dim_' + x for x in meta_train.columns]\n",
    "\n",
    "p = pd.DataFrame()\n",
    "\n",
    "for i in range(19):\n",
    "    p[i] = meta_test[i].mean(axis = 1)\n",
    "\n",
    "p.columns = meta_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train.to_csv('/home/libo/daguan/stack_new/train_meta_base_att_rnn_word_v4_300dim.csv', index = None ,encoding = 'utf-8')\n",
    "\n",
    "p.to_csv('/home/libo/daguan/stack_new/test_meta_base_att_rnn_word_v4_300dim.csv', index = None ,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
