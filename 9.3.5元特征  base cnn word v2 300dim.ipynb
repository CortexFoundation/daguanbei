{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "window.load_remote_theme = true\n",
       "var theme_js = \"https://odhk.github.io/hyrule_theme/custom.js\";\n",
       "\n",
       "window.load_local_theme = function(){\n",
       "    var hostname = document.location.hostname\n",
       "    return ((hostname == \"localhost\" || hostname == '127.0.0.1') && !load_remote_theme)\n",
       "}\n",
       "\n",
       "var url = load_local_theme() ? document.location.origin + \"/files/theme/custom.js\" : theme_js\n",
       "\n",
       "$.getScript(url)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "window.load_remote_theme = true\n",
    "var theme_js = \"https://odhk.github.io/hyrule_theme/custom.js\";\n",
    "\n",
    "window.load_local_theme = function(){\n",
    "    var hostname = document.location.hostname\n",
    "    return ((hostname == \"localhost\" || hostname == '127.0.0.1') && !load_remote_theme)\n",
    "}\n",
    "\n",
    "var url = load_local_theme() ? document.location.origin + \"/files/theme/custom.js\" : theme_js\n",
    "\n",
    "$.getScript(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/libo/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "1271461it [01:52, 11291.63it/s]\n",
      "  2%|▏         | 26715/1271460 [00:00<00:04, 267089.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1271460 unique tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1271460/1271460 [00:04<00:00, 286425.91it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import keras\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Lambda, Embedding,constraints,\\\n",
    "Dropout, Activation,GRU,Bidirectional,Subtract, Permute, TimeDistributed, Reshape\n",
    "from keras.layers import Conv1D,Conv2D,MaxPooling2D,GlobalAveragePooling1D,GlobalMaxPooling1D, MaxPooling1D, Flatten\n",
    "from keras.layers import CuDNNGRU, CuDNNLSTM, SpatialDropout1D,Layer, initializers, regularizers\n",
    "from keras.layers.merge import concatenate, Concatenate, Average, Dot, Maximum, Multiply, Subtract\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from keras.activations import softmax\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import *\n",
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy.special import erfinv\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train = pd.read_csv('train_set.csv')\n",
    "test = pd.read_csv('test_set.csv')\n",
    "\n",
    "label = train['class']\n",
    "\n",
    "char_embedding_index = {}\n",
    "f = open('word_embedding_300dim_new2.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0].lower()\n",
    "    embeddings = np.asarray(values[1:], dtype = 'float32')\n",
    "    char_embedding_index[word] = embeddings\n",
    "f.close()\n",
    "\n",
    "len(char_embedding_index)\n",
    "\n",
    "api_embedding_file_path = 'word_embedding_300dim_new2.txt'\n",
    "embedding_dim = 300\n",
    "max_nb_api = len(char_embedding_index) + 1\n",
    "max_sequence_length_api =1000\n",
    "\n",
    "all_word = list(train['word_seg'].values) + list(test['word_seg'].values)\n",
    "\n",
    "all_word_list = []\n",
    "for i in list(all_word):\n",
    "    all_word_list.append(i.split(' '))\n",
    "\n",
    "len(all_word_list)\n",
    "\n",
    "train['word_seg'].apply(lambda x:len(x.split(' '))).describe()\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "tokenizer_api = Tokenizer()\n",
    "\n",
    "tokenizer_api.fit_on_texts(all_word_list)\n",
    "\n",
    "api_seq = tokenizer_api.texts_to_sequences(all_word_list)\n",
    "\n",
    "data['word_seq'] = pad_sequences(api_seq, maxlen = max_sequence_length_api, padding='post').tolist()\n",
    "\n",
    "api_index = tokenizer_api.word_index\n",
    "print('Found %s unique tokens' % len(api_index))\n",
    "nb_words = min(max_nb_api, len(char_embedding_index)+1)\n",
    "api_embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "\n",
    "for word, i in tqdm(api_index.items()):\n",
    "    embedding_vector = char_embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        api_embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        print('no char%s'%word)\n",
    "\n",
    "data['id'] = train['id']\n",
    "\n",
    "train_FE = data.iloc[:len(label),:]\n",
    "test_FE = data.iloc[len(label):,:]\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "def get_input(df):\n",
    "    \n",
    "    _input = [np.array(df.word_seq.values.tolist())]\n",
    "    \n",
    "    \n",
    "    return _input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CNN1():\n",
    "    \n",
    "    #Input layer\n",
    "    char_input = Input(shape=(max_sequence_length_api,), dtype='int32')\n",
    "    \n",
    "    api_embedding_layer = Embedding(nb_words,\n",
    "        embedding_dim,\n",
    "        weights=[api_embedding_matrix],\n",
    "        input_length=max_sequence_length_api,\n",
    "        trainable=False)\n",
    "    \n",
    "    char_cnn = api_embedding_layer(char_input)\n",
    "    \n",
    "    kernel_sizes = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "    pooled_word_q1 = []\n",
    "    pooled_word_q2 = []\n",
    "    \n",
    "    for kernel in kernel_sizes:\n",
    "            \n",
    "        conv_word_q1 = Conv1D(filters=200,\n",
    "                          kernel_size=kernel,\n",
    "                          padding='same',\n",
    "                          strides=1, \n",
    "                          kernel_initializer='he_uniform',\n",
    "                          activation='sigmoid')(char_cnn)\n",
    "\n",
    "        pool_word_q1 = MaxPooling1D(pool_size = max_sequence_length_api)(conv_word_q1)\n",
    "        pool_word_q2 = AveragePooling1D(pool_size = max_sequence_length_api)(conv_word_q1)\n",
    "        \n",
    "        pooled_word_q1.append(pool_word_q1)\n",
    "        pooled_word_q2.append(pool_word_q2)\n",
    "        \n",
    "    merged_cnn_word_q1 = Concatenate(axis=-1)(pooled_word_q1)\n",
    "    merged_cnn_word_q2 = Concatenate(axis=-1)(pooled_word_q2)\n",
    "    \n",
    "    flatten_cnn_word_q1 = Flatten()(merged_cnn_word_q1)\n",
    "    flatten_cnn_word_q2 = Flatten()(merged_cnn_word_q2)\n",
    "    \n",
    "    max_sub_char_abs = Lambda(lambda x:K.abs(x[0] - x[1]))([flatten_cnn_word_q1, flatten_cnn_word_q2])\n",
    "    max_multi_char = Lambda(lambda x: x[0] * x[1])([flatten_cnn_word_q1, flatten_cnn_word_q2])\n",
    "    \n",
    "    merged = concatenate([flatten_cnn_word_q1, flatten_cnn_word_q2, \\\n",
    "                          max_sub_char_abs, max_multi_char])\n",
    "    merged = Dropout(0.2)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dense(512, activation='relu')(merged)\n",
    "    merged = Dropout(0.1)(merged)\n",
    "    merged = BatchNormalization()(merged) \n",
    "    preds = Dense(19, activation = 'softmax')(merged)\n",
    "    \n",
    "    model = Model(inputs=[char_input],outputs=preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train = pd.DataFrame(np.array([None]*train_FE.shape[0]*19).reshape(train_FE.shape[0], 19))\n",
    "meta_train.columns = ['prob' + str(x) for x in range(19)]\n",
    "meta_test = pd.DataFrame()\n",
    "\n",
    "label = train['class']\n",
    "\n",
    "label = label - 1\n",
    "\n",
    "categorical_labels = keras.utils.np_utils.to_categorical(label, num_classes=19)\n",
    "\n",
    "index = pd.read_csv('fold.csv')\n",
    "index['id'] = train['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train, test, all_word\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******0*******\n",
      "Train on 92040 samples, validate on 10237 samples\n",
      "Epoch 1/20\n",
      "92040/92040 [==============================] - 279s 3ms/step - loss: 1.1882 - acc: 0.6585 - val_loss: 0.8322 - val_acc: 0.7551\n",
      "Epoch 2/20\n",
      "92040/92040 [==============================] - 278s 3ms/step - loss: 0.7985 - acc: 0.7623 - val_loss: 0.7961 - val_acc: 0.7583\n",
      "Epoch 3/20\n",
      "92040/92040 [==============================] - 277s 3ms/step - loss: 0.6522 - acc: 0.8019 - val_loss: 0.7682 - val_acc: 0.7740\n",
      "Epoch 4/20\n",
      "92040/92040 [==============================] - 278s 3ms/step - loss: 0.5160 - acc: 0.8394 - val_loss: 0.8018 - val_acc: 0.7701\n",
      "Epoch 5/20\n",
      "92040/92040 [==============================] - 278s 3ms/step - loss: 0.3848 - acc: 0.8787 - val_loss: 0.8325 - val_acc: 0.7774\n",
      "Epoch 6/20\n",
      "92040/92040 [==============================] - 278s 3ms/step - loss: 0.2714 - acc: 0.9149 - val_loss: 0.9308 - val_acc: 0.7724\n",
      "Epoch 7/20\n",
      "92040/92040 [==============================] - 278s 3ms/step - loss: 0.2119 - acc: 0.9349 - val_loss: 0.9884 - val_acc: 0.7708\n",
      "10237/10237 [==============================] - 9s 858us/step\n",
      "102277/102277 [==============================] - 87s 851us/step\n",
      "*******1*******\n",
      "Train on 92043 samples, validate on 10234 samples\n",
      "Epoch 1/20\n",
      "92043/92043 [==============================] - 279s 3ms/step - loss: 1.1851 - acc: 0.6587 - val_loss: 0.8592 - val_acc: 0.7474\n",
      "Epoch 2/20\n",
      "92043/92043 [==============================] - 278s 3ms/step - loss: 0.7921 - acc: 0.7651 - val_loss: 0.8084 - val_acc: 0.7610\n",
      "Epoch 3/20\n",
      "92043/92043 [==============================] - 278s 3ms/step - loss: 0.6452 - acc: 0.8044 - val_loss: 0.8255 - val_acc: 0.7618\n",
      "Epoch 4/20\n",
      "92043/92043 [==============================] - 278s 3ms/step - loss: 0.5131 - acc: 0.8405 - val_loss: 0.8339 - val_acc: 0.7697\n",
      "Epoch 5/20\n",
      "92043/92043 [==============================] - 278s 3ms/step - loss: 0.3739 - acc: 0.8829 - val_loss: 0.9583 - val_acc: 0.7573\n",
      "Epoch 6/20\n",
      "92043/92043 [==============================] - 278s 3ms/step - loss: 0.2719 - acc: 0.9151 - val_loss: 0.9738 - val_acc: 0.7698\n",
      "Epoch 7/20\n",
      "92043/92043 [==============================] - 278s 3ms/step - loss: 0.2058 - acc: 0.9368 - val_loss: 1.0423 - val_acc: 0.7632\n",
      "Epoch 8/20\n",
      "92043/92043 [==============================] - 278s 3ms/step - loss: 0.1637 - acc: 0.9509 - val_loss: 1.0866 - val_acc: 0.7673\n",
      "10234/10234 [==============================] - 9s 855us/step\n",
      "102277/102277 [==============================] - 88s 858us/step\n",
      "*******2*******\n",
      "Train on 92044 samples, validate on 10233 samples\n",
      "Epoch 1/20\n",
      "92044/92044 [==============================] - 276s 3ms/step - loss: 1.1932 - acc: 0.6564 - val_loss: 0.8265 - val_acc: 0.7582\n",
      "Epoch 2/20\n",
      "92044/92044 [==============================] - 275s 3ms/step - loss: 0.7965 - acc: 0.7624 - val_loss: 0.7569 - val_acc: 0.7761\n",
      "Epoch 3/20\n",
      "92044/92044 [==============================] - 275s 3ms/step - loss: 0.6509 - acc: 0.8020 - val_loss: 0.7453 - val_acc: 0.7792\n",
      "Epoch 4/20\n",
      "92044/92044 [==============================] - 274s 3ms/step - loss: 0.5180 - acc: 0.8396 - val_loss: 0.7797 - val_acc: 0.7782\n",
      "Epoch 5/20\n",
      "92044/92044 [==============================] - 275s 3ms/step - loss: 0.3797 - acc: 0.8800 - val_loss: 0.8284 - val_acc: 0.7799\n",
      "Epoch 6/20\n",
      "92044/92044 [==============================] - 275s 3ms/step - loss: 0.2708 - acc: 0.9154 - val_loss: 0.9422 - val_acc: 0.7746\n",
      "Epoch 7/20\n",
      "92044/92044 [==============================] - 275s 3ms/step - loss: 0.2102 - acc: 0.9350 - val_loss: 0.9877 - val_acc: 0.7787\n",
      "10233/10233 [==============================] - 9s 840us/step\n",
      "102277/102277 [==============================] - 86s 843us/step\n",
      "*******3*******\n",
      "Train on 92046 samples, validate on 10231 samples\n",
      "Epoch 1/20\n",
      "92046/92046 [==============================] - 276s 3ms/step - loss: 1.1869 - acc: 0.6594 - val_loss: 0.8211 - val_acc: 0.7552\n",
      "Epoch 2/20\n",
      "92046/92046 [==============================] - 275s 3ms/step - loss: 0.7993 - acc: 0.7625 - val_loss: 0.7775 - val_acc: 0.7728\n",
      "Epoch 3/20\n",
      "92046/92046 [==============================] - 275s 3ms/step - loss: 0.6506 - acc: 0.8024 - val_loss: 0.7747 - val_acc: 0.7774\n",
      "Epoch 4/20\n",
      "92046/92046 [==============================] - 275s 3ms/step - loss: 0.5164 - acc: 0.8401 - val_loss: 0.7789 - val_acc: 0.7778\n",
      "Epoch 5/20\n",
      "92046/92046 [==============================] - 275s 3ms/step - loss: 0.3776 - acc: 0.8813 - val_loss: 0.8355 - val_acc: 0.7746\n",
      "Epoch 6/20\n",
      "92046/92046 [==============================] - 275s 3ms/step - loss: 0.2690 - acc: 0.9154 - val_loss: 0.9283 - val_acc: 0.7693\n",
      "10231/10231 [==============================] - 9s 840us/step\n",
      "102277/102277 [==============================] - 86s 844us/step\n",
      "*******4*******\n",
      "Train on 92049 samples, validate on 10228 samples\n",
      "Epoch 1/20\n",
      "92049/92049 [==============================] - 276s 3ms/step - loss: 1.1982 - acc: 0.6569 - val_loss: 0.8379 - val_acc: 0.7473\n",
      "Epoch 2/20\n",
      "92049/92049 [==============================] - 275s 3ms/step - loss: 0.7994 - acc: 0.7621 - val_loss: 0.7701 - val_acc: 0.7695\n",
      "Epoch 3/20\n",
      "92049/92049 [==============================] - 275s 3ms/step - loss: 0.6564 - acc: 0.8012 - val_loss: 0.7905 - val_acc: 0.7675\n",
      "Epoch 4/20\n",
      "92049/92049 [==============================] - 276s 3ms/step - loss: 0.5197 - acc: 0.8375 - val_loss: 0.7791 - val_acc: 0.7770\n",
      "Epoch 5/20\n",
      "92049/92049 [==============================] - 275s 3ms/step - loss: 0.3805 - acc: 0.8796 - val_loss: 0.8749 - val_acc: 0.7681\n",
      "Epoch 6/20\n",
      "92049/92049 [==============================] - 275s 3ms/step - loss: 0.2764 - acc: 0.9130 - val_loss: 0.8933 - val_acc: 0.7763\n",
      "10228/10228 [==============================] - 9s 840us/step\n",
      "102277/102277 [==============================] - 86s 843us/step\n",
      "*******5*******\n",
      "Train on 92051 samples, validate on 10226 samples\n",
      "Epoch 1/20\n",
      "92051/92051 [==============================] - 276s 3ms/step - loss: 1.1946 - acc: 0.6582 - val_loss: 0.8377 - val_acc: 0.7525\n",
      "Epoch 2/20\n",
      "92051/92051 [==============================] - 275s 3ms/step - loss: 0.7974 - acc: 0.7626 - val_loss: 0.7833 - val_acc: 0.7662\n",
      "Epoch 3/20\n",
      "92051/92051 [==============================] - 275s 3ms/step - loss: 0.6524 - acc: 0.8023 - val_loss: 0.7910 - val_acc: 0.7691\n",
      "Epoch 4/20\n",
      "92051/92051 [==============================] - 275s 3ms/step - loss: 0.5186 - acc: 0.8391 - val_loss: 0.8093 - val_acc: 0.7709\n",
      "Epoch 5/20\n",
      "92051/92051 [==============================] - 275s 3ms/step - loss: 0.3778 - acc: 0.8805 - val_loss: 0.8627 - val_acc: 0.7737\n",
      "Epoch 6/20\n",
      "92051/92051 [==============================] - 275s 3ms/step - loss: 0.2682 - acc: 0.9151 - val_loss: 0.9339 - val_acc: 0.7724\n",
      "Epoch 7/20\n",
      "92051/92051 [==============================] - 275s 3ms/step - loss: 0.2057 - acc: 0.9373 - val_loss: 0.9941 - val_acc: 0.7754\n",
      "Epoch 8/20\n",
      "92051/92051 [==============================] - 275s 3ms/step - loss: 0.1656 - acc: 0.9500 - val_loss: 1.1166 - val_acc: 0.7677\n",
      "Epoch 9/20\n",
      "92051/92051 [==============================] - 275s 3ms/step - loss: 0.1454 - acc: 0.9572 - val_loss: 1.0983 - val_acc: 0.7722\n",
      "10226/10226 [==============================] - 9s 844us/step\n",
      "102277/102277 [==============================] - 86s 843us/step\n",
      "*******6*******\n",
      "Train on 92053 samples, validate on 10224 samples\n",
      "Epoch 1/20\n",
      "92053/92053 [==============================] - 276s 3ms/step - loss: 1.2022 - acc: 0.6555 - val_loss: 0.8147 - val_acc: 0.7602\n",
      "Epoch 2/20\n",
      "92053/92053 [==============================] - 275s 3ms/step - loss: 0.8002 - acc: 0.7604 - val_loss: 0.7468 - val_acc: 0.7819\n",
      "Epoch 3/20\n",
      "92053/92053 [==============================] - 275s 3ms/step - loss: 0.6521 - acc: 0.8025 - val_loss: 0.7416 - val_acc: 0.7876\n",
      "Epoch 4/20\n",
      "92053/92053 [==============================] - 274s 3ms/step - loss: 0.5204 - acc: 0.8376 - val_loss: 0.7388 - val_acc: 0.7924\n",
      "Epoch 5/20\n",
      "92053/92053 [==============================] - 275s 3ms/step - loss: 0.3788 - acc: 0.8803 - val_loss: 0.8281 - val_acc: 0.7800\n",
      "Epoch 6/20\n",
      "92053/92053 [==============================] - 275s 3ms/step - loss: 0.2745 - acc: 0.9141 - val_loss: 0.9038 - val_acc: 0.7811\n",
      "10224/10224 [==============================] - 9s 841us/step\n",
      "102277/102277 [==============================] - 86s 841us/step\n",
      "*******7*******\n",
      "Train on 92054 samples, validate on 10223 samples\n",
      "Epoch 1/20\n",
      "92054/92054 [==============================] - 276s 3ms/step - loss: 1.1910 - acc: 0.6576 - val_loss: 0.8335 - val_acc: 0.7568\n",
      "Epoch 2/20\n",
      "92054/92054 [==============================] - 275s 3ms/step - loss: 0.7970 - acc: 0.7617 - val_loss: 0.7836 - val_acc: 0.7740\n",
      "Epoch 3/20\n",
      "92054/92054 [==============================] - 275s 3ms/step - loss: 0.6509 - acc: 0.8021 - val_loss: 0.7731 - val_acc: 0.7757\n",
      "Epoch 4/20\n",
      "92054/92054 [==============================] - 275s 3ms/step - loss: 0.5125 - acc: 0.8406 - val_loss: 0.8251 - val_acc: 0.7730\n",
      "Epoch 5/20\n",
      "92054/92054 [==============================] - 275s 3ms/step - loss: 0.3713 - acc: 0.8815 - val_loss: 0.8732 - val_acc: 0.7738\n",
      "10223/10223 [==============================] - 9s 843us/step\n",
      "102277/102277 [==============================] - 87s 849us/step\n",
      "*******8*******\n",
      "Train on 92056 samples, validate on 10221 samples\n",
      "Epoch 1/20\n",
      "92056/92056 [==============================] - 276s 3ms/step - loss: 1.1973 - acc: 0.6565 - val_loss: 0.8165 - val_acc: 0.7566\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Problems closing file (File write failed: time = tue sep  4 15:16:35 2018\n, filename = '10folds_base_cnn_wrod_v2_300dim8.hdf5', file descriptor = 44, errno = 28, error message = 'no space left on device', buf = 0x55951aaaea80, total write size = 4096, bytes this sub-write = 4096, bytes actually written = 18446744073709551615, offset = 4096)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite)\u001b[0m\n\u001b[1;32m   2621\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2622\u001b[0;31m             \u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2623\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36msave_weights_to_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m   2977\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2978\u001b[0;31m                 \u001b[0mparam_dset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, args, val)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfspace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdxpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dxpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.DatasetID.write\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_proxy.pyx\u001b[0m in \u001b[0;36mh5py._proxy.dset_rw\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_proxy.pyx\u001b[0m in \u001b[0;36mh5py._proxy.H5PY_H5Dwrite\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't prepare for writing data (File write failed: time = tue sep  4 15:16:35 2018\n, filename = '10folds_base_cnn_wrod_v2_300dim8.hdf5', file descriptor = 44, errno = 28, error message = 'no space left on device', buf = 0x104f2f4b800, total write size = 12348784, bytes this sub-write = 12348784, bytes actually written = 18446744073709551615, offset = 1513414656)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1254\u001b[0m                             \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m                                 \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    443\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite)\u001b[0m\n\u001b[1;32m   2621\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m             \u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m     def load_weights(self, filepath, by_name=False,\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwith_phil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mid_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mid_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mh5i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5i.pyx\u001b[0m in \u001b[0;36mh5py.h5i.dec_ref\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Problems closing file (File write failed: time = tue sep  4 15:16:35 2018\n, filename = '10folds_base_cnn_wrod_v2_300dim8.hdf5', file descriptor = 44, errno = 28, error message = 'no space left on device', buf = 0x55951aaaea80, total write size = 4096, bytes this sub-write = 4096, bytes actually written = 18446744073709551615, offset = 4096)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#logloss_list = []\n",
    "for i in range(10):\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    print('*******%s*******'%i)\n",
    "    idx_train = np.array(index[index['fold'] != i]['id'])\n",
    "    idx_val = np.array(index[index['fold'] == i]['id'])\n",
    "    \n",
    "    \n",
    "    X_train = train_FE.loc[idx_train, :]\n",
    "    y_train = categorical_labels[idx_train]\n",
    "\n",
    "    X_val = train_FE.loc[idx_val, :]\n",
    "    y_val = categorical_labels[idx_val]\n",
    "\n",
    "    #dtest = test.loc[:,user_col]\n",
    "    \n",
    "    X_train = get_input(X_train)\n",
    "    X_val = get_input(X_val)\n",
    "    X_test = get_input(test_FE)\n",
    "    \n",
    "    BATCH_SIZE = 128\n",
    "    bst_model_path = '10folds_' + 'base_cnn_wrod_v2_300dim' + str(i) + '.hdf5'\n",
    "    early_stopping =EarlyStopping(monitor='val_acc', patience=2)\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "    callbacks = [\n",
    "            early_stopping,\n",
    "            model_checkpoint\n",
    "        ]\n",
    "    model = CNN1()\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',\\\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    hist = model.fit(X_train, y_train, \\\n",
    "            validation_data=(X_val, y_val), \\\n",
    "            epochs=20, batch_size=BATCH_SIZE, shuffle=True, callbacks=callbacks)#callbacks=callbacks\n",
    "    \n",
    "    model = CNN1()\n",
    "    model.load_weights(bst_model_path)\n",
    "    \n",
    "    pred = pd.DataFrame(model.predict(X_val,batch_size=128,verbose=1))\n",
    "    \n",
    "    for i in range(19):\n",
    "        meta_train.loc[idx_val, 'prob' + str(i)] = pred[i].values\n",
    "    \n",
    "    prob = model.predict(X_test,batch_size=128,verbose=1)\n",
    "    meta_test = pd.concat([meta_test, pd.DataFrame(prob)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******8*******\n",
      "Train on 92056 samples, validate on 10221 samples\n",
      "Epoch 1/20\n",
      "92056/92056 [==============================] - 274s 3ms/step - loss: 1.1922 - acc: 0.6596 - val_loss: 0.8081 - val_acc: 0.7578\n",
      "Epoch 2/20\n",
      "92056/92056 [==============================] - 275s 3ms/step - loss: 0.7971 - acc: 0.7622 - val_loss: 0.7521 - val_acc: 0.7696\n",
      "Epoch 3/20\n",
      "92056/92056 [==============================] - 275s 3ms/step - loss: 0.6507 - acc: 0.8026 - val_loss: 0.7548 - val_acc: 0.7755\n",
      "Epoch 4/20\n",
      "92056/92056 [==============================] - 275s 3ms/step - loss: 0.5160 - acc: 0.8383 - val_loss: 0.7691 - val_acc: 0.7778\n",
      "Epoch 5/20\n",
      "92056/92056 [==============================] - 275s 3ms/step - loss: 0.3769 - acc: 0.8808 - val_loss: 0.8123 - val_acc: 0.7817\n",
      "Epoch 6/20\n",
      "92056/92056 [==============================] - 275s 3ms/step - loss: 0.2705 - acc: 0.9144 - val_loss: 0.8871 - val_acc: 0.7789\n",
      "Epoch 7/20\n",
      "92056/92056 [==============================] - 275s 3ms/step - loss: 0.2071 - acc: 0.9359 - val_loss: 0.9384 - val_acc: 0.7827\n",
      "Epoch 8/20\n",
      "92056/92056 [==============================] - 275s 3ms/step - loss: 0.1679 - acc: 0.9499 - val_loss: 0.9887 - val_acc: 0.7809\n",
      "Epoch 9/20\n",
      "92056/92056 [==============================] - 275s 3ms/step - loss: 0.1451 - acc: 0.9581 - val_loss: 1.0625 - val_acc: 0.7701\n",
      "10221/10221 [==============================] - 9s 845us/step\n",
      "102277/102277 [==============================] - 86s 843us/step\n",
      "*******9*******\n",
      "Train on 92057 samples, validate on 10220 samples\n",
      "Epoch 1/20\n",
      "92057/92057 [==============================] - 276s 3ms/step - loss: 1.1922 - acc: 0.6593 - val_loss: 0.8212 - val_acc: 0.7557\n",
      "Epoch 2/20\n",
      "92057/92057 [==============================] - 275s 3ms/step - loss: 0.7934 - acc: 0.7635 - val_loss: 0.7746 - val_acc: 0.7674\n",
      "Epoch 3/20\n",
      "92057/92057 [==============================] - 275s 3ms/step - loss: 0.6532 - acc: 0.8027 - val_loss: 0.7590 - val_acc: 0.7745\n",
      "Epoch 4/20\n",
      "92057/92057 [==============================] - 274s 3ms/step - loss: 0.5135 - acc: 0.8408 - val_loss: 0.7815 - val_acc: 0.7750\n",
      "Epoch 5/20\n",
      "92057/92057 [==============================] - 275s 3ms/step - loss: 0.3772 - acc: 0.8819 - val_loss: 0.8302 - val_acc: 0.7778\n",
      "Epoch 6/20\n",
      "92057/92057 [==============================] - 275s 3ms/step - loss: 0.2680 - acc: 0.9150 - val_loss: 0.9057 - val_acc: 0.7762\n",
      "Epoch 7/20\n",
      "92057/92057 [==============================] - 275s 3ms/step - loss: 0.2035 - acc: 0.9384 - val_loss: 0.9969 - val_acc: 0.7753\n",
      "10220/10220 [==============================] - 9s 841us/step\n",
      "102277/102277 [==============================] - 86s 844us/step\n",
      "CPU times: user 44min 52s, sys: 14min 58s, total: 59min 51s\n",
      "Wall time: 1h 18min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#logloss_list = []\n",
    "for i in [8, 9]:\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    print('*******%s*******'%i)\n",
    "    idx_train = np.array(index[index['fold'] != i]['id'])\n",
    "    idx_val = np.array(index[index['fold'] == i]['id'])\n",
    "    \n",
    "    \n",
    "    X_train = train_FE.loc[idx_train, :]\n",
    "    y_train = categorical_labels[idx_train]\n",
    "\n",
    "    X_val = train_FE.loc[idx_val, :]\n",
    "    y_val = categorical_labels[idx_val]\n",
    "\n",
    "    #dtest = test.loc[:,user_col]\n",
    "    \n",
    "    X_train = get_input(X_train)\n",
    "    X_val = get_input(X_val)\n",
    "    X_test = get_input(test_FE)\n",
    "    \n",
    "    BATCH_SIZE = 128\n",
    "    bst_model_path = '10folds_' + 'base_cnn_wrod_v2_300dim' + str(i) + '.hdf5'\n",
    "    early_stopping =EarlyStopping(monitor='val_acc', patience=2)\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "    callbacks = [\n",
    "            early_stopping,\n",
    "            model_checkpoint\n",
    "        ]\n",
    "    model = CNN1()\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',\\\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    hist = model.fit(X_train, y_train, \\\n",
    "            validation_data=(X_val, y_val), \\\n",
    "            epochs=20, batch_size=BATCH_SIZE, shuffle=True, callbacks=callbacks)#callbacks=callbacks\n",
    "    \n",
    "    model = CNN1()\n",
    "    model.load_weights(bst_model_path)\n",
    "    \n",
    "    pred = pd.DataFrame(model.predict(X_val,batch_size=128,verbose=1))\n",
    "    \n",
    "    for i in range(19):\n",
    "        meta_train.loc[idx_val, 'prob' + str(i)] = pred[i].values\n",
    "    \n",
    "    prob = model.predict(X_test,batch_size=128,verbose=1)\n",
    "    meta_test = pd.concat([meta_test, pd.DataFrame(prob)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train.columns = ['meta_base_cnn_word_v2_300dim_' + x for x in meta_train.columns]\n",
    "\n",
    "p = pd.DataFrame()\n",
    "\n",
    "for i in range(19):\n",
    "    p[i] = meta_test[i].mean(axis = 1)\n",
    "\n",
    "p.columns = meta_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((102277, 19), (102277, 19))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape, meta_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train.to_csv('/home/libo/daguan/stack_new/train_meta_base_cnn_word_v2_300dim.csv', index = None ,encoding = 'utf-8')\n",
    "\n",
    "p.to_csv('/home/libo/daguan/stack_new/test_meta_base_cnn_word_v2_300dim.csv', index = None ,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_set.csv')\n",
    "\n",
    "pred = pd.DataFrame(test['id'])\n",
    "\n",
    "pred['class'] = np.argmax(p.values, axis=1) + 1\n",
    "\n",
    "pred.to_csv('/home/libo/daguan/stack_new/9.4.1.csv', index = None ,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102247</th>\n",
       "      <td>102247</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102248</th>\n",
       "      <td>102248</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102249</th>\n",
       "      <td>102249</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102250</th>\n",
       "      <td>102250</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102251</th>\n",
       "      <td>102251</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102252</th>\n",
       "      <td>102252</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102253</th>\n",
       "      <td>102253</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102254</th>\n",
       "      <td>102254</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102255</th>\n",
       "      <td>102255</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102256</th>\n",
       "      <td>102256</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102257</th>\n",
       "      <td>102257</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102258</th>\n",
       "      <td>102258</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102259</th>\n",
       "      <td>102259</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102260</th>\n",
       "      <td>102260</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102261</th>\n",
       "      <td>102261</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102262</th>\n",
       "      <td>102262</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102263</th>\n",
       "      <td>102263</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102264</th>\n",
       "      <td>102264</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102265</th>\n",
       "      <td>102265</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102266</th>\n",
       "      <td>102266</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102267</th>\n",
       "      <td>102267</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102268</th>\n",
       "      <td>102268</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102269</th>\n",
       "      <td>102269</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102270</th>\n",
       "      <td>102270</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102271</th>\n",
       "      <td>102271</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102272</th>\n",
       "      <td>102272</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102273</th>\n",
       "      <td>102273</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102274</th>\n",
       "      <td>102274</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102275</th>\n",
       "      <td>102275</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102276</th>\n",
       "      <td>102276</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102277 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  class\n",
       "0            0      5\n",
       "1            1      4\n",
       "2            2     13\n",
       "3            3      4\n",
       "4            4      5\n",
       "5            5      5\n",
       "6            6     15\n",
       "7            7     19\n",
       "8            8      3\n",
       "9            9     12\n",
       "10          10      8\n",
       "11          11      8\n",
       "12          12      5\n",
       "13          13     13\n",
       "14          14     15\n",
       "15          15     18\n",
       "16          16      9\n",
       "17          17      9\n",
       "18          18     12\n",
       "19          19     14\n",
       "20          20      9\n",
       "21          21     11\n",
       "22          22      9\n",
       "23          23      9\n",
       "24          24      2\n",
       "25          25     15\n",
       "26          26      3\n",
       "27          27     15\n",
       "28          28      4\n",
       "29          29     19\n",
       "...        ...    ...\n",
       "102247  102247     16\n",
       "102248  102248      8\n",
       "102249  102249     10\n",
       "102250  102250      2\n",
       "102251  102251     18\n",
       "102252  102252      5\n",
       "102253  102253     18\n",
       "102254  102254      2\n",
       "102255  102255     14\n",
       "102256  102256      1\n",
       "102257  102257      6\n",
       "102258  102258     12\n",
       "102259  102259     15\n",
       "102260  102260     14\n",
       "102261  102261     19\n",
       "102262  102262      2\n",
       "102263  102263     12\n",
       "102264  102264     15\n",
       "102265  102265     14\n",
       "102266  102266     15\n",
       "102267  102267      3\n",
       "102268  102268      8\n",
       "102269  102269     14\n",
       "102270  102270      6\n",
       "102271  102271     15\n",
       "102272  102272      8\n",
       "102273  102273     12\n",
       "102274  102274      6\n",
       "102275  102275     14\n",
       "102276  102276     13\n",
       "\n",
       "[102277 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
