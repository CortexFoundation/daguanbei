{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "window.load_remote_theme = true\n",
       "var theme_js = \"https://odhk.github.io/hyrule_theme/custom.js\";\n",
       "\n",
       "window.load_local_theme = function(){\n",
       "    var hostname = document.location.hostname\n",
       "    return ((hostname == \"localhost\" || hostname == '127.0.0.1') && !load_remote_theme)\n",
       "}\n",
       "\n",
       "var url = load_local_theme() ? document.location.origin + \"/files/theme/custom.js\" : theme_js\n",
       "\n",
       "$.getScript(url)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "window.load_remote_theme = true\n",
    "var theme_js = \"https://odhk.github.io/hyrule_theme/custom.js\";\n",
    "\n",
    "window.load_local_theme = function(){\n",
    "    var hostname = document.location.hostname\n",
    "    return ((hostname == \"localhost\" || hostname == '127.0.0.1') && !load_remote_theme)\n",
    "}\n",
    "\n",
    "var url = load_local_theme() ? document.location.origin + \"/files/theme/custom.js\" : theme_js\n",
    "\n",
    "$.getScript(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/libo/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "1271461it [01:53, 11188.35it/s]\n",
      "  2%|▏         | 27391/1271460 [00:00<00:04, 273841.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1271460 unique tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1271460/1271460 [00:04<00:00, 279594.54it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import keras\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Lambda, Embedding,constraints,\\\n",
    "Dropout, Activation,GRU,Bidirectional,Subtract, Permute, TimeDistributed, Reshape\n",
    "from keras.layers import Conv1D,Conv2D,MaxPooling2D,GlobalAveragePooling1D,GlobalMaxPooling1D, MaxPooling1D, Flatten\n",
    "from keras.layers import CuDNNGRU, CuDNNLSTM, SpatialDropout1D,Layer, initializers, regularizers\n",
    "from keras.layers.merge import concatenate, Concatenate, Average, Dot, Maximum, Multiply, Subtract\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from keras.activations import softmax\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import *\n",
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy.special import erfinv\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train = pd.read_csv('train_set.csv')\n",
    "test = pd.read_csv('test_set.csv')\n",
    "\n",
    "label = train['class']\n",
    "\n",
    "char_embedding_index = {}\n",
    "f = open('word_embedding_300dim_new2.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0].lower()\n",
    "    embeddings = np.asarray(values[1:], dtype = 'float32')\n",
    "    char_embedding_index[word] = embeddings\n",
    "f.close()\n",
    "\n",
    "len(char_embedding_index)\n",
    "\n",
    "api_embedding_file_path = 'word_embedding_300dim_new2.txt'\n",
    "embedding_dim = 300\n",
    "max_nb_api = len(char_embedding_index) + 1\n",
    "max_sequence_length_api =1000\n",
    "\n",
    "all_word = list(train['word_seg'].values) + list(test['word_seg'].values)\n",
    "\n",
    "all_word_list = []\n",
    "for i in list(all_word):\n",
    "    all_word_list.append(i.split(' '))\n",
    "\n",
    "len(all_word_list)\n",
    "\n",
    "train['word_seg'].apply(lambda x:len(x.split(' '))).describe()\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "tokenizer_api = Tokenizer()\n",
    "\n",
    "tokenizer_api.fit_on_texts(all_word_list)\n",
    "\n",
    "api_seq = tokenizer_api.texts_to_sequences(all_word_list)\n",
    "\n",
    "data['word_seq'] = pad_sequences(api_seq, maxlen = max_sequence_length_api, padding='post').tolist()\n",
    "\n",
    "api_index = tokenizer_api.word_index\n",
    "print('Found %s unique tokens' % len(api_index))\n",
    "nb_words = min(max_nb_api, len(char_embedding_index)+1)\n",
    "api_embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "\n",
    "for word, i in tqdm(api_index.items()):\n",
    "    embedding_vector = char_embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        api_embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        print('no char%s'%word)\n",
    "\n",
    "data['id'] = train['id']\n",
    "\n",
    "train_FE = data.iloc[:len(label),:]\n",
    "test_FE = data.iloc[len(label):,:]\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "def get_input(df):\n",
    "    \n",
    "    _input = [np.array(df.word_seq.values.tolist())]\n",
    "    \n",
    "    \n",
    "    return _input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train = pd.DataFrame(np.array([None]*train_FE.shape[0]*19).reshape(train_FE.shape[0], 19))\n",
    "meta_train.columns = ['prob' + str(x) for x in range(19)]\n",
    "meta_test = pd.DataFrame()\n",
    "\n",
    "label = train['class']\n",
    "\n",
    "label = label - 1\n",
    "\n",
    "categorical_labels = keras.utils.np_utils.to_categorical(label, num_classes=19)\n",
    "\n",
    "index = pd.read_csv('fold.csv')\n",
    "index['id'] = train['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention(x):\n",
    "    \n",
    "    q1, q2 = x[0], x[1]\n",
    "    \n",
    "    #compute match matrix euclidean match score\n",
    "    match_matrix = 1. + K.sqrt(\n",
    "        -2 * K.batch_dot(q1, q2, axes=[2, 2]) +\n",
    "        K.expand_dims(K.sum(K.square(q1), axis=2), 2) +\n",
    "        K.expand_dims(K.sum(K.square(q2), axis=2), 1)\n",
    "    )\n",
    "    \n",
    "    match_matrix = K.maximum(match_matrix, K.epsilon())\n",
    "    match_matrix = 1. / match_matrix\n",
    "    #compute attention output\n",
    "    q1_new = K.batch_dot(K.softmax(match_matrix, axis = 1), q2)\n",
    "    match_matrix_T = Permute((2, 1))(match_matrix)\n",
    "    q2_new = K.batch_dot(K.softmax(match_matrix_T, axis = 1), q1)\n",
    "    \n",
    "    return [q1_new, q2_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Globel_attention_RCNN():\n",
    "    \n",
    "    #Input layer\n",
    "    char_input = Input(shape=(max_sequence_length_api,), dtype='int32')\n",
    "    \n",
    "    api_embedding_layer = Embedding(nb_words,\n",
    "        embedding_dim,\n",
    "        weights=[api_embedding_matrix],\n",
    "        input_length=max_sequence_length_api,\n",
    "        trainable=False)\n",
    "    \n",
    "    \n",
    "    #RNN\n",
    "    char_rnn_layer_1 = Bidirectional(CuDNNLSTM(150, \n",
    "                               return_sequences=True))\n",
    "    char_rnn_layer_2 = Bidirectional(CuDNNLSTM(150, \n",
    "                               return_sequences=True))\n",
    "    \n",
    "    char_embedding = api_embedding_layer(char_input)\n",
    "    \n",
    "    #CNN part\n",
    "    #v1\n",
    "    kernel_sizes = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "    pooled_word_q1 = []\n",
    "    pooled_word_q2 = []\n",
    "    \n",
    "    for kernel in kernel_sizes:\n",
    "            \n",
    "        conv_word_q1 = Conv1D(filters=200,\n",
    "                          kernel_size=kernel,\n",
    "                          padding='same',\n",
    "                          strides=1, \n",
    "                          kernel_initializer='he_uniform',\n",
    "                          activation='relu')(char_embedding)\n",
    "\n",
    "        pool_word_q1 = MaxPooling1D(pool_size = max_sequence_length_api)(conv_word_q1)\n",
    "        pool_word_q2 = AveragePooling1D(pool_size = max_sequence_length_api)(conv_word_q1)\n",
    "        \n",
    "        pooled_word_q1.append(pool_word_q1)\n",
    "        pooled_word_q2.append(pool_word_q2)\n",
    "        \n",
    "    merged_cnn_word_q1 = Concatenate(axis=-1)(pooled_word_q1)\n",
    "    merged_cnn_word_q2 = Concatenate(axis=-1)(pooled_word_q2)\n",
    "    \n",
    "    flatten_cnn_word_q1 = Flatten()(merged_cnn_word_q1)\n",
    "    flatten_cnn_word_q2 = Flatten()(merged_cnn_word_q2)\n",
    "    \n",
    "    #双层BiLSTM\n",
    "    char_rnn_1= char_rnn_layer_1(char_embedding)\n",
    "    char_rnn_2= char_rnn_layer_2(char_rnn_1)\n",
    "    \n",
    "    #max avg \n",
    "    char_att_maxpooling_1 = GlobalMaxPooling1D()(char_rnn_1)\n",
    "    char_att_avgpooling_1 = GlobalAveragePooling1D()(char_rnn_1)\n",
    "    \n",
    "    char_att_maxpooling_2 = GlobalMaxPooling1D()(char_rnn_2)\n",
    "    char_att_avgpooling_2 = GlobalAveragePooling1D()(char_rnn_2)\n",
    "    \n",
    "    #batch normalization\n",
    "    char_embedding = BatchNormalization()(char_embedding)\n",
    "    char_rnn_2 = BatchNormalization()(char_rnn_2)\n",
    "    \n",
    "    #globel attention information\n",
    "    r = Lambda(attention, name = 'attention')([char_embedding, char_rnn_2])\n",
    "    \n",
    "    attention_map_left = r[0]\n",
    "    attention_map_right = r[1]\n",
    "    \n",
    "    att_maxpooling_left = GlobalMaxPooling1D()(attention_map_left)\n",
    "    att_maxpooling_right = GlobalMaxPooling1D()(attention_map_right)\n",
    "    \n",
    "    att_avgpooling_left = GlobalAveragePooling1D()(attention_map_left)\n",
    "    att_avgpooling_right = GlobalAveragePooling1D()(attention_map_right)\n",
    "   \n",
    "    max_sub_char_abs_attention = Lambda(lambda x:K.abs(x[0] - x[1]))([att_maxpooling_left, att_maxpooling_right])\n",
    "    max_multi_char_attention = Lambda(lambda x: x[0] * x[1])([att_maxpooling_left, att_maxpooling_right])\n",
    "\n",
    "    avg_sub_char_abs_attention = Lambda(lambda x:K.abs(x[0] - x[1]))([att_avgpooling_left, att_avgpooling_right])\n",
    "    avg_multi_char_attention = Lambda(lambda x: x[0] * x[1])([att_avgpooling_left, att_avgpooling_right])\n",
    "\n",
    "    \n",
    "    max_sub_char_abs = Lambda(lambda x:K.abs(x[0] - x[1]))([char_att_maxpooling_1, char_att_maxpooling_2])\n",
    "    max_multi_char = Lambda(lambda x: x[0] * x[1])([char_att_maxpooling_1, char_att_maxpooling_2])\n",
    "\n",
    "    avg_sub_char_abs = Lambda(lambda x:K.abs(x[0] - x[1]))([char_att_avgpooling_1, char_att_avgpooling_2])\n",
    "    avg_multi_char = Lambda(lambda x: x[0] * x[1])([char_att_avgpooling_1, char_att_avgpooling_2])\n",
    "    \n",
    "    \n",
    "    #merged feature maps\n",
    "    merged = concatenate([char_att_maxpooling_1, char_att_avgpooling_1, \\\n",
    "                          char_att_maxpooling_2, char_att_avgpooling_2,\\\n",
    "                         max_sub_char_abs_attention, max_multi_char_attention, \\\n",
    "                         avg_sub_char_abs_attention, avg_multi_char_attention, \\\n",
    "                         att_maxpooling_left, att_maxpooling_right, \\\n",
    "                         att_avgpooling_left, att_avgpooling_right, \\\n",
    "                         max_sub_char_abs, max_multi_char, avg_sub_char_abs, avg_multi_char, \\\n",
    "                         flatten_cnn_word_q1, flatten_cnn_word_q2,])\n",
    "    \n",
    "    merged = Dropout(0.4)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dense(512, activation='relu')(merged)\n",
    "    merged = Dropout(0.1)(merged)\n",
    "    merged = BatchNormalization()(merged) \n",
    "    preds = Dense(19, activation = 'softmax')(merged)\n",
    "    \n",
    "    model = Model(inputs=[char_input],outputs=preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******0*******\n",
      "Train on 92040 samples, validate on 10237 samples\n",
      "Epoch 1/10\n",
      "92040/92040 [==============================] - 829s 9ms/step - loss: 1.0543 - acc: 0.6963 - val_loss: 0.8754 - val_acc: 0.7429\n",
      "Epoch 2/10\n",
      "92040/92040 [==============================] - 826s 9ms/step - loss: 0.5836 - acc: 0.8193 - val_loss: 0.7720 - val_acc: 0.7733\n",
      "Epoch 6/10\n",
      "92040/92040 [==============================] - 826s 9ms/step - loss: 0.5110 - acc: 0.8399 - val_loss: 0.8002 - val_acc: 0.7737\n",
      "Epoch 7/10\n",
      "92040/92040 [==============================] - 826s 9ms/step - loss: 0.4473 - acc: 0.8554 - val_loss: 0.8016 - val_acc: 0.7772\n",
      "Epoch 8/10\n",
      "92040/92040 [==============================] - 826s 9ms/step - loss: 0.3892 - acc: 0.8740 - val_loss: 0.8428 - val_acc: 0.7721\n",
      "Epoch 9/10\n",
      "92040/92040 [==============================] - 826s 9ms/step - loss: 0.3458 - acc: 0.8856 - val_loss: 0.8770 - val_acc: 0.7708\n",
      "Epoch 10/10\n",
      "92040/92040 [==============================] - 826s 9ms/step - loss: 0.3061 - acc: 0.8990 - val_loss: 0.8547 - val_acc: 0.7810\n",
      "10237/10237 [==============================] - 27s 3ms/step\n",
      "102277/102277 [==============================] - 259s 3ms/step\n",
      "*******1*******\n",
      "Train on 92043 samples, validate on 10234 samples\n",
      "Epoch 1/10\n",
      "92043/92043 [==============================] - 830s 9ms/step - loss: 1.0503 - acc: 0.6971 - val_loss: 1.1733 - val_acc: 0.6672\n",
      "Epoch 2/10\n",
      "92043/92043 [==============================] - 828s 9ms/step - loss: 0.8214 - acc: 0.7566 - val_loss: 0.9491 - val_acc: 0.7157\n",
      "Epoch 3/10\n",
      "92043/92043 [==============================] - 827s 9ms/step - loss: 0.7383 - acc: 0.7778 - val_loss: 0.8134 - val_acc: 0.7611\n",
      "Epoch 4/10\n",
      "92043/92043 [==============================] - 827s 9ms/step - loss: 0.6602 - acc: 0.7992 - val_loss: 0.8182 - val_acc: 0.7624\n",
      "Epoch 5/10\n",
      "92043/92043 [==============================] - 827s 9ms/step - loss: 0.5864 - acc: 0.8178 - val_loss: 0.7886 - val_acc: 0.7726\n",
      "Epoch 6/10\n",
      "92043/92043 [==============================] - 827s 9ms/step - loss: 0.5158 - acc: 0.8366 - val_loss: 0.8048 - val_acc: 0.7752\n",
      "Epoch 7/10\n",
      "92043/92043 [==============================] - 827s 9ms/step - loss: 0.4487 - acc: 0.8556 - val_loss: 0.8389 - val_acc: 0.7729\n",
      "Epoch 8/10\n",
      "92043/92043 [==============================] - 827s 9ms/step - loss: 0.4474 - acc: 0.8561 - val_loss: 0.8513 - val_acc: 0.7628\n",
      "Epoch 9/10\n",
      "92043/92043 [==============================] - 827s 9ms/step - loss: 0.3812 - acc: 0.8752 - val_loss: 0.8744 - val_acc: 0.7731\n",
      "Epoch 10/10\n",
      "92043/92043 [==============================] - 827s 9ms/step - loss: 0.3203 - acc: 0.8934 - val_loss: 0.9323 - val_acc: 0.7707\n",
      "10234/10234 [==============================] - 26s 3ms/step\n",
      "102277/102277 [==============================] - 259s 3ms/step\n",
      "*******2*******\n",
      "Train on 92044 samples, validate on 10233 samples\n",
      "Epoch 1/10\n",
      "92044/92044 [==============================] - 828s 9ms/step - loss: 1.0465 - acc: 0.6986 - val_loss: 0.8679 - val_acc: 0.7484\n",
      "Epoch 2/10\n",
      "92044/92044 [==============================] - 826s 9ms/step - loss: 0.8224 - acc: 0.7570 - val_loss: 0.7897 - val_acc: 0.7657\n",
      "Epoch 3/10\n",
      "92044/92044 [==============================] - 826s 9ms/step - loss: 0.7343 - acc: 0.7782 - val_loss: 0.7582 - val_acc: 0.7737\n",
      "Epoch 4/10\n",
      "92044/92044 [==============================] - 826s 9ms/step - loss: 0.7022 - acc: 0.7866 - val_loss: 1.0616 - val_acc: 0.6932\n",
      "Epoch 5/10\n",
      "92044/92044 [==============================] - 827s 9ms/step - loss: 0.6105 - acc: 0.8113 - val_loss: 1.1452 - val_acc: 0.6878\n",
      "Epoch 6/10\n",
      "92044/92044 [==============================] - 827s 9ms/step - loss: 0.5339 - acc: 0.8317 - val_loss: 1.2176 - val_acc: 0.6721\n",
      "Epoch 7/10\n",
      "92044/92044 [==============================] - 826s 9ms/step - loss: 0.4643 - acc: 0.8517 - val_loss: 0.8926 - val_acc: 0.7527\n",
      "Epoch 8/10\n",
      "92044/92044 [==============================] - 827s 9ms/step - loss: 0.4012 - acc: 0.8697 - val_loss: 0.8161 - val_acc: 0.7707\n",
      "Epoch 9/10\n",
      "92044/92044 [==============================] - 826s 9ms/step - loss: 0.3499 - acc: 0.8852 - val_loss: 0.9828 - val_acc: 0.7494\n",
      "Epoch 10/10\n",
      "92044/92044 [==============================] - 827s 9ms/step - loss: 0.3149 - acc: 0.8962 - val_loss: 0.8518 - val_acc: 0.7717\n",
      "10233/10233 [==============================] - 26s 3ms/step\n",
      "102277/102277 [==============================] - 259s 3ms/step\n",
      "*******3*******\n",
      "Train on 92046 samples, validate on 10231 samples\n",
      "Epoch 1/10\n",
      "92046/92046 [==============================] - 829s 9ms/step - loss: 1.0570 - acc: 0.6974 - val_loss: 0.8472 - val_acc: 0.7526\n",
      "Epoch 2/10\n",
      "92046/92046 [==============================] - 826s 9ms/step - loss: 0.8252 - acc: 0.7556 - val_loss: 0.9529 - val_acc: 0.7257\n",
      "Epoch 3/10\n",
      "92046/92046 [==============================] - 827s 9ms/step - loss: 0.7365 - acc: 0.7779 - val_loss: 0.8043 - val_acc: 0.7610\n",
      "Epoch 4/10\n",
      "92046/92046 [==============================] - 828s 9ms/step - loss: 0.6632 - acc: 0.7969 - val_loss: 0.7801 - val_acc: 0.7693\n",
      "Epoch 5/10\n",
      "92046/92046 [==============================] - 827s 9ms/step - loss: 0.5886 - acc: 0.8172 - val_loss: 0.8079 - val_acc: 0.7689\n",
      "Epoch 6/10\n",
      "92046/92046 [==============================] - 827s 9ms/step - loss: 0.5133 - acc: 0.8373 - val_loss: 0.7852 - val_acc: 0.7770\n",
      "Epoch 7/10\n",
      "92046/92046 [==============================] - 827s 9ms/step - loss: 0.4447 - acc: 0.8575 - val_loss: 0.8045 - val_acc: 0.7794\n",
      "Epoch 8/10\n",
      "92046/92046 [==============================] - 828s 9ms/step - loss: 0.3927 - acc: 0.8730 - val_loss: 0.8072 - val_acc: 0.7782\n",
      "Epoch 9/10\n",
      "92046/92046 [==============================] - 828s 9ms/step - loss: 0.3428 - acc: 0.8872 - val_loss: 0.8836 - val_acc: 0.7679\n",
      "Epoch 10/10\n",
      "92046/92046 [==============================] - 828s 9ms/step - loss: 0.3079 - acc: 0.8977 - val_loss: 0.8845 - val_acc: 0.7731\n",
      "10231/10231 [==============================] - 26s 3ms/step\n",
      "102277/102277 [==============================] - 259s 3ms/step\n",
      "*******4*******\n",
      "Train on 92049 samples, validate on 10228 samples\n",
      "Epoch 1/10\n",
      "92049/92049 [==============================] - 829s 9ms/step - loss: 1.0488 - acc: 0.6989 - val_loss: 0.8893 - val_acc: 0.7370\n",
      "Epoch 2/10\n",
      "92049/92049 [==============================] - 827s 9ms/step - loss: 0.8156 - acc: 0.7580 - val_loss: 0.8264 - val_acc: 0.7578\n",
      "Epoch 3/10\n",
      "92049/92049 [==============================] - 827s 9ms/step - loss: 0.7302 - acc: 0.7803 - val_loss: 0.7855 - val_acc: 0.7668\n",
      "Epoch 4/10\n",
      "92049/92049 [==============================] - 826s 9ms/step - loss: 0.6633 - acc: 0.7983 - val_loss: 0.7619 - val_acc: 0.7746\n",
      "Epoch 5/10\n",
      "92049/92049 [==============================] - 826s 9ms/step - loss: 0.5847 - acc: 0.8176 - val_loss: 0.7897 - val_acc: 0.7660\n",
      "Epoch 6/10\n",
      "92049/92049 [==============================] - 826s 9ms/step - loss: 0.5131 - acc: 0.8373 - val_loss: 0.7665 - val_acc: 0.7809\n",
      "Epoch 7/10\n",
      "92049/92049 [==============================] - 827s 9ms/step - loss: 0.4472 - acc: 0.8565 - val_loss: 0.7915 - val_acc: 0.7772\n",
      "Epoch 8/10\n",
      "92049/92049 [==============================] - 826s 9ms/step - loss: 0.3904 - acc: 0.8730 - val_loss: 0.8035 - val_acc: 0.7751\n",
      "Epoch 9/10\n",
      "92049/92049 [==============================] - 827s 9ms/step - loss: 0.3403 - acc: 0.8884 - val_loss: 0.8620 - val_acc: 0.7745\n",
      "Epoch 10/10\n",
      "92049/92049 [==============================] - 827s 9ms/step - loss: 0.3047 - acc: 0.8986 - val_loss: 0.8875 - val_acc: 0.7782\n",
      "10228/10228 [==============================] - 26s 3ms/step\n",
      "102277/102277 [==============================] - 258s 3ms/step\n",
      "*******5*******\n",
      "Train on 92051 samples, validate on 10226 samples\n",
      "Epoch 1/10\n",
      "92051/92051 [==============================] - 828s 9ms/step - loss: 1.0486 - acc: 0.6975 - val_loss: 0.9867 - val_acc: 0.7129\n",
      "Epoch 2/10\n",
      "92051/92051 [==============================] - 825s 9ms/step - loss: 0.8136 - acc: 0.7579 - val_loss: 0.8330 - val_acc: 0.7535\n",
      "Epoch 3/10\n",
      "92051/92051 [==============================] - 825s 9ms/step - loss: 0.7284 - acc: 0.7792 - val_loss: 0.8183 - val_acc: 0.7617\n",
      "Epoch 4/10\n",
      "92051/92051 [==============================] - 825s 9ms/step - loss: 0.6545 - acc: 0.8012 - val_loss: 6.7206 - val_acc: 0.1680\n",
      "Epoch 5/10\n",
      "92051/92051 [==============================] - 825s 9ms/step - loss: 0.6206 - acc: 0.8079 - val_loss: 0.7781 - val_acc: 0.7691\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92051/92051 [==============================] - 825s 9ms/step - loss: 0.5194 - acc: 0.8365 - val_loss: 0.7788 - val_acc: 0.7736\n",
      "Epoch 7/10\n",
      "92051/92051 [==============================] - 825s 9ms/step - loss: 0.4552 - acc: 0.8531 - val_loss: 0.8043 - val_acc: 0.7712\n",
      "Epoch 8/10\n",
      "92051/92051 [==============================] - 825s 9ms/step - loss: 0.3987 - acc: 0.8706 - val_loss: 0.8247 - val_acc: 0.7724\n",
      "Epoch 9/10\n",
      "92051/92051 [==============================] - 825s 9ms/step - loss: 0.3500 - acc: 0.8847 - val_loss: 0.8774 - val_acc: 0.7714\n",
      "Epoch 10/10\n",
      "92051/92051 [==============================] - 825s 9ms/step - loss: 0.3082 - acc: 0.8977 - val_loss: 0.9143 - val_acc: 0.7708\n",
      "10226/10226 [==============================] - 26s 3ms/step\n",
      "102277/102277 [==============================] - 259s 3ms/step\n",
      "*******6*******\n",
      "Train on 92053 samples, validate on 10224 samples\n",
      "Epoch 1/10\n",
      "92053/92053 [==============================] - 828s 9ms/step - loss: 1.0546 - acc: 0.6964 - val_loss: 0.8404 - val_acc: 0.7579\n",
      "Epoch 2/10\n",
      "92053/92053 [==============================] - 826s 9ms/step - loss: 0.5190 - acc: 0.8365 - val_loss: 0.7419 - val_acc: 0.7913\n",
      "Epoch 7/10\n",
      "92053/92053 [==============================] - 827s 9ms/step - loss: 0.4527 - acc: 0.8561 - val_loss: 0.7579 - val_acc: 0.7912\n",
      "Epoch 8/10\n",
      "92053/92053 [==============================] - 826s 9ms/step - loss: 0.3945 - acc: 0.8723 - val_loss: 0.7814 - val_acc: 0.7883\n",
      "Epoch 9/10\n",
      "92053/92053 [==============================] - 828s 9ms/step - loss: 0.3419 - acc: 0.8877 - val_loss: 0.7939 - val_acc: 0.7894\n",
      "Epoch 10/10\n",
      "92053/92053 [==============================] - 830s 9ms/step - loss: 0.3056 - acc: 0.8982 - val_loss: 0.8512 - val_acc: 0.7824\n",
      "10224/10224 [==============================] - 27s 3ms/step\n",
      "102277/102277 [==============================] - 263s 3ms/step\n",
      "*******7*******\n",
      "Train on 92054 samples, validate on 10223 samples\n",
      "Epoch 1/10\n",
      "92054/92054 [==============================] - 831s 9ms/step - loss: 1.0526 - acc: 0.6968 - val_loss: 1.1809 - val_acc: 0.6633\n",
      "Epoch 2/10\n",
      "92054/92054 [==============================] - 829s 9ms/step - loss: 0.8150 - acc: 0.7572 - val_loss: 0.8435 - val_acc: 0.7556\n",
      "Epoch 3/10\n",
      "92054/92054 [==============================] - 828s 9ms/step - loss: 0.7300 - acc: 0.7803 - val_loss: 0.8116 - val_acc: 0.7684\n",
      "Epoch 4/10\n",
      "92054/92054 [==============================] - 827s 9ms/step - loss: 0.6633 - acc: 0.7988 - val_loss: 0.8396 - val_acc: 0.7539\n",
      "Epoch 5/10\n",
      "92054/92054 [==============================] - 826s 9ms/step - loss: 0.5867 - acc: 0.8198 - val_loss: 0.7733 - val_acc: 0.7793\n",
      "Epoch 6/10\n",
      "92054/92054 [==============================] - 827s 9ms/step - loss: 0.5064 - acc: 0.8405 - val_loss: 0.7859 - val_acc: 0.7804\n",
      "Epoch 7/10\n",
      "92054/92054 [==============================] - 827s 9ms/step - loss: 0.4421 - acc: 0.8574 - val_loss: 0.8074 - val_acc: 0.7785\n",
      "Epoch 8/10\n",
      "92054/92054 [==============================] - 826s 9ms/step - loss: 0.3883 - acc: 0.8737 - val_loss: 0.8379 - val_acc: 0.7762\n",
      "Epoch 9/10\n",
      "92054/92054 [==============================] - 827s 9ms/step - loss: 0.3387 - acc: 0.8884 - val_loss: 0.8502 - val_acc: 0.7776\n",
      "Epoch 10/10\n",
      "92054/92054 [==============================] - 827s 9ms/step - loss: 0.3033 - acc: 0.8992 - val_loss: 0.8817 - val_acc: 0.7769\n",
      "10223/10223 [==============================] - 27s 3ms/step\n",
      "102277/102277 [==============================] - 261s 3ms/step\n",
      "*******8*******\n",
      "Train on 92056 samples, validate on 10221 samples\n",
      "Epoch 1/10\n",
      "92056/92056 [==============================] - 829s 9ms/step - loss: 1.0539 - acc: 0.6975 - val_loss: 0.8548 - val_acc: 0.7506\n",
      "Epoch 2/10\n",
      "92056/92056 [==============================] - 827s 9ms/step - loss: 0.8189 - acc: 0.7578 - val_loss: 0.7831 - val_acc: 0.7648\n",
      "Epoch 3/10\n",
      "92056/92056 [==============================] - 826s 9ms/step - loss: 0.7341 - acc: 0.7788 - val_loss: 0.7495 - val_acc: 0.7750\n",
      "Epoch 4/10\n",
      "92056/92056 [==============================] - 826s 9ms/step - loss: 0.6620 - acc: 0.7979 - val_loss: 0.7304 - val_acc: 0.7781\n",
      "Epoch 5/10\n",
      "92056/92056 [==============================] - 826s 9ms/step - loss: 0.5913 - acc: 0.8173 - val_loss: 0.7412 - val_acc: 0.7830\n",
      "Epoch 6/10\n",
      "92056/92056 [==============================] - 826s 9ms/step - loss: 0.5132 - acc: 0.8367 - val_loss: 0.7679 - val_acc: 0.7809\n",
      "Epoch 7/10\n",
      "92056/92056 [==============================] - 827s 9ms/step - loss: 0.4456 - acc: 0.8570 - val_loss: 0.7504 - val_acc: 0.7871\n",
      "Epoch 8/10\n",
      "92056/92056 [==============================] - 827s 9ms/step - loss: 0.3893 - acc: 0.8732 - val_loss: 0.7964 - val_acc: 0.7867\n",
      "Epoch 9/10\n",
      "92056/92056 [==============================] - 828s 9ms/step - loss: 0.3427 - acc: 0.8866 - val_loss: 0.8097 - val_acc: 0.7859\n",
      "Epoch 10/10\n",
      "92056/92056 [==============================] - 827s 9ms/step - loss: 0.3034 - acc: 0.8998 - val_loss: 0.8481 - val_acc: 0.7823\n",
      "10221/10221 [==============================] - 26s 3ms/step\n",
      "102277/102277 [==============================] - 260s 3ms/step\n",
      "*******9*******\n",
      "Train on 92057 samples, validate on 10220 samples\n",
      "Epoch 1/10\n",
      "92057/92057 [==============================] - 830s 9ms/step - loss: 1.0515 - acc: 0.6979 - val_loss: 0.9268 - val_acc: 0.7258\n",
      "Epoch 2/10\n",
      "92057/92057 [==============================] - 827s 9ms/step - loss: 0.8134 - acc: 0.7588 - val_loss: 0.8167 - val_acc: 0.7617\n",
      "Epoch 3/10\n",
      "92057/92057 [==============================] - 826s 9ms/step - loss: 0.7336 - acc: 0.7792 - val_loss: 0.7901 - val_acc: 0.7605\n",
      "Epoch 4/10\n",
      "92057/92057 [==============================] - 828s 9ms/step - loss: 0.6641 - acc: 0.7969 - val_loss: 0.7710 - val_acc: 0.7690\n",
      "Epoch 5/10\n",
      "92057/92057 [==============================] - 827s 9ms/step - loss: 0.5882 - acc: 0.8180 - val_loss: 0.7551 - val_acc: 0.7755\n",
      "Epoch 6/10\n",
      "92057/92057 [==============================] - 828s 9ms/step - loss: 0.5120 - acc: 0.8374 - val_loss: 0.7556 - val_acc: 0.7815\n",
      "Epoch 7/10\n",
      "92057/92057 [==============================] - 826s 9ms/step - loss: 0.4468 - acc: 0.8566 - val_loss: 0.8147 - val_acc: 0.7680\n",
      "Epoch 8/10\n",
      "92057/92057 [==============================] - 826s 9ms/step - loss: 0.3915 - acc: 0.8740 - val_loss: 0.7911 - val_acc: 0.7786\n",
      "Epoch 9/10\n",
      "92057/92057 [==============================] - 827s 9ms/step - loss: 0.3405 - acc: 0.8869 - val_loss: 0.8296 - val_acc: 0.7762\n",
      "Epoch 10/10\n",
      "92057/92057 [==============================] - 826s 9ms/step - loss: 0.3452 - acc: 0.8868 - val_loss: 0.8052 - val_acc: 0.7739\n",
      "10220/10220 [==============================] - 27s 3ms/step\n",
      "102277/102277 [==============================] - 260s 3ms/step\n",
      "CPU times: user 23h 48min 10s, sys: 1h 33min 32s, total: 1d 1h 21min 43s\n",
      "Wall time: 23h 50min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#logloss_list = []\n",
    "for i in range(10):\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    print('*******%s*******'%i)\n",
    "    idx_train = np.array(index[index['fold'] != i]['id'])\n",
    "    idx_val = np.array(index[index['fold'] == i]['id'])\n",
    "    \n",
    "    \n",
    "    X_train = train_FE.loc[idx_train, :]\n",
    "    y_train = categorical_labels[idx_train]\n",
    "\n",
    "    X_val = train_FE.loc[idx_val, :]\n",
    "    y_val = categorical_labels[idx_val]\n",
    "\n",
    "    #dtest = test.loc[:,user_col]\n",
    "    \n",
    "    X_train = get_input(X_train)\n",
    "    X_val = get_input(X_val)\n",
    "    X_test = get_input(test_FE)\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    model = Globel_attention_RCNN()\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',\\\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    hist = model.fit(X_train, y_train, \\\n",
    "            validation_data=(X_val, y_val), \\\n",
    "            epochs=10, batch_size=BATCH_SIZE, shuffle=True)#callbacks=callbacks\n",
    "\n",
    "    \n",
    "    pred = pd.DataFrame(model.predict(X_val,batch_size=128,verbose=1))\n",
    "    \n",
    "    for i in range(19):\n",
    "        meta_train.loc[idx_val, 'prob' + str(i)] = pred[i].values\n",
    "    \n",
    "    prob = model.predict(X_test,batch_size=128,verbose=1)\n",
    "    meta_test = pd.concat([meta_test, pd.DataFrame(prob)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train.columns = ['meta_word_v2_300dim_' + x for x in meta_train.columns]\n",
    "\n",
    "p = pd.DataFrame()\n",
    "\n",
    "for i in range(19):\n",
    "    p[i] = meta_test[i].mean(axis = 1)\n",
    "\n",
    "p.columns = meta_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((102277, 19), (102277, 19))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape, meta_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train.to_csv('/home/libo/daguan/stack_new/train_meta_word_v2_300dim.csv', index = None ,encoding = 'utf-8')\n",
    "\n",
    "p.to_csv('/home/libo/daguan/stack_new/test_meta_meta_word_v2_300dim.csv', index = None ,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
