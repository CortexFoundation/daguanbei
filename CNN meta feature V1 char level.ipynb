{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "window.load_remote_theme = true\n",
       "var theme_js = \"https://odhk.github.io/hyrule_theme/custom.js\";\n",
       "\n",
       "window.load_local_theme = function(){\n",
       "    var hostname = document.location.hostname\n",
       "    return ((hostname == \"localhost\" || hostname == '127.0.0.1') && !load_remote_theme)\n",
       "}\n",
       "\n",
       "var url = load_local_theme() ? document.location.origin + \"/files/theme/custom.js\" : theme_js\n",
       "\n",
       "$.getScript(url)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "window.load_remote_theme = true\n",
    "var theme_js = \"https://odhk.github.io/hyrule_theme/custom.js\";\n",
    "\n",
    "window.load_local_theme = function(){\n",
    "    var hostname = document.location.hostname\n",
    "    return ((hostname == \"localhost\" || hostname == '127.0.0.1') && !load_remote_theme)\n",
    "}\n",
    "\n",
    "var url = load_local_theme() ? document.location.origin + \"/files/theme/custom.js\" : theme_js\n",
    "\n",
    "$.getScript(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/libo/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import keras\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Lambda, Embedding,constraints,\\\n",
    "Dropout, Activation,GRU,Bidirectional,Subtract, Permute, TimeDistributed, Reshape\n",
    "from keras.layers import Conv1D,Conv2D,MaxPooling2D,GlobalAveragePooling1D,GlobalMaxPooling1D, MaxPooling1D, Flatten\n",
    "from keras.layers import CuDNNGRU, CuDNNLSTM, SpatialDropout1D,Layer, initializers, regularizers\n",
    "from keras.layers.merge import concatenate, Concatenate, Average, Dot, Maximum, Multiply, Subtract\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from keras.activations import softmax\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import *\n",
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy.special import erfinv\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_set.csv')\n",
    "test = pd.read_csv('test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label = train['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "api_embedding_file_path = 'word_embedding_300dim_new.txt'\n",
    "embedding_dim = 300\n",
    "max_nb_api = 20000\n",
    "max_sequence_length_api = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16053it [00:01, 9306.08it/s]\n"
     ]
    }
   ],
   "source": [
    "char_embedding_index = {}\n",
    "f = open('word_embedding_300dim_new.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0].lower()\n",
    "    embeddings = np.asarray(values[1:], dtype = 'float32')\n",
    "    char_embedding_index[word] = embeddings\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204554"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_word = list(train['article'].values) + list(test['article'].values)\n",
    "\n",
    "all_word_list = []\n",
    "for i in list(all_word):\n",
    "    all_word_list.append(i.split(' '))\n",
    "\n",
    "len(all_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer_api = Tokenizer()\n",
    "\n",
    "tokenizer_api.fit_on_texts(all_word_list)\n",
    "\n",
    "api_seq = tokenizer_api.texts_to_sequences(all_word_list)\n",
    "\n",
    "data['char_seq'] = pad_sequences(api_seq, maxlen = max_sequence_length_api, padding='post').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16052 unique tokens\n"
     ]
    }
   ],
   "source": [
    "api_index = tokenizer_api.word_index\n",
    "print('Found %s unique tokens' % len(api_index))\n",
    "nb_words = min(max_nb_api, len(char_embedding_index)+1)\n",
    "api_embedding_matrix = np.zeros((nb_words, embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16052/16052 [00:00<00:00, 198183.12it/s]\n"
     ]
    }
   ],
   "source": [
    "for word, i in tqdm(api_index.items()):\n",
    "    embedding_vector = char_embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        api_embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        print('no char%s'%word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['id'] = train['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_FE = data.iloc[:len(label),:]\n",
    "test_FE = data.iloc[len(label):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_input(df):\n",
    "    \n",
    "    _input = [np.array(df.char_seq.values.tolist())]\n",
    "    \n",
    "    \n",
    "    return _input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CNN1():\n",
    "    \n",
    "    #Input layer\n",
    "    char_input = Input(shape=(max_sequence_length_api,), dtype='int32')\n",
    "    \n",
    "    api_embedding_layer = Embedding(nb_words,\n",
    "        embedding_dim,\n",
    "        weights=[api_embedding_matrix],\n",
    "        input_length=max_sequence_length_api,\n",
    "        trainable=False)\n",
    "    \n",
    "    char_cnn = api_embedding_layer(char_input)\n",
    "    \n",
    "    kernel_sizes = [1, 2, 3, 4, 5, 6]\n",
    "    pooled_word_q1 = []\n",
    "    pooled_word_q2 = []\n",
    "    \n",
    "    for kernel in kernel_sizes:\n",
    "            \n",
    "        conv_word_q1 = Conv1D(filters=300,\n",
    "                          kernel_size=kernel,\n",
    "                          padding='same',\n",
    "                          strides=1, \n",
    "                          kernel_initializer='he_uniform',\n",
    "                          activation='relu')(char_cnn)\n",
    "\n",
    "        pool_word_q1 = MaxPooling1D(pool_size = max_sequence_length_api)(conv_word_q1)\n",
    "        pool_word_q2 = AveragePooling1D(pool_size = max_sequence_length_api)(conv_word_q1)\n",
    "        \n",
    "        pooled_word_q1.append(pool_word_q1)\n",
    "        pooled_word_q2.append(pool_word_q2)\n",
    "        \n",
    "    merged_cnn_word_q1 = Concatenate(axis=-1)(pooled_word_q1)\n",
    "    merged_cnn_word_q2 = Concatenate(axis=-1)(pooled_word_q2)\n",
    "    \n",
    "    flatten_cnn_word_q1 = Flatten()(merged_cnn_word_q1)\n",
    "    flatten_cnn_word_q2 = Flatten()(merged_cnn_word_q2)\n",
    "    \n",
    "    merged = concatenate([flatten_cnn_word_q1, flatten_cnn_word_q2])\n",
    "    merged = Dropout(0.2)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dense(300, activation='relu')(merged)\n",
    "    merged = Dropout(0.1)(merged)\n",
    "    merged = BatchNormalization()(merged) \n",
    "    preds = Dense(19, activation = 'softmax')(merged)\n",
    "    \n",
    "    model = Model(inputs=[char_input],outputs=preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label = train['class']\n",
    "\n",
    "label = label - 1\n",
    "\n",
    "categorical_labels = keras.utils.np_utils.to_categorical(label, num_classes=19)\n",
    "\n",
    "index = pd.read_csv('fold.csv')\n",
    "index['id'] = train['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train = pd.DataFrame(np.array([None]*train_FE.shape[0]*19).reshape(train_FE.shape[0], 19))\n",
    "meta_train.columns = ['prob' + str(x) for x in range(19)]\n",
    "meta_test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******0*******\n",
      "Train on 92040 samples, validate on 10237 samples\n",
      "Epoch 1/6\n",
      "92040/92040 [==============================] - 598s 6ms/step - loss: 0.6961 - acc: 0.7891 - val_loss: 0.8310 - val_acc: 0.7541\n",
      "Epoch 4/6\n",
      "92040/92040 [==============================] - 598s 6ms/step - loss: 0.5801 - acc: 0.8194 - val_loss: 0.8544 - val_acc: 0.7564\n",
      "Epoch 5/6\n",
      "92040/92040 [==============================] - 598s 6ms/step - loss: 0.4731 - acc: 0.8505 - val_loss: 0.9055 - val_acc: 0.7561\n",
      "Epoch 6/6\n",
      "92040/92040 [==============================] - 598s 6ms/step - loss: 0.3841 - acc: 0.8762 - val_loss: 0.9430 - val_acc: 0.7563\n",
      "10237/10237 [==============================] - 16s 2ms/step\n",
      "102277/102277 [==============================] - 152s 1ms/step\n",
      "*******1*******\n",
      "Train on 92043 samples, validate on 10234 samples\n",
      "Epoch 1/6\n",
      "92043/92043 [==============================] - 600s 7ms/step - loss: 1.1463 - acc: 0.6676 - val_loss: 0.9146 - val_acc: 0.7368\n",
      "Epoch 2/6\n",
      "92043/92043 [==============================] - 599s 7ms/step - loss: 0.8320 - acc: 0.7537 - val_loss: 0.8782 - val_acc: 0.7462\n",
      "Epoch 3/6\n",
      "92043/92043 [==============================] - 599s 7ms/step - loss: 0.6996 - acc: 0.7881 - val_loss: 0.8805 - val_acc: 0.7495\n",
      "Epoch 4/6\n",
      "92043/92043 [==============================] - 598s 7ms/step - loss: 0.5835 - acc: 0.8186 - val_loss: 0.8925 - val_acc: 0.7514\n",
      "Epoch 5/6\n",
      "92043/92043 [==============================] - 599s 7ms/step - loss: 0.4792 - acc: 0.8479 - val_loss: 0.9263 - val_acc: 0.7517\n",
      "Epoch 6/6\n",
      "92043/92043 [==============================] - 598s 6ms/step - loss: 0.3889 - acc: 0.8748 - val_loss: 1.0085 - val_acc: 0.7494\n",
      "10234/10234 [==============================] - 16s 2ms/step\n",
      "102277/102277 [==============================] - 152s 1ms/step\n",
      "*******2*******\n",
      "Train on 92044 samples, validate on 10233 samples\n",
      "Epoch 1/6\n",
      "92044/92044 [==============================] - 600s 7ms/step - loss: 1.1519 - acc: 0.6651 - val_loss: 0.8812 - val_acc: 0.7435\n",
      "Epoch 2/6\n",
      "92044/92044 [==============================] - 599s 7ms/step - loss: 0.8285 - acc: 0.7532 - val_loss: 0.8286 - val_acc: 0.7555\n",
      "Epoch 3/6\n",
      "92044/92044 [==============================] - 598s 6ms/step - loss: 0.7014 - acc: 0.7877 - val_loss: 0.8289 - val_acc: 0.7563\n",
      "Epoch 4/6\n",
      "92044/92044 [==============================] - 598s 6ms/step - loss: 0.5869 - acc: 0.8181 - val_loss: 0.8357 - val_acc: 0.7619\n",
      "Epoch 5/6\n",
      "92044/92044 [==============================] - 598s 6ms/step - loss: 0.4779 - acc: 0.8492 - val_loss: 0.8701 - val_acc: 0.7644\n",
      "Epoch 6/6\n",
      "92044/92044 [==============================] - 598s 6ms/step - loss: 0.3886 - acc: 0.8753 - val_loss: 0.9425 - val_acc: 0.7505\n",
      "10233/10233 [==============================] - 16s 2ms/step\n",
      "102277/102277 [==============================] - 152s 1ms/step\n",
      "*******3*******\n",
      "Train on 92046 samples, validate on 10231 samples\n",
      "Epoch 1/6\n",
      "92046/92046 [==============================] - 601s 7ms/step - loss: 1.1501 - acc: 0.6656 - val_loss: 0.8929 - val_acc: 0.7418\n",
      "Epoch 2/6\n",
      "92046/92046 [==============================] - 599s 7ms/step - loss: 0.8353 - acc: 0.7524 - val_loss: 0.8414 - val_acc: 0.7522\n",
      "Epoch 3/6\n",
      "92046/92046 [==============================] - 598s 6ms/step - loss: 0.7026 - acc: 0.7866 - val_loss: 0.8320 - val_acc: 0.7580\n",
      "Epoch 4/6\n",
      "92046/92046 [==============================] - 599s 7ms/step - loss: 0.5855 - acc: 0.8186 - val_loss: 0.8545 - val_acc: 0.7595\n",
      "Epoch 5/6\n",
      "92046/92046 [==============================] - 617s 7ms/step - loss: 0.4806 - acc: 0.8475 - val_loss: 0.9061 - val_acc: 0.7526\n",
      "Epoch 6/6\n",
      "92046/92046 [==============================] - 605s 7ms/step - loss: 0.3905 - acc: 0.8752 - val_loss: 0.9446 - val_acc: 0.7580\n",
      "10231/10231 [==============================] - 20s 2ms/step\n",
      "102277/102277 [==============================] - 152s 1ms/step\n",
      "*******4*******\n",
      "Train on 92049 samples, validate on 10228 samples\n",
      "Epoch 1/6\n",
      "92049/92049 [==============================] - 634s 7ms/step - loss: 1.1471 - acc: 0.6677 - val_loss: 0.8670 - val_acc: 0.7481\n",
      "Epoch 2/6\n",
      "92049/92049 [==============================] - 599s 7ms/step - loss: 0.8368 - acc: 0.7523 - val_loss: 0.8211 - val_acc: 0.7567\n",
      "Epoch 3/6\n",
      "92049/92049 [==============================] - 603s 7ms/step - loss: 0.7049 - acc: 0.7867 - val_loss: 0.8350 - val_acc: 0.7572\n",
      "Epoch 4/6\n",
      "92049/92049 [==============================] - 609s 7ms/step - loss: 0.5864 - acc: 0.8181 - val_loss: 0.8400 - val_acc: 0.7598\n",
      "Epoch 5/6\n",
      "92049/92049 [==============================] - 627s 7ms/step - loss: 0.4840 - acc: 0.8473 - val_loss: 0.8826 - val_acc: 0.7619\n",
      "Epoch 6/6\n",
      "92049/92049 [==============================] - 600s 7ms/step - loss: 0.3945 - acc: 0.8722 - val_loss: 0.9202 - val_acc: 0.7638\n",
      "10228/10228 [==============================] - 17s 2ms/step\n",
      "102277/102277 [==============================] - 181s 2ms/step\n",
      "*******5*******\n",
      "Train on 92051 samples, validate on 10226 samples\n",
      "Epoch 1/6\n",
      "92051/92051 [==============================] - 681s 7ms/step - loss: 1.1521 - acc: 0.6660 - val_loss: 0.8711 - val_acc: 0.7457\n",
      "Epoch 2/6\n",
      "92051/92051 [==============================] - 735s 8ms/step - loss: 0.8310 - acc: 0.7516 - val_loss: 0.8347 - val_acc: 0.7522\n",
      "Epoch 3/6\n",
      "92051/92051 [==============================] - 719s 8ms/step - loss: 0.6971 - acc: 0.7894 - val_loss: 0.8373 - val_acc: 0.7526\n",
      "Epoch 4/6\n",
      "92051/92051 [==============================] - 900s 10ms/step - loss: 0.5878 - acc: 0.8174 - val_loss: 0.8394 - val_acc: 0.7590\n",
      "Epoch 5/6\n",
      "92051/92051 [==============================] - 638s 7ms/step - loss: 0.4807 - acc: 0.8478 - val_loss: 0.9051 - val_acc: 0.7548\n",
      "Epoch 6/6\n",
      "92051/92051 [==============================] - 625s 7ms/step - loss: 0.3864 - acc: 0.8765 - val_loss: 0.9539 - val_acc: 0.7576\n",
      "10226/10226 [==============================] - 17s 2ms/step\n",
      "102277/102277 [==============================] - 204s 2ms/step\n",
      "*******6*******\n",
      "Train on 92053 samples, validate on 10224 samples\n",
      "Epoch 1/6\n",
      "92053/92053 [==============================] - 642s 7ms/step - loss: 1.1510 - acc: 0.6659 - val_loss: 0.8448 - val_acc: 0.7501\n",
      "Epoch 2/6\n",
      "92053/92053 [==============================] - 710s 8ms/step - loss: 0.8307 - acc: 0.7530 - val_loss: 0.8042 - val_acc: 0.7620\n",
      "Epoch 3/6\n",
      "92053/92053 [==============================] - 796s 9ms/step - loss: 0.7048 - acc: 0.7856 - val_loss: 0.8066 - val_acc: 0.7650\n",
      "Epoch 4/6\n",
      "92053/92053 [==============================] - 710s 8ms/step - loss: 0.5899 - acc: 0.8171 - val_loss: 0.8107 - val_acc: 0.7686\n",
      "Epoch 5/6\n",
      "92053/92053 [==============================] - 695s 8ms/step - loss: 0.4856 - acc: 0.8454 - val_loss: 0.8563 - val_acc: 0.7640\n",
      "Epoch 6/6\n",
      "92053/92053 [==============================] - 638s 7ms/step - loss: 0.3950 - acc: 0.8717 - val_loss: 0.8866 - val_acc: 0.7709\n",
      "10224/10224 [==============================] - 16s 2ms/step\n",
      "102277/102277 [==============================] - 166s 2ms/step\n",
      "*******7*******\n",
      "Train on 92054 samples, validate on 10223 samples\n",
      "Epoch 1/6\n",
      "92054/92054 [==============================] - 621s 7ms/step - loss: 1.1448 - acc: 0.6679 - val_loss: 0.9060 - val_acc: 0.7353\n",
      "Epoch 2/6\n",
      "92054/92054 [==============================] - 687s 7ms/step - loss: 0.8252 - acc: 0.7551 - val_loss: 0.8475 - val_acc: 0.7494\n",
      "Epoch 3/6\n",
      "92054/92054 [==============================] - 726s 8ms/step - loss: 0.6965 - acc: 0.7890 - val_loss: 0.8508 - val_acc: 0.7506\n",
      "Epoch 4/6\n",
      "92054/92054 [==============================] - 619s 7ms/step - loss: 0.5814 - acc: 0.8179 - val_loss: 0.8663 - val_acc: 0.7557\n",
      "Epoch 5/6\n",
      "92054/92054 [==============================] - 599s 7ms/step - loss: 0.4795 - acc: 0.8471 - val_loss: 0.8986 - val_acc: 0.7600\n",
      "Epoch 6/6\n",
      "92054/92054 [==============================] - 599s 7ms/step - loss: 0.3882 - acc: 0.8750 - val_loss: 0.9490 - val_acc: 0.7541\n",
      "10223/10223 [==============================] - 16s 2ms/step\n",
      "102277/102277 [==============================] - 214s 2ms/step\n",
      "*******8*******\n",
      "Train on 92056 samples, validate on 10221 samples\n",
      "Epoch 1/6\n",
      "92056/92056 [==============================] - 655s 7ms/step - loss: 1.1529 - acc: 0.6664 - val_loss: 0.8694 - val_acc: 0.7426\n",
      "Epoch 2/6\n",
      "92056/92056 [==============================] - 700s 8ms/step - loss: 0.8316 - acc: 0.7541 - val_loss: 0.8207 - val_acc: 0.7589\n",
      "Epoch 3/6\n",
      "92056/92056 [==============================] - 612s 7ms/step - loss: 0.7034 - acc: 0.7863 - val_loss: 0.8019 - val_acc: 0.7592\n",
      "Epoch 4/6\n",
      "92056/92056 [==============================] - 600s 7ms/step - loss: 0.5855 - acc: 0.8187 - val_loss: 0.8210 - val_acc: 0.7642\n",
      "Epoch 5/6\n",
      "92056/92056 [==============================] - 657s 7ms/step - loss: 0.4779 - acc: 0.8488 - val_loss: 0.8666 - val_acc: 0.7615\n",
      "Epoch 6/6\n",
      "92056/92056 [==============================] - 631s 7ms/step - loss: 0.3916 - acc: 0.8740 - val_loss: 0.9396 - val_acc: 0.7549\n",
      "10221/10221 [==============================] - 16s 2ms/step\n",
      "102277/102277 [==============================] - 166s 2ms/step\n",
      "*******9*******\n",
      "Train on 92057 samples, validate on 10220 samples\n",
      "Epoch 1/6\n",
      "92057/92057 [==============================] - 672s 7ms/step - loss: 1.1453 - acc: 0.6683 - val_loss: 0.8748 - val_acc: 0.7413\n",
      "Epoch 2/6\n",
      "92057/92057 [==============================] - 625s 7ms/step - loss: 0.8292 - acc: 0.7535 - val_loss: 0.8515 - val_acc: 0.7471\n",
      "Epoch 3/6\n",
      "92057/92057 [==============================] - 740s 8ms/step - loss: 0.7020 - acc: 0.7872 - val_loss: 0.8332 - val_acc: 0.7611\n",
      "Epoch 4/6\n",
      "92057/92057 [==============================] - 658s 7ms/step - loss: 0.5877 - acc: 0.8185 - val_loss: 0.8511 - val_acc: 0.7568\n",
      "Epoch 5/6\n",
      "92057/92057 [==============================] - 605s 7ms/step - loss: 0.4792 - acc: 0.8490 - val_loss: 0.8709 - val_acc: 0.7592\n",
      "Epoch 6/6\n",
      "92057/92057 [==============================] - 713s 8ms/step - loss: 0.3923 - acc: 0.8746 - val_loss: 0.9171 - val_acc: 0.7555\n",
      "10220/10220 [==============================] - 16s 2ms/step\n",
      "102277/102277 [==============================] - 369s 4ms/step\n",
      "CPU times: user 6h 24min 17s, sys: 2h 29min 2s, total: 8h 53min 20s\n",
      "Wall time: 11h 38min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#logloss_list = []\n",
    "for i in range(10):\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    print('*******%s*******'%i)\n",
    "    idx_train = np.array(index[index['fold'] != i]['id'])\n",
    "    idx_val = np.array(index[index['fold'] == i]['id'])\n",
    "    \n",
    "    \n",
    "    X_train = train_FE.loc[idx_train, :]\n",
    "    y_train = categorical_labels[idx_train]\n",
    "\n",
    "    X_val = train_FE.loc[idx_val, :]\n",
    "    y_val = categorical_labels[idx_val]\n",
    "\n",
    "    #dtest = test.loc[:,user_col]\n",
    "    \n",
    "    X_train = get_input(X_train)\n",
    "    X_val = get_input(X_val)\n",
    "    X_test = get_input(test_FE)\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "#     bst_model_path = '5folds_' + 'dae_nn_v1_' + str(i) + '.hdf5'\n",
    "#     early_stopping =EarlyStopping(monitor='val_loss', patience=10)\n",
    "#     model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "#     callbacks = [\n",
    "#             early_stopping,\n",
    "#             model_checkpoint\n",
    "#         ]\n",
    "    model = CNN1()\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',\\\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    hist = model.fit(X_train, y_train, \\\n",
    "            validation_data=(X_val, y_val), \\\n",
    "            epochs=6, batch_size=BATCH_SIZE, shuffle=True,)#callbacks=callbacks\n",
    "    \n",
    "#     model = NN()\n",
    "#     model.load_weights(bst_model_path)\n",
    "    \n",
    "    pred = pd.DataFrame(model.predict(X_val,batch_size=128,verbose=1))\n",
    "    \n",
    "    for i in range(19):\n",
    "        meta_train.loc[idx_val, 'prob' + str(i)] = pred[i].values\n",
    "    \n",
    "    prob = model.predict(X_test,batch_size=128,verbose=1)\n",
    "    meta_test = pd.concat([meta_test, pd.DataFrame(prob)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train.columns = ['meta_cnn_char_v1_' + x for x in meta_train.columns]\n",
    "\n",
    "p = pd.DataFrame()\n",
    "\n",
    "for i in range(19):\n",
    "    p[i] = meta_test[i].mean(axis = 1)\n",
    "\n",
    "p.columns = meta_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train.to_csv('/home/libo/daguan/stack/train_meta_cnn_char_v1.csv', index = None ,encoding = 'utf-8')\n",
    "\n",
    "p.to_csv('/home/libo/daguan/stack/test_meta_cnn_char_v1.csv', index = None ,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 2000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 2000, 300)    4816200     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 2000, 300)    90300       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 2000, 300)    180300      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 2000, 300)    270300      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 2000, 300)    360300      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 2000, 300)    450300      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 2000, 300)    540300      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1, 300)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1, 300)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1, 300)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1, 300)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 1, 300)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 1, 300)       0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_1 (AveragePoo (None, 1, 300)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_2 (AveragePoo (None, 1, 300)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_3 (AveragePoo (None, 1, 300)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_4 (AveragePoo (None, 1, 300)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_5 (AveragePoo (None, 1, 300)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_6 (AveragePoo (None, 1, 300)       0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 1800)      0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "                                                                 max_pooling1d_5[0][0]            \n",
      "                                                                 max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1, 1800)      0           average_pooling1d_1[0][0]        \n",
      "                                                                 average_pooling1d_2[0][0]        \n",
      "                                                                 average_pooling1d_3[0][0]        \n",
      "                                                                 average_pooling1d_4[0][0]        \n",
      "                                                                 average_pooling1d_5[0][0]        \n",
      "                                                                 average_pooling1d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1800)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 1800)         0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 3600)         0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 3600)         0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 3600)         14400       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 300)          1080300     batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 300)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 300)          1200        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 19)           5719        batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 7,809,619\n",
      "Trainable params: 2,985,619\n",
      "Non-trainable params: 4,824,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gap = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_sentence(data, k):\n",
    "    return data[:, k*gap:(k+1)*gap, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CNN1():\n",
    "    \n",
    "    #Input layer\n",
    "    char_input = Input(shape=(max_sequence_length_api,), dtype='int32')\n",
    "    \n",
    "    api_embedding_layer = Embedding(nb_words,\n",
    "        embedding_dim,\n",
    "        weights=[api_embedding_matrix],\n",
    "        input_length=max_sequence_length_api,\n",
    "        trainable=False)\n",
    "    \n",
    "    char_cnn = api_embedding_layer(char_input)\n",
    "    \n",
    "    pooled_word_q1 = []\n",
    "    \n",
    "    for k_ in range(20):\n",
    "        temp = Lambda(split_sentence, name = 'split_sentence'+str(k_), arguments={'k':k_})(char_cnn)\n",
    "\n",
    "        kernel_sizes = [1,2,3,4,5,6,7]\n",
    "\n",
    "        for kernel in kernel_sizes:\n",
    "\n",
    "            conv_word_q1 = Conv1D(filters=300,\n",
    "                              kernel_size=kernel,\n",
    "                              padding='same',\n",
    "                              strides=1, \n",
    "                              kernel_initializer='he_uniform',\n",
    "                              activation='relu')(temp)\n",
    "\n",
    "            pool_word_q1 = MaxPooling1D(pool_size = 100)(conv_word_q1)\n",
    "\n",
    "            pooled_word_q1.append(pool_word_q1)\n",
    "\n",
    "    merged_cnn_word_q1 = Concatenate(axis=-1)(pooled_word_q1)\n",
    "\n",
    "    flatten_cnn_word_q1 = Flatten()(merged_cnn_word_q1)\n",
    "    \n",
    "    merged = Dropout(0.2)(flatten_cnn_word_q1)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dense(300, activation='relu')(merged)\n",
    "    merged = Dropout(0.1)(merged)\n",
    "    merged = BatchNormalization()(merged) \n",
    "    preds = Dense(19, activation = 'softmax')(merged)\n",
    "    \n",
    "    model = Model(inputs=[char_input],outputs=preds)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******0*******\n",
      "Train on 92040 samples, validate on 10237 samples\n",
      "Epoch 1/30\n",
      "92040/92040 [==============================] - 989s 11ms/step - loss: 1.2921 - acc: 0.6281 - val_loss: 0.9760 - val_acc: 0.7153\n",
      "Epoch 2/30\n",
      "92040/92040 [==============================] - 800s 9ms/step - loss: 0.7106 - acc: 0.7916 - val_loss: 0.9816 - val_acc: 0.7244\n",
      "Epoch 3/30\n",
      "92040/92040 [==============================] - 717s 8ms/step - loss: 0.3684 - acc: 0.8940 - val_loss: 1.0647 - val_acc: 0.7311\n",
      "Epoch 4/30\n",
      "92040/92040 [==============================] - 2258s 25ms/step - loss: 0.2363 - acc: 0.9362 - val_loss: 1.1801 - val_acc: 0.7324\n",
      "Epoch 5/30\n",
      "92040/92040 [==============================] - 715s 8ms/step - loss: 0.1785 - acc: 0.9541 - val_loss: 1.2238 - val_acc: 0.7348\n",
      "Epoch 6/30\n",
      "92056/92056 [==============================] - 718s 8ms/step - loss: 0.1516 - acc: 0.9601 - val_loss: 1.2733 - val_acc: 0.7365\n",
      "Epoch 7/30\n",
      "92057/92057 [==============================] - 721s 8ms/step - loss: 0.3655 - acc: 0.8955 - val_loss: 1.1023 - val_acc: 0.7235\n",
      "Epoch 4/30\n",
      "92057/92057 [==============================] - 720s 8ms/step - loss: 0.2325 - acc: 0.9379 - val_loss: 1.1534 - val_acc: 0.7332\n",
      "102277/102277 [==============================] - 216s 2ms/step\n",
      "CPU times: user 6h 21min 27s, sys: 5h 2min 25s, total: 11h 23min 52s\n",
      "Wall time: 11h 27min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logloss_list = []\n",
    "for i in range(10):\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    print('*******%s*******'%i)\n",
    "    idx_train = np.array(index[index['fold'] != i]['id'])\n",
    "    idx_val = np.array(index[index['fold'] == i]['id'])\n",
    "    \n",
    "    \n",
    "    X_train = train_FE.loc[idx_train, :]\n",
    "    y_train = categorical_labels[idx_train]\n",
    "\n",
    "    X_val = train_FE.loc[idx_val, :]\n",
    "    y_val = categorical_labels[idx_val]\n",
    "    \n",
    "    X_train = get_input(X_train)\n",
    "    X_val = get_input(X_val)\n",
    "    X_test = get_input(test_FE)\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "    bst_model_path = '10folds_' + 'stack_cnn_char_v1_' + str(i) + '.hdf5'\n",
    "    early_stopping =EarlyStopping(monitor='val_acc', patience=2)\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "    callbacks = [\n",
    "            early_stopping,\n",
    "            model_checkpoint\n",
    "        ]\n",
    "    model = CNN1()\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',\\\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    hist = model.fit(X_train, y_train, \\\n",
    "            validation_data=(X_val, y_val), \\\n",
    "            epochs=30, batch_size=BATCH_SIZE, shuffle=True,callbacks=callbacks)#callbacks=callbacks\n",
    "    \n",
    "    model = CNN1()\n",
    "    model.load_weights(bst_model_path)\n",
    "    \n",
    "    prob = model.predict(X_test,batch_size=64,verbose=1)\n",
    "    meta_test = pd.concat([meta_test, pd.DataFrame(prob)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
