{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "window.load_remote_theme = true\n",
       "var theme_js = \"https://odhk.github.io/hyrule_theme/custom.js\";\n",
       "\n",
       "window.load_local_theme = function(){\n",
       "    var hostname = document.location.hostname\n",
       "    return ((hostname == \"localhost\" || hostname == '127.0.0.1') && !load_remote_theme)\n",
       "}\n",
       "\n",
       "var url = load_local_theme() ? document.location.origin + \"/files/theme/custom.js\" : theme_js\n",
       "\n",
       "$.getScript(url)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "window.load_remote_theme = true\n",
    "var theme_js = \"https://odhk.github.io/hyrule_theme/custom.js\";\n",
    "\n",
    "window.load_local_theme = function(){\n",
    "    var hostname = document.location.hostname\n",
    "    return ((hostname == \"localhost\" || hostname == '127.0.0.1') && !load_remote_theme)\n",
    "}\n",
    "\n",
    "var url = load_local_theme() ? document.location.origin + \"/files/theme/custom.js\" : theme_js\n",
    "\n",
    "$.getScript(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/libo/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "1271461it [02:15, 9403.19it/s]\n",
      "  2%|▏         | 22133/1271460 [00:00<00:05, 221252.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1271460 unique tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1271460/1271460 [00:04<00:00, 256223.11it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import keras\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Lambda, Embedding,constraints,\\\n",
    "Dropout, Activation,GRU,Bidirectional,Subtract, Permute, TimeDistributed, Reshape\n",
    "from keras.layers import Conv1D,Conv2D,MaxPooling2D,GlobalAveragePooling1D,GlobalMaxPooling1D, MaxPooling1D, Flatten\n",
    "from keras.layers import CuDNNGRU, CuDNNLSTM, SpatialDropout1D,Layer, initializers, regularizers\n",
    "from keras.layers.merge import concatenate, Concatenate, Average, Dot, Maximum, Multiply, Subtract\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from keras.activations import softmax\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import *\n",
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy.special import erfinv\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train = pd.read_csv('train_set.csv')\n",
    "test = pd.read_csv('test_set.csv')\n",
    "\n",
    "label = train['class']\n",
    "\n",
    "char_embedding_index = {}\n",
    "f = open('word_embedding_300dim_new2.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0].lower()\n",
    "    embeddings = np.asarray(values[1:], dtype = 'float32')\n",
    "    char_embedding_index[word] = embeddings\n",
    "f.close()\n",
    "\n",
    "len(char_embedding_index)\n",
    "\n",
    "api_embedding_file_path = 'word_embedding_300dim_new2.txt'\n",
    "embedding_dim = 300\n",
    "max_nb_api = len(char_embedding_index) + 1\n",
    "max_sequence_length_api =1000\n",
    "\n",
    "all_word = list(train['word_seg'].values) + list(test['word_seg'].values)\n",
    "\n",
    "all_word_list = []\n",
    "for i in list(all_word):\n",
    "    all_word_list.append(i.split(' '))\n",
    "\n",
    "len(all_word_list)\n",
    "\n",
    "train['word_seg'].apply(lambda x:len(x.split(' '))).describe()\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "tokenizer_api = Tokenizer()\n",
    "\n",
    "tokenizer_api.fit_on_texts(all_word_list)\n",
    "\n",
    "api_seq = tokenizer_api.texts_to_sequences(all_word_list)\n",
    "\n",
    "data['word_seq'] = pad_sequences(api_seq, maxlen = max_sequence_length_api, padding='post').tolist()\n",
    "\n",
    "api_index = tokenizer_api.word_index\n",
    "print('Found %s unique tokens' % len(api_index))\n",
    "nb_words = min(max_nb_api, len(char_embedding_index)+1)\n",
    "api_embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "\n",
    "for word, i in tqdm(api_index.items()):\n",
    "    embedding_vector = char_embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        api_embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        print('no char%s'%word)\n",
    "\n",
    "data['id'] = train['id']\n",
    "\n",
    "train_FE = data.iloc[:len(label),:]\n",
    "test_FE = data.iloc[len(label):,:]\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "\n",
    "def get_input(df):\n",
    "    \n",
    "    _input = [np.array(df.word_seq.values.tolist())]\n",
    "    \n",
    "    \n",
    "    return _input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gap = 100\n",
    "add_length = 20\n",
    "\n",
    "def split_sentence(data, k):\n",
    "    \n",
    "    start_index = max(0, k*gap - add_length)\n",
    "    end_index = min(add_length + (k+1)*gap, max_sequence_length_api)\n",
    "        \n",
    "    return data[:, start_index:end_index, :]\n",
    "\n",
    "def get_target_tensor(data, k):\n",
    "    \n",
    "    if k == 0:\n",
    "        return data[:, :gap, :]\n",
    "    \n",
    "    if k == max_sequence_length_api/gap:\n",
    "        return data[:, add_length:, :]\n",
    "    \n",
    "    else:\n",
    "        return data[:, add_length:(gap+add_length), :]\n",
    "\n",
    "def attention(x):\n",
    "    \n",
    "    q1, q2 = x[0], x[1]\n",
    "    \n",
    "    #compute match matrix euclidean match score\n",
    "    match_matrix = 1. + K.sqrt(\n",
    "        -2 * K.batch_dot(q1, q2, axes=[2, 2]) +\n",
    "        K.expand_dims(K.sum(K.square(q1), axis=2), 2) +\n",
    "        K.expand_dims(K.sum(K.square(q2), axis=2), 1)\n",
    "    )\n",
    "    \n",
    "    match_matrix = K.maximum(match_matrix, K.epsilon())\n",
    "    match_matrix = 1. / match_matrix\n",
    "    #compute attention output\n",
    "    q1_new = K.batch_dot(K.softmax(match_matrix, axis = 1), q2)\n",
    "    match_matrix_T = Permute((2, 1))(match_matrix)\n",
    "    q2_new = K.batch_dot(K.softmax(match_matrix_T, axis = 1), q1)\n",
    "    \n",
    "    return [q1_new, q2_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DR_Stack_RNN():\n",
    "    \n",
    "    #Input layer\n",
    "    char_input = Input(shape=(max_sequence_length_api,), dtype='int32')\n",
    "    \n",
    "    api_embedding_layer = Embedding(nb_words,\n",
    "        embedding_dim,\n",
    "        weights=[api_embedding_matrix],\n",
    "        input_length=max_sequence_length_api,\n",
    "        trainable=False)\n",
    "    \n",
    "    #RNN\n",
    "    char_rnn_layer_1 = Bidirectional(CuDNNLSTM(150, \n",
    "                               return_sequences=True))\n",
    "    char_rnn_layer_2 = Bidirectional(CuDNNLSTM(150, \n",
    "                               return_sequences=True))\n",
    "    \n",
    "    char_embedding = api_embedding_layer(char_input)\n",
    "    \n",
    "    pooled_rnn_max_1 = []\n",
    "    pooled_rnn_avg_1 = []\n",
    "    pooled_rnn_max_2 = []\n",
    "    pooled_rnn_avg_2 = []\n",
    "    \n",
    "    att_pooled_rnn_max_1 = []\n",
    "    att_pooled_rnn_max_2 = []\n",
    "    att_pooled_rnn_avg_1 = []\n",
    "    att_pooled_rnn_avg_2 = []\n",
    "    \n",
    "    for k_ in range(10):\n",
    "        \n",
    "        temp = Lambda(split_sentence, name = 'split_sentence'+str(k_), arguments={'k':k_})(char_embedding)\n",
    "        \n",
    "        temp = BatchNormalization()(temp)\n",
    "        \n",
    "        char_rnn_1= char_rnn_layer_1(temp)\n",
    "        char_rnn_2= char_rnn_layer_2(char_rnn_1)\n",
    "        \n",
    "        \n",
    "        #max and  avg for temp and outputs of rnn \n",
    "        char_rnn_maxpooling_1 = GlobalMaxPooling1D()(Lambda(get_target_tensor,arguments={'k':k_})(char_rnn_1))\n",
    "        char_rnn_avgpooling_1 = GlobalAveragePooling1D()(Lambda(get_target_tensor, arguments={'k':k_})(char_rnn_1))\n",
    "        \n",
    "        char_rnn_maxpooling_2 = GlobalMaxPooling1D()(Lambda(get_target_tensor, arguments={'k':k_})(char_rnn_2))\n",
    "        char_rnn_avgpooling_2 = GlobalAveragePooling1D()(Lambda(get_target_tensor, arguments={'k':k_})(char_rnn_2))\n",
    "        \n",
    "        pooled_rnn_max_1.append(char_rnn_maxpooling_1)\n",
    "        pooled_rnn_max_2.append(char_rnn_maxpooling_2)\n",
    "        \n",
    "        pooled_rnn_avg_1.append(char_rnn_avgpooling_1)\n",
    "        pooled_rnn_avg_2.append(char_rnn_avgpooling_2)\n",
    "        \n",
    "        #max and avg for temp and outputs of rnn after attention\n",
    "        #batch normalization\n",
    "        temp = BatchNormalization()(temp)\n",
    "        char_rnn_2 = BatchNormalization()(char_rnn_2)\n",
    "        \n",
    "        r = Lambda(attention)([temp, char_rnn_2])\n",
    "    \n",
    "        attention_map_left = r[0]\n",
    "        attention_map_right = r[1]\n",
    "        \n",
    "        attention_map_left = Lambda(get_target_tensor,arguments={'k':k_})(attention_map_left)\n",
    "        attention_map_right = Lambda(get_target_tensor,arguments={'k':k_})(attention_map_right)\n",
    "        \n",
    "        att_maxpooling_left = GlobalMaxPooling1D()(attention_map_left)\n",
    "        att_maxpooling_right = GlobalMaxPooling1D()(attention_map_right)\n",
    "\n",
    "        att_avgpooling_left = GlobalAveragePooling1D()(attention_map_left)\n",
    "        att_avgpooling_right = GlobalAveragePooling1D()(attention_map_right)\n",
    "        \n",
    "        att_pooled_rnn_max_1.append(att_maxpooling_left)\n",
    "        att_pooled_rnn_max_2.append(att_maxpooling_right)\n",
    "        \n",
    "        att_pooled_rnn_avg_1.append(att_avgpooling_left)\n",
    "        att_pooled_rnn_avg_2.append(att_avgpooling_right)\n",
    "        \n",
    "    merged_pooled_rnn_max_1 = Concatenate(axis=-1)(pooled_rnn_max_1)\n",
    "    merged_pooled_rnn_max_1_sigmoid = Dense(100, activation = 'sigmoid')(merged_pooled_rnn_max_1)\n",
    "    merged_pooled_rnn_max_1_tanh = Dense(100, activation = 'tanh')(merged_pooled_rnn_max_1)\n",
    "\n",
    "    merged_pooled_rnn_max_2 = Concatenate(axis=-1)(pooled_rnn_max_2)\n",
    "    merged_pooled_rnn_max_2_sigmoid = Dense(100, activation = 'sigmoid')(merged_pooled_rnn_max_2)\n",
    "    merged_pooled_rnn_max_2_tanh = Dense(100, activation = 'tanh')(merged_pooled_rnn_max_2)\n",
    "\n",
    "    merged_pooled_rnn_avg_1 = Concatenate(axis=-1)(pooled_rnn_avg_1)\n",
    "    merged_pooled_rnn_avg_1_sigmoid = Dense(100, activation = 'sigmoid')(merged_pooled_rnn_avg_1)\n",
    "    merged_pooled_rnn_avg_1_tanh = Dense(100, activation = 'tanh')(merged_pooled_rnn_avg_1)\n",
    "    \n",
    "    merged_pooled_rnn_avg_2 = Concatenate(axis=-1)(pooled_rnn_avg_2)\n",
    "    merged_pooled_rnn_avg_2_sigmoid = Dense(100, activation = 'sigmoid')(merged_pooled_rnn_avg_2)\n",
    "    merged_pooled_rnn_avg_2_tanh = Dense(100, activation = 'tanh')(merged_pooled_rnn_avg_2)\n",
    "    \n",
    "    #sub and multi\n",
    "    max_sub_char_abs_sigmoid = Lambda(lambda x:K.abs(x[0] - x[1]))([merged_pooled_rnn_max_1_sigmoid, merged_pooled_rnn_max_2_sigmoid])\n",
    "    max_multi_char_sigmoid = Lambda(lambda x: x[0] * x[1])([merged_pooled_rnn_max_1_sigmoid, merged_pooled_rnn_max_2_sigmoid])\n",
    "\n",
    "    avg_sub_char_abs_sigmoid = Lambda(lambda x:K.abs(x[0] - x[1]))([merged_pooled_rnn_avg_1_sigmoid, merged_pooled_rnn_avg_2_sigmoid])\n",
    "    avg_multi_char_sigmoid = Lambda(lambda x: x[0] * x[1])([merged_pooled_rnn_avg_1_sigmoid, merged_pooled_rnn_avg_2_sigmoid])\n",
    "  \n",
    "    max_sub_char_abs_tanh = Lambda(lambda x:K.abs(x[0] - x[1]))([merged_pooled_rnn_max_1_tanh, merged_pooled_rnn_max_2_tanh])\n",
    "    max_multi_char_tanh = Lambda(lambda x: x[0] * x[1])([merged_pooled_rnn_max_1_tanh, merged_pooled_rnn_max_2_tanh])\n",
    "\n",
    "    avg_sub_char_abs_tanh = Lambda(lambda x:K.abs(x[0] - x[1]))([merged_pooled_rnn_avg_1_tanh, merged_pooled_rnn_avg_2_tanh])\n",
    "    avg_multi_char_tanh = Lambda(lambda x: x[0] * x[1])([merged_pooled_rnn_avg_1_tanh, merged_pooled_rnn_avg_2_tanh])\n",
    "  \n",
    "\n",
    "\n",
    "    att_merged_pooled_rnn_max_1 = Concatenate(axis=-1)(att_pooled_rnn_max_1)\n",
    "    att_merged_pooled_rnn_max_1_sigmoid = Dense(100, activation = 'sigmoid')(att_merged_pooled_rnn_max_1)\n",
    "    att_merged_pooled_rnn_max_1_tanh = Dense(100, activation = 'tanh')(att_merged_pooled_rnn_max_1)\n",
    "\n",
    "    att_merged_pooled_rnn_max_2 = Concatenate(axis=-1)(att_pooled_rnn_max_2)\n",
    "    att_merged_pooled_rnn_max_2_sigmoid = Dense(100, activation = 'sigmoid')(att_merged_pooled_rnn_max_2)\n",
    "    att_merged_pooled_rnn_max_2_tanh = Dense(100, activation = 'tanh')(att_merged_pooled_rnn_max_2)\n",
    "\n",
    "    att_merged_pooled_rnn_avg_1 = Concatenate(axis=-1)(att_pooled_rnn_avg_1)\n",
    "    att_merged_pooled_rnn_avg_1_sigmoid = Dense(100, activation = 'sigmoid')(att_merged_pooled_rnn_avg_1)\n",
    "    att_merged_pooled_rnn_avg_1_tanh = Dense(100, activation = 'tanh')(att_merged_pooled_rnn_avg_1)\n",
    "   \n",
    "    att_merged_pooled_rnn_avg_2 = Concatenate(axis=-1)(att_pooled_rnn_avg_2)\n",
    "    att_merged_pooled_rnn_avg_2_sigmoid = Dense(100, activation = 'sigmoid')(att_merged_pooled_rnn_avg_2)\n",
    "    att_merged_pooled_rnn_avg_2_tanh = Dense(100, activation = 'tanh')(att_merged_pooled_rnn_avg_2)\n",
    "\n",
    "    \n",
    "    #sub and multi\n",
    "    max_sub_char_abs_attention_sigmoid = Lambda(lambda x:K.abs(x[0] - x[1]))([att_merged_pooled_rnn_max_1_sigmoid, att_merged_pooled_rnn_max_2_sigmoid])\n",
    "    max_sub_char_abs_attention_tanh = Lambda(lambda x:K.abs(x[0] - x[1]))([att_merged_pooled_rnn_max_1_tanh, att_merged_pooled_rnn_max_2_tanh])\n",
    "    \n",
    "    max_multi_char_attention_sigmoid = Lambda(lambda x: x[0] * x[1])([att_merged_pooled_rnn_max_1_sigmoid, att_merged_pooled_rnn_max_2_sigmoid])\n",
    "    max_multi_char_attention_tanh = Lambda(lambda x: x[0] * x[1])([att_merged_pooled_rnn_max_1_tanh, att_merged_pooled_rnn_max_2_tanh])\n",
    "\n",
    "    avg_sub_char_abs_attention_sigmoid = Lambda(lambda x:K.abs(x[0] - x[1]))([att_merged_pooled_rnn_avg_1_sigmoid, att_merged_pooled_rnn_avg_2_sigmoid])\n",
    "    avg_sub_char_abs_attention_tanh = Lambda(lambda x:K.abs(x[0] - x[1]))([att_merged_pooled_rnn_avg_1_tanh, att_merged_pooled_rnn_avg_2_tanh])\n",
    "    \n",
    "    avg_multi_char_attention_sigmoid = Lambda(lambda x: x[0] * x[1])([att_merged_pooled_rnn_avg_1_sigmoid, att_merged_pooled_rnn_avg_2_sigmoid])\n",
    "    avg_multi_char_attention_tanh = Lambda(lambda x: x[0] * x[1])([att_merged_pooled_rnn_avg_1_tanh, att_merged_pooled_rnn_avg_2_tanh])\n",
    "\n",
    "    merged = concatenate([merged_pooled_rnn_max_1_sigmoid, merged_pooled_rnn_max_2_sigmoid, \\\n",
    "                          merged_pooled_rnn_avg_1_sigmoid, merged_pooled_rnn_avg_2_sigmoid, \\\n",
    "                          att_merged_pooled_rnn_max_1_sigmoid, att_merged_pooled_rnn_max_2_sigmoid, \\\n",
    "                          att_merged_pooled_rnn_avg_1_sigmoid, att_merged_pooled_rnn_avg_2_sigmoid, \\\n",
    "                          max_sub_char_abs_sigmoid, max_multi_char_sigmoid, avg_sub_char_abs_sigmoid, avg_multi_char_sigmoid, \\\n",
    "                          max_sub_char_abs_attention_sigmoid, max_multi_char_attention_sigmoid, \\\n",
    "                          avg_sub_char_abs_attention_sigmoid, avg_multi_char_attention_sigmoid, \\\n",
    "                          \n",
    "                          merged_pooled_rnn_max_1_tanh, merged_pooled_rnn_max_2_tanh, \\\n",
    "                          merged_pooled_rnn_avg_1_tanh, merged_pooled_rnn_avg_2_tanh, \\\n",
    "                          att_merged_pooled_rnn_max_1_tanh, att_merged_pooled_rnn_max_2_tanh, \\\n",
    "                          att_merged_pooled_rnn_avg_1_tanh, att_merged_pooled_rnn_avg_2_tanh, \\\n",
    "                          max_sub_char_abs_tanh, max_multi_char_tanh, avg_sub_char_abs_tanh, avg_multi_char_tanh, \\\n",
    "                          max_sub_char_abs_attention_tanh, max_multi_char_attention_tanh, \\\n",
    "                          avg_sub_char_abs_attention_tanh, avg_multi_char_attention_tanh])\n",
    "    \n",
    "    merged = Dropout(0.5)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dense(512, activation='relu')(merged)\n",
    "    merged = Dropout(0.1)(merged)\n",
    "    merged = BatchNormalization()(merged) \n",
    "    preds = Dense(19, activation = 'softmax')(merged)\n",
    "    \n",
    "    model = Model(inputs=[char_input],outputs=preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = DR_Stack_RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train = pd.DataFrame(np.array([None]*train_FE.shape[0]*19).reshape(train_FE.shape[0], 19))\n",
    "meta_train.columns = ['prob' + str(x) for x in range(19)]\n",
    "meta_test = pd.DataFrame()\n",
    "\n",
    "label = train['class']\n",
    "\n",
    "label = label - 1\n",
    "\n",
    "categorical_labels = keras.utils.np_utils.to_categorical(label, num_classes=19)\n",
    "\n",
    "index = pd.read_csv('fold.csv')\n",
    "index['id'] = train['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******0*******\n",
      "Train on 92040 samples, validate on 10237 samples\n",
      "Epoch 1/20\n",
      "92040/92040 [==============================] - 883s 10ms/step - loss: 1.1248 - acc: 0.6817 - val_loss: 0.9034 - val_acc: 0.7384\n",
      "Epoch 2/20\n",
      "92040/92040 [==============================] - 855s 9ms/step - loss: 0.8342 - acc: 0.7602 - val_loss: 0.8135 - val_acc: 0.7650\n",
      "Epoch 3/20\n",
      "92040/92040 [==============================] - 861s 9ms/step - loss: 0.4666 - acc: 0.8615 - val_loss: 0.8776 - val_acc: 0.7714\n",
      "Epoch 7/20\n",
      "92040/92040 [==============================] - 860s 9ms/step - loss: 0.3823 - acc: 0.8842 - val_loss: 0.8990 - val_acc: 0.7692\n",
      "10237/10237 [==============================] - 31s 3ms/step\n",
      "102277/102277 [==============================] - 282s 3ms/step\n",
      "*******1*******\n",
      "Train on 92043 samples, validate on 10234 samples\n",
      "Epoch 1/20\n",
      "92043/92043 [==============================] - 870s 9ms/step - loss: 1.1155 - acc: 0.6842 - val_loss: 0.8990 - val_acc: 0.7443\n",
      "Epoch 2/20\n",
      "92043/92043 [==============================] - 853s 9ms/step - loss: 0.8266 - acc: 0.7630 - val_loss: 0.8595 - val_acc: 0.7525\n",
      "Epoch 3/20\n",
      "92043/92043 [==============================] - 859s 9ms/step - loss: 0.7252 - acc: 0.7903 - val_loss: 0.8446 - val_acc: 0.7600\n",
      "Epoch 4/20\n",
      "92043/92043 [==============================] - 852s 9ms/step - loss: 0.6366 - acc: 0.8129 - val_loss: 0.8171 - val_acc: 0.7700\n",
      "Epoch 5/20\n",
      "92043/92043 [==============================] - 859s 9ms/step - loss: 0.5475 - acc: 0.8398 - val_loss: 0.8364 - val_acc: 0.7658\n",
      "Epoch 6/20\n",
      "92043/92043 [==============================] - 862s 9ms/step - loss: 0.4612 - acc: 0.8615 - val_loss: 0.8795 - val_acc: 0.7703\n",
      "Epoch 7/20\n",
      "92043/92043 [==============================] - 861s 9ms/step - loss: 0.3792 - acc: 0.8851 - val_loss: 0.9311 - val_acc: 0.7654\n",
      "Epoch 8/20\n",
      "92043/92043 [==============================] - 863s 9ms/step - loss: 0.3110 - acc: 0.9043 - val_loss: 1.0011 - val_acc: 0.7633\n",
      "10234/10234 [==============================] - 31s 3ms/step\n",
      "102277/102277 [==============================] - 280s 3ms/step\n",
      "*******2*******\n",
      "Train on 92044 samples, validate on 10233 samples\n",
      "Epoch 1/20\n",
      "92044/92044 [==============================] - 872s 9ms/step - loss: 1.1212 - acc: 0.6831 - val_loss: 0.8587 - val_acc: 0.7561\n",
      "Epoch 2/20\n",
      "92044/92044 [==============================] - 862s 9ms/step - loss: 0.8306 - acc: 0.7611 - val_loss: 0.8237 - val_acc: 0.7582\n",
      "Epoch 3/20\n",
      "92044/92044 [==============================] - 863s 9ms/step - loss: 0.7335 - acc: 0.7875 - val_loss: 0.7870 - val_acc: 0.7717\n",
      "Epoch 4/20\n",
      "92044/92044 [==============================] - 857s 9ms/step - loss: 0.6440 - acc: 0.8118 - val_loss: 0.7858 - val_acc: 0.7752\n",
      "Epoch 5/20\n",
      "92044/92044 [==============================] - 855s 9ms/step - loss: 0.5529 - acc: 0.8377 - val_loss: 0.8047 - val_acc: 0.7778\n",
      "Epoch 6/20\n",
      "92044/92044 [==============================] - 862s 9ms/step - loss: 0.4610 - acc: 0.8614 - val_loss: 0.8309 - val_acc: 0.7742\n",
      "Epoch 7/20\n",
      "92044/92044 [==============================] - 860s 9ms/step - loss: 0.3784 - acc: 0.8849 - val_loss: 0.9007 - val_acc: 0.7747\n",
      "10233/10233 [==============================] - 31s 3ms/step\n",
      "102277/102277 [==============================] - 283s 3ms/step\n",
      "*******3*******\n",
      "Train on 92046 samples, validate on 10231 samples\n",
      "Epoch 1/20\n",
      "92046/92046 [==============================] - 875s 10ms/step - loss: 1.1215 - acc: 0.6826 - val_loss: 0.8621 - val_acc: 0.7523\n",
      "Epoch 2/20\n",
      "92046/92046 [==============================] - 861s 9ms/step - loss: 0.8335 - acc: 0.7599 - val_loss: 0.7951 - val_acc: 0.7712\n",
      "Epoch 3/20\n",
      "92046/92046 [==============================] - 862s 9ms/step - loss: 0.7316 - acc: 0.7874 - val_loss: 0.7913 - val_acc: 0.7691\n",
      "Epoch 4/20\n",
      "92046/92046 [==============================] - 863s 9ms/step - loss: 0.6438 - acc: 0.8118 - val_loss: 0.7754 - val_acc: 0.7806\n",
      "Epoch 5/20\n",
      "92046/92046 [==============================] - 864s 9ms/step - loss: 0.5530 - acc: 0.8367 - val_loss: 0.8172 - val_acc: 0.7797\n",
      "Epoch 6/20\n",
      "92046/92046 [==============================] - 855s 9ms/step - loss: 0.4633 - acc: 0.8616 - val_loss: 0.8334 - val_acc: 0.7752\n",
      "10231/10231 [==============================] - 31s 3ms/step\n",
      "102277/102277 [==============================] - 281s 3ms/step\n",
      "*******4*******\n",
      "Train on 92049 samples, validate on 10228 samples\n",
      "Epoch 1/20\n",
      "92049/92049 [==============================] - 875s 10ms/step - loss: 1.1228 - acc: 0.6824 - val_loss: 0.8621 - val_acc: 0.7495\n",
      "Epoch 2/20\n",
      "92049/92049 [==============================] - 862s 9ms/step - loss: 0.8283 - acc: 0.7622 - val_loss: 0.8179 - val_acc: 0.7608\n",
      "Epoch 3/20\n",
      "92049/92049 [==============================] - 863s 9ms/step - loss: 0.7341 - acc: 0.7876 - val_loss: 0.7874 - val_acc: 0.7714\n",
      "Epoch 4/20\n",
      "92049/92049 [==============================] - 862s 9ms/step - loss: 0.6443 - acc: 0.8112 - val_loss: 0.7851 - val_acc: 0.7734\n",
      "Epoch 5/20\n",
      "92049/92049 [==============================] - 863s 9ms/step - loss: 0.5601 - acc: 0.8345 - val_loss: 0.8075 - val_acc: 0.7667\n",
      "Epoch 6/20\n",
      "92049/92049 [==============================] - 862s 9ms/step - loss: 0.4714 - acc: 0.8594 - val_loss: 0.8305 - val_acc: 0.7696\n",
      "10228/10228 [==============================] - 30s 3ms/step\n",
      "102277/102277 [==============================] - 280s 3ms/step\n",
      "*******5*******\n",
      "Train on 92051 samples, validate on 10226 samples\n",
      "Epoch 1/20\n",
      "92051/92051 [==============================] - 875s 10ms/step - loss: 1.1218 - acc: 0.6842 - val_loss: 0.8699 - val_acc: 0.7417\n",
      "Epoch 2/20\n",
      "92051/92051 [==============================] - 856s 9ms/step - loss: 0.8303 - acc: 0.7608 - val_loss: 0.8238 - val_acc: 0.7612\n",
      "Epoch 3/20\n",
      "92051/92051 [==============================] - 861s 9ms/step - loss: 0.7325 - acc: 0.7885 - val_loss: 0.7813 - val_acc: 0.7721\n",
      "Epoch 4/20\n",
      "92051/92051 [==============================] - 863s 9ms/step - loss: 0.6439 - acc: 0.8106 - val_loss: 0.7875 - val_acc: 0.7771\n",
      "Epoch 5/20\n",
      "92051/92051 [==============================] - 863s 9ms/step - loss: 0.5512 - acc: 0.8367 - val_loss: 0.8028 - val_acc: 0.7742\n",
      "Epoch 6/20\n",
      "92051/92051 [==============================] - 864s 9ms/step - loss: 0.4613 - acc: 0.8627 - val_loss: 0.8562 - val_acc: 0.7747\n",
      "10226/10226 [==============================] - 31s 3ms/step\n",
      "102277/102277 [==============================] - 281s 3ms/step\n",
      "*******6*******\n",
      "Train on 92053 samples, validate on 10224 samples\n",
      "Epoch 1/20\n",
      "92053/92053 [==============================] - 875s 10ms/step - loss: 1.1297 - acc: 0.6805 - val_loss: 0.8420 - val_acc: 0.7528\n",
      "Epoch 2/20\n",
      "92053/92053 [==============================] - 861s 9ms/step - loss: 0.8329 - acc: 0.7601 - val_loss: 0.7816 - val_acc: 0.7769\n",
      "Epoch 3/20\n",
      "92053/92053 [==============================] - 860s 9ms/step - loss: 0.7356 - acc: 0.7854 - val_loss: 0.8010 - val_acc: 0.7729\n",
      "Epoch 4/20\n",
      "92053/92053 [==============================] - 849s 9ms/step - loss: 0.6481 - acc: 0.8107 - val_loss: 0.7567 - val_acc: 0.7840\n",
      "Epoch 5/20\n",
      "92053/92053 [==============================] - 861s 9ms/step - loss: 0.5591 - acc: 0.8353 - val_loss: 0.7733 - val_acc: 0.7869\n",
      "Epoch 6/20\n",
      "92053/92053 [==============================] - 858s 9ms/step - loss: 0.4686 - acc: 0.8597 - val_loss: 0.8213 - val_acc: 0.7836\n",
      "Epoch 7/20\n",
      "92053/92053 [==============================] - 861s 9ms/step - loss: 0.3855 - acc: 0.8827 - val_loss: 0.8396 - val_acc: 0.7837\n",
      "10224/10224 [==============================] - 31s 3ms/step\n",
      "102277/102277 [==============================] - 278s 3ms/step\n",
      "*******7*******\n",
      "Train on 92054 samples, validate on 10223 samples\n",
      "Epoch 1/20\n",
      "92054/92054 [==============================] - 875s 10ms/step - loss: 1.1280 - acc: 0.6815 - val_loss: 0.9022 - val_acc: 0.7430\n",
      "Epoch 2/20\n",
      "92054/92054 [==============================] - 863s 9ms/step - loss: 0.8304 - acc: 0.7605 - val_loss: 0.8170 - val_acc: 0.7676\n",
      "Epoch 3/20\n",
      "92054/92054 [==============================] - 863s 9ms/step - loss: 0.7304 - acc: 0.7888 - val_loss: 0.7954 - val_acc: 0.7760\n",
      "Epoch 4/20\n",
      "92054/92054 [==============================] - 861s 9ms/step - loss: 0.6424 - acc: 0.8114 - val_loss: 0.7933 - val_acc: 0.7731\n",
      "Epoch 5/20\n",
      "92054/92054 [==============================] - 863s 9ms/step - loss: 0.5528 - acc: 0.8371 - val_loss: 0.8104 - val_acc: 0.7758\n",
      "10223/10223 [==============================] - 31s 3ms/step\n",
      "102277/102277 [==============================] - 270s 3ms/step\n",
      "*******8*******\n",
      "Train on 92056 samples, validate on 10221 samples\n",
      "Epoch 1/20\n",
      "92056/92056 [==============================] - 876s 10ms/step - loss: 1.1321 - acc: 0.6807 - val_loss: 0.8503 - val_acc: 0.7490\n",
      "Epoch 2/20\n",
      "92056/92056 [==============================] - 863s 9ms/step - loss: 0.8373 - acc: 0.7600 - val_loss: 0.7862 - val_acc: 0.7702\n",
      "Epoch 3/20\n",
      "92056/92056 [==============================] - 863s 9ms/step - loss: 0.7364 - acc: 0.7873 - val_loss: 0.7638 - val_acc: 0.7791\n",
      "Epoch 4/20\n",
      "92056/92056 [==============================] - 861s 9ms/step - loss: 0.6512 - acc: 0.8082 - val_loss: 0.7496 - val_acc: 0.7836\n",
      "Epoch 5/20\n",
      "92056/92056 [==============================] - 862s 9ms/step - loss: 0.5579 - acc: 0.8353 - val_loss: 0.7682 - val_acc: 0.7810\n",
      "Epoch 6/20\n",
      "92057/92057 [==============================] - 787s 9ms/step - loss: 0.6506 - acc: 0.8106 - val_loss: 0.7627 - val_acc: 0.7812\n",
      "Epoch 5/20\n",
      "92057/92057 [==============================] - 789s 9ms/step - loss: 0.5558 - acc: 0.8358 - val_loss: 0.7898 - val_acc: 0.7789\n",
      "Epoch 6/20\n",
      "92057/92057 [==============================] - 786s 9ms/step - loss: 0.4623 - acc: 0.8617 - val_loss: 0.8397 - val_acc: 0.7750\n",
      "10220/10220 [==============================] - 27s 3ms/step\n",
      "102277/102277 [==============================] - 250s 2ms/step\n",
      "CPU times: user 19h 38min 22s, sys: 2h 2min 36s, total: 21h 40min 59s\n",
      "Wall time: 16h 30min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#logloss_list = []\n",
    "for i in range(10):\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    print('*******%s*******'%i)\n",
    "    idx_train = np.array(index[index['fold'] != i]['id'])\n",
    "    idx_val = np.array(index[index['fold'] == i]['id'])\n",
    "    \n",
    "    \n",
    "    X_train = train_FE.loc[idx_train, :]\n",
    "    y_train = categorical_labels[idx_train]\n",
    "\n",
    "    X_val = train_FE.loc[idx_val, :]\n",
    "    y_val = categorical_labels[idx_val]\n",
    "\n",
    "    #dtest = test.loc[:,user_col]\n",
    "    \n",
    "    X_train = get_input(X_train)\n",
    "    X_val = get_input(X_val)\n",
    "    X_test = get_input(test_FE)\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "    bst_model_path = '10folds_' + 'base_Stack_att_RNN_char_v1_300dim' + str(i) + '.hdf5'\n",
    "    early_stopping =EarlyStopping(monitor='val_acc', patience=2)\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "    callbacks = [\n",
    "            early_stopping,\n",
    "            model_checkpoint\n",
    "        ]\n",
    "    model = DR_Stack_RNN()\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',\\\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    hist = model.fit(X_train, y_train, \\\n",
    "            validation_data=(X_val, y_val), \\\n",
    "            epochs=20, batch_size=BATCH_SIZE, shuffle=True, callbacks=callbacks)#callbacks=callbacks\n",
    "    \n",
    "    model = DR_Stack_RNN()\n",
    "    model.load_weights(bst_model_path)\n",
    "    \n",
    "    pred = pd.DataFrame(model.predict(X_val,batch_size=128,verbose=1))\n",
    "    \n",
    "    for i in range(19):\n",
    "        meta_train.loc[idx_val, 'prob' + str(i)] = pred[i].values\n",
    "    \n",
    "    prob = model.predict(X_test,batch_size=128,verbose=1)\n",
    "    meta_test = pd.concat([meta_test, pd.DataFrame(prob)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102277, 190)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(test['id'])\n",
    "\n",
    "p = pd.DataFrame()\n",
    "\n",
    "for i in range(19):\n",
    "    p[i] = meta_test[i].mean(axis = 1)\n",
    "\n",
    "pred['class'] = np.argmax(p.values, axis=1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred.to_csv('/home/libo/daguan/stack_new/9.6.1.csv', index = None ,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******0*******\n",
      "Train on 92040 samples, validate on 10237 samples\n",
      "Epoch 1/20\n",
      "92040/92040 [==============================] - 916s 10ms/step - loss: 1.0493 - acc: 0.6998 - val_loss: 0.8728 - val_acc: 0.7432\n",
      "Epoch 2/20\n",
      "92040/92040 [==============================] - 895s 10ms/step - loss: 0.7944 - acc: 0.7669 - val_loss: 0.8042 - val_acc: 0.7665\n",
      "Epoch 3/20\n",
      "92040/92040 [==============================] - 904s 10ms/step - loss: 0.6897 - acc: 0.7940 - val_loss: 0.8143 - val_acc: 0.7682\n",
      "Epoch 4/20\n",
      "92040/92040 [==============================] - 903s 10ms/step - loss: 0.5933 - acc: 0.8185 - val_loss: 0.8104 - val_acc: 0.7733\n",
      "Epoch 5/20\n",
      "92040/92040 [==============================] - 906s 10ms/step - loss: 0.4884 - acc: 0.8489 - val_loss: 0.8304 - val_acc: 0.7704\n",
      "Epoch 6/20\n",
      "92040/92040 [==============================] - 904s 10ms/step - loss: 0.3921 - acc: 0.8753 - val_loss: 0.8811 - val_acc: 0.7683\n",
      "10237/10237 [==============================] - 32s 3ms/step\n",
      "102277/102277 [==============================] - 302s 3ms/step\n",
      "*******1*******\n",
      "Train on 92043 samples, validate on 10234 samples\n",
      "Epoch 1/20\n",
      "32256/92043 [=========>....................] - ETA: 9:38 - loss: 1.2351 - acc: 0.6526"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#logloss_list = []\n",
    "for i in range(10):\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    print('*******%s*******'%i)\n",
    "    idx_train = np.array(index[index['fold'] != i]['id'])\n",
    "    idx_val = np.array(index[index['fold'] == i]['id'])\n",
    "    \n",
    "    \n",
    "    X_train = train_FE.loc[idx_train, :]\n",
    "    y_train = categorical_labels[idx_train]\n",
    "\n",
    "    X_val = train_FE.loc[idx_val, :]\n",
    "    y_val = categorical_labels[idx_val]\n",
    "\n",
    "    #dtest = test.loc[:,user_col]\n",
    "    \n",
    "    X_train = get_input(X_train)\n",
    "    X_val = get_input(X_val)\n",
    "    X_test = get_input(test_FE)\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "    bst_model_path = '10folds_' + 'base_Stack_att_RNN_char_v1_300dim' + str(i) + '.hdf5'\n",
    "    early_stopping =EarlyStopping(monitor='val_acc', patience=2)\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "    callbacks = [\n",
    "            early_stopping,\n",
    "            model_checkpoint\n",
    "        ]\n",
    "    model = DR_Stack_RNN()\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',\\\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    hist = model.fit(X_train, y_train, \\\n",
    "            validation_data=(X_val, y_val), \\\n",
    "            epochs=20, batch_size=BATCH_SIZE, shuffle=True, callbacks=callbacks)#callbacks=callbacks\n",
    "    \n",
    "    model = DR_Stack_RNN()\n",
    "    model.load_weights(bst_model_path)\n",
    "    \n",
    "    pred = pd.DataFrame(model.predict(X_val,batch_size=128,verbose=1))\n",
    "    \n",
    "    for i in range(19):\n",
    "        meta_train.loc[idx_val, 'prob' + str(i)] = pred[i].values\n",
    "    \n",
    "    prob = model.predict(X_test,batch_size=128,verbose=1)\n",
    "    meta_test = pd.concat([meta_test, pd.DataFrame(prob)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******0*******\n",
      "Train on 92040 samples, validate on 10237 samples\n",
      "Epoch 1/20\n",
      "92040/92040 [==============================] - 919s 10ms/step - loss: 1.0332 - acc: 0.7065 - val_loss: 0.8520 - val_acc: 0.7503\n",
      "Epoch 2/20\n",
      "92040/92040 [==============================] - 908s 10ms/step - loss: 0.7780 - acc: 0.7713 - val_loss: 0.7937 - val_acc: 0.7658\n",
      "Epoch 3/20\n",
      " 1792/92040 [..............................] - ETA: 14:06 - loss: 0.6358 - acc: 0.8125"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#logloss_list = []\n",
    "for i in range(10):\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    print('*******%s*******'%i)\n",
    "    idx_train = np.array(index[index['fold'] != i]['id'])\n",
    "    idx_val = np.array(index[index['fold'] == i]['id'])\n",
    "    \n",
    "    \n",
    "    X_train = train_FE.loc[idx_train, :]\n",
    "    y_train = categorical_labels[idx_train]\n",
    "\n",
    "    X_val = train_FE.loc[idx_val, :]\n",
    "    y_val = categorical_labels[idx_val]\n",
    "\n",
    "    #dtest = test.loc[:,user_col]\n",
    "    \n",
    "    X_train = get_input(X_train)\n",
    "    X_val = get_input(X_val)\n",
    "    X_test = get_input(test_FE)\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "    bst_model_path = '10folds_' + 'base_Stack_att_RNN_char_v1_300dim' + str(i) + '.hdf5'\n",
    "    early_stopping =EarlyStopping(monitor='val_acc', patience=2)\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "    callbacks = [\n",
    "            early_stopping,\n",
    "            model_checkpoint\n",
    "        ]\n",
    "    model = DR_Stack_RNN()\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',\\\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    hist = model.fit(X_train, y_train, \\\n",
    "            validation_data=(X_val, y_val), \\\n",
    "            epochs=20, batch_size=BATCH_SIZE, shuffle=True, callbacks=callbacks)#callbacks=callbacks\n",
    "    \n",
    "    model = DR_Stack_RNN()\n",
    "    model.load_weights(bst_model_path)\n",
    "    \n",
    "    pred = pd.DataFrame(model.predict(X_val,batch_size=128,verbose=1))\n",
    "    \n",
    "    for i in range(19):\n",
    "        meta_train.loc[idx_val, 'prob' + str(i)] = pred[i].values\n",
    "    \n",
    "    prob = model.predict(X_test,batch_size=128,verbose=1)\n",
    "    meta_test = pd.concat([meta_test, pd.DataFrame(prob)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******0*******\n",
      "Train on 92040 samples, validate on 10237 samples\n",
      "Epoch 1/20\n",
      "92040/92040 [==============================] - 632s 7ms/step - loss: 1.0284 - acc: 0.7043 - val_loss: 0.8407 - val_acc: 0.7508\n",
      "Epoch 2/20\n",
      "92040/92040 [==============================] - 627s 7ms/step - loss: 0.7790 - acc: 0.7704 - val_loss: 0.7890 - val_acc: 0.7685\n",
      "Epoch 3/20\n",
      "92040/92040 [==============================] - 625s 7ms/step - loss: 0.6721 - acc: 0.7982 - val_loss: 0.7773 - val_acc: 0.7731\n",
      "Epoch 4/20\n",
      "92040/92040 [==============================] - 626s 7ms/step - loss: 0.5674 - acc: 0.8253 - val_loss: 0.7945 - val_acc: 0.7754\n",
      "Epoch 5/20\n",
      "92040/92040 [==============================] - 625s 7ms/step - loss: 0.4576 - acc: 0.8551 - val_loss: 0.8414 - val_acc: 0.7752\n",
      "Epoch 6/20\n",
      "92040/92040 [==============================] - 625s 7ms/step - loss: 0.3502 - acc: 0.8862 - val_loss: 0.8898 - val_acc: 0.7652\n",
      "10237/10237 [==============================] - 22s 2ms/step\n",
      "102277/102277 [==============================] - 210s 2ms/step\n",
      "*******1*******\n",
      "Train on 92043 samples, validate on 10234 samples\n",
      "Epoch 1/20\n",
      "92043/92043 [==============================] - 629s 7ms/step - loss: 1.0253 - acc: 0.7040 - val_loss: 0.8667 - val_acc: 0.7523\n",
      "Epoch 2/20\n",
      "92043/92043 [==============================] - 625s 7ms/step - loss: 0.7785 - acc: 0.7704 - val_loss: 0.8284 - val_acc: 0.7588\n",
      "Epoch 3/20\n",
      "92043/92043 [==============================] - 624s 7ms/step - loss: 0.6713 - acc: 0.7984 - val_loss: 0.8148 - val_acc: 0.7630\n",
      "Epoch 4/20\n",
      "92043/92043 [==============================] - 622s 7ms/step - loss: 0.5630 - acc: 0.8275 - val_loss: 0.8293 - val_acc: 0.7635\n",
      "Epoch 5/20\n",
      "92043/92043 [==============================] - 624s 7ms/step - loss: 0.4558 - acc: 0.8563 - val_loss: 0.8873 - val_acc: 0.7660\n",
      "Epoch 6/20\n",
      "92043/92043 [==============================] - 627s 7ms/step - loss: 0.3449 - acc: 0.8895 - val_loss: 0.9880 - val_acc: 0.7652\n",
      "Epoch 7/20\n",
      "92043/92043 [==============================] - 626s 7ms/step - loss: 0.2617 - acc: 0.9142 - val_loss: 1.0306 - val_acc: 0.7631\n",
      "10234/10234 [==============================] - 22s 2ms/step\n",
      "102277/102277 [==============================] - 210s 2ms/step\n",
      "*******2*******\n",
      "Train on 92044 samples, validate on 10233 samples\n",
      "Epoch 1/20\n",
      "92044/92044 [==============================] - 631s 7ms/step - loss: 1.0314 - acc: 0.7049 - val_loss: 0.8576 - val_acc: 0.7490\n",
      "Epoch 2/20\n",
      "92044/92044 [==============================] - 627s 7ms/step - loss: 0.7831 - acc: 0.7683 - val_loss: 0.7893 - val_acc: 0.7660\n",
      "Epoch 3/20\n",
      "92044/92044 [==============================] - 625s 7ms/step - loss: 0.6727 - acc: 0.7976 - val_loss: 0.7687 - val_acc: 0.7751\n",
      "Epoch 4/20\n",
      "92044/92044 [==============================] - 626s 7ms/step - loss: 0.5680 - acc: 0.8243 - val_loss: 0.8065 - val_acc: 0.7766\n",
      "Epoch 5/20\n",
      "92044/92044 [==============================] - 627s 7ms/step - loss: 0.4582 - acc: 0.8565 - val_loss: 0.8332 - val_acc: 0.7775\n",
      "Epoch 6/20\n",
      "92044/92044 [==============================] - 628s 7ms/step - loss: 0.3514 - acc: 0.8870 - val_loss: 0.9161 - val_acc: 0.7724\n",
      "Epoch 7/20\n",
      "92044/92044 [==============================] - 627s 7ms/step - loss: 0.2670 - acc: 0.9118 - val_loss: 1.0007 - val_acc: 0.7695\n",
      "10233/10233 [==============================] - 22s 2ms/step\n",
      "102277/102277 [==============================] - 211s 2ms/step\n",
      "*******3*******\n",
      "Train on 92046 samples, validate on 10231 samples\n",
      "Epoch 1/20\n",
      "92046/92046 [==============================] - 629s 7ms/step - loss: 1.0338 - acc: 0.7046 - val_loss: 0.8421 - val_acc: 0.7566\n",
      "Epoch 2/20\n",
      "92046/92046 [==============================] - 626s 7ms/step - loss: 0.7807 - acc: 0.7694 - val_loss: 0.7921 - val_acc: 0.7686\n",
      "Epoch 3/20\n",
      "92046/92046 [==============================] - 625s 7ms/step - loss: 0.6731 - acc: 0.7963 - val_loss: 0.7985 - val_acc: 0.7702\n",
      "Epoch 4/20\n",
      "92046/92046 [==============================] - 625s 7ms/step - loss: 0.5666 - acc: 0.8260 - val_loss: 0.7939 - val_acc: 0.7788\n",
      "Epoch 5/20\n",
      "92046/92046 [==============================] - 624s 7ms/step - loss: 0.4556 - acc: 0.8572 - val_loss: 0.8478 - val_acc: 0.7737\n",
      "Epoch 6/20\n",
      "92046/92046 [==============================] - 625s 7ms/step - loss: 0.3452 - acc: 0.8894 - val_loss: 0.9251 - val_acc: 0.7713\n",
      "10231/10231 [==============================] - 22s 2ms/step\n",
      "102277/102277 [==============================] - 210s 2ms/step\n",
      "*******4*******\n",
      "Train on 92049 samples, validate on 10228 samples\n",
      "Epoch 1/20\n",
      "92049/92049 [==============================] - 633s 7ms/step - loss: 1.0335 - acc: 0.7040 - val_loss: 0.8393 - val_acc: 0.7560\n",
      "Epoch 2/20\n",
      "92049/92049 [==============================] - 629s 7ms/step - loss: 0.7784 - acc: 0.7707 - val_loss: 0.7867 - val_acc: 0.7639\n",
      "Epoch 3/20\n",
      "92049/92049 [==============================] - 629s 7ms/step - loss: 0.6725 - acc: 0.7982 - val_loss: 0.7750 - val_acc: 0.7775\n",
      "Epoch 4/20\n",
      "92049/92049 [==============================] - 628s 7ms/step - loss: 0.5654 - acc: 0.8270 - val_loss: 0.7728 - val_acc: 0.7737\n",
      "Epoch 5/20\n",
      "92049/92049 [==============================] - 630s 7ms/step - loss: 0.4539 - acc: 0.8563 - val_loss: 0.8404 - val_acc: 0.7695\n",
      "10228/10228 [==============================] - 22s 2ms/step\n",
      "102277/102277 [==============================] - 210s 2ms/step\n",
      "*******5*******\n",
      "Train on 92051 samples, validate on 10226 samples\n",
      "Epoch 1/20\n",
      "92051/92051 [==============================] - 628s 7ms/step - loss: 1.0308 - acc: 0.7036 - val_loss: 0.8387 - val_acc: 0.7538\n",
      "Epoch 2/20\n",
      "92051/92051 [==============================] - 622s 7ms/step - loss: 0.7805 - acc: 0.7708 - val_loss: 0.7966 - val_acc: 0.7633\n",
      "Epoch 3/20\n",
      "92051/92051 [==============================] - 625s 7ms/step - loss: 0.6708 - acc: 0.7980 - val_loss: 0.8058 - val_acc: 0.7648\n",
      "Epoch 4/20\n",
      "92051/92051 [==============================] - 625s 7ms/step - loss: 0.5650 - acc: 0.8266 - val_loss: 0.8025 - val_acc: 0.7728\n",
      "Epoch 5/20\n",
      "92051/92051 [==============================] - 626s 7ms/step - loss: 0.4526 - acc: 0.8578 - val_loss: 0.8285 - val_acc: 0.7712\n",
      "Epoch 6/20\n",
      "92051/92051 [==============================] - 627s 7ms/step - loss: 0.3430 - acc: 0.8894 - val_loss: 0.9308 - val_acc: 0.7695\n",
      "10226/10226 [==============================] - 22s 2ms/step\n",
      "102277/102277 [==============================] - 212s 2ms/step\n",
      "*******6*******\n",
      "Train on 92053 samples, validate on 10224 samples\n",
      "Epoch 1/20\n",
      "92053/92053 [==============================] - 628s 7ms/step - loss: 1.0319 - acc: 0.7048 - val_loss: 0.8192 - val_acc: 0.7659\n",
      "Epoch 2/20\n",
      "92053/92053 [==============================] - 625s 7ms/step - loss: 0.7896 - acc: 0.7674 - val_loss: 0.7674 - val_acc: 0.7750\n",
      "Epoch 3/20\n",
      "92053/92053 [==============================] - 627s 7ms/step - loss: 0.6785 - acc: 0.7955 - val_loss: 0.7364 - val_acc: 0.7889\n",
      "Epoch 4/20\n",
      "92053/92053 [==============================] - 626s 7ms/step - loss: 0.5733 - acc: 0.8233 - val_loss: 0.7589 - val_acc: 0.7849\n",
      "Epoch 5/20\n",
      "92053/92053 [==============================] - 627s 7ms/step - loss: 0.4633 - acc: 0.8551 - val_loss: 0.8176 - val_acc: 0.7815\n",
      "10224/10224 [==============================] - 22s 2ms/step\n",
      "102277/102277 [==============================] - 211s 2ms/step\n",
      "*******7*******\n",
      "Train on 92054 samples, validate on 10223 samples\n",
      "Epoch 1/20\n",
      "92054/92054 [==============================] - 630s 7ms/step - loss: 1.0303 - acc: 0.7047 - val_loss: 0.8666 - val_acc: 0.7502\n",
      "Epoch 2/20\n",
      "92054/92054 [==============================] - 626s 7ms/step - loss: 0.7821 - acc: 0.7698 - val_loss: 0.8010 - val_acc: 0.7757\n",
      "Epoch 3/20\n",
      "92054/92054 [==============================] - 623s 7ms/step - loss: 0.6724 - acc: 0.7981 - val_loss: 0.7967 - val_acc: 0.7716\n",
      "Epoch 4/20\n",
      "92054/92054 [==============================] - 624s 7ms/step - loss: 0.5673 - acc: 0.8261 - val_loss: 0.8098 - val_acc: 0.7758\n",
      "Epoch 5/20\n",
      "92054/92054 [==============================] - 624s 7ms/step - loss: 0.4551 - acc: 0.8558 - val_loss: 0.8641 - val_acc: 0.7702\n",
      "Epoch 6/20\n",
      "92054/92054 [==============================] - 624s 7ms/step - loss: 0.3472 - acc: 0.8884 - val_loss: 0.9563 - val_acc: 0.7705\n",
      "10223/10223 [==============================] - 22s 2ms/step\n",
      "102277/102277 [==============================] - 210s 2ms/step\n",
      "*******8*******\n",
      "Train on 92056 samples, validate on 10221 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92056/92056 [==============================] - 631s 7ms/step - loss: 1.0342 - acc: 0.7050 - val_loss: 0.8177 - val_acc: 0.7581\n",
      "Epoch 2/20\n",
      "92056/92056 [==============================] - 626s 7ms/step - loss: 0.7882 - acc: 0.7686 - val_loss: 0.7696 - val_acc: 0.7753\n",
      "Epoch 3/20\n",
      "77440/92057 [========================>.....] - ETA: 1:35 - loss: 1.0606 - acc: 0.6966"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#logloss_list = []\n",
    "for i in range(10):\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    print('*******%s*******'%i)\n",
    "    idx_train = np.array(index[index['fold'] != i]['id'])\n",
    "    idx_val = np.array(index[index['fold'] == i]['id'])\n",
    "    \n",
    "    \n",
    "    X_train = train_FE.loc[idx_train, :]\n",
    "    y_train = categorical_labels[idx_train]\n",
    "\n",
    "    X_val = train_FE.loc[idx_val, :]\n",
    "    y_val = categorical_labels[idx_val]\n",
    "\n",
    "    #dtest = test.loc[:,user_col]\n",
    "    \n",
    "    X_train = get_input(X_train)\n",
    "    X_val = get_input(X_val)\n",
    "    X_test = get_input(test_FE)\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "    bst_model_path = '10folds_' + 'base_Stack_att_RNN_char_v1_300dim' + str(i) + '.hdf5'\n",
    "    early_stopping =EarlyStopping(monitor='val_acc', patience=2)\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "    callbacks = [\n",
    "            early_stopping,\n",
    "            model_checkpoint\n",
    "        ]\n",
    "    model = DR_Stack_RNN()\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',\\\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    hist = model.fit(X_train, y_train, \\\n",
    "            validation_data=(X_val, y_val), \\\n",
    "            epochs=20, batch_size=BATCH_SIZE, shuffle=True, callbacks=callbacks)#callbacks=callbacks\n",
    "    \n",
    "    model = DR_Stack_RNN()\n",
    "    model.load_weights(bst_model_path)\n",
    "    \n",
    "    pred = pd.DataFrame(model.predict(X_val,batch_size=128,verbose=1))\n",
    "    \n",
    "    for i in range(19):\n",
    "        meta_train.loc[idx_val, 'prob' + str(i)] = pred[i].values\n",
    "    \n",
    "    prob = model.predict(X_test,batch_size=128,verbose=1)\n",
    "    meta_test = pd.concat([meta_test, pd.DataFrame(prob)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train.columns = ['meta_base_Stack_att_RNN_char_300dim_v1' + x for x in meta_train.columns]\n",
    "\n",
    "p = pd.DataFrame()\n",
    "\n",
    "for i in range(19):\n",
    "    p[i] = meta_test[i].mean(axis = 1)\n",
    "\n",
    "p.columns = meta_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((102277, 19), (102277, 19))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape, meta_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train.to_csv('/home/libo/daguan/stack_new/train_meta_base_Stack_att_RNN_char_300dim_v1.csv', index = None ,encoding = 'utf-8')\n",
    "\n",
    "p.to_csv('/home/libo/daguan/stack_new/test_metabase_Stack_att_RNN_char_300dim_v1.csv', index = None ,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_set.csv')\n",
    "\n",
    "pred = pd.DataFrame(test['id'])\n",
    "\n",
    "pred['class'] = np.argmax(p.values, axis=1) + 1\n",
    "\n",
    "pred.to_csv('/home/libo/daguan/stack_new/9.5.1.csv', index = None ,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102247</th>\n",
       "      <td>102247</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102248</th>\n",
       "      <td>102248</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102249</th>\n",
       "      <td>102249</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102250</th>\n",
       "      <td>102250</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102251</th>\n",
       "      <td>102251</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102252</th>\n",
       "      <td>102252</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102253</th>\n",
       "      <td>102253</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102254</th>\n",
       "      <td>102254</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102255</th>\n",
       "      <td>102255</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102256</th>\n",
       "      <td>102256</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102257</th>\n",
       "      <td>102257</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102258</th>\n",
       "      <td>102258</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102259</th>\n",
       "      <td>102259</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102260</th>\n",
       "      <td>102260</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102261</th>\n",
       "      <td>102261</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102262</th>\n",
       "      <td>102262</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102263</th>\n",
       "      <td>102263</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102264</th>\n",
       "      <td>102264</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102265</th>\n",
       "      <td>102265</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102266</th>\n",
       "      <td>102266</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102267</th>\n",
       "      <td>102267</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102268</th>\n",
       "      <td>102268</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102269</th>\n",
       "      <td>102269</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102270</th>\n",
       "      <td>102270</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102271</th>\n",
       "      <td>102271</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102272</th>\n",
       "      <td>102272</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102273</th>\n",
       "      <td>102273</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102274</th>\n",
       "      <td>102274</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102275</th>\n",
       "      <td>102275</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102276</th>\n",
       "      <td>102276</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102277 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  class\n",
       "0            0      5\n",
       "1            1      4\n",
       "2            2     13\n",
       "3            3      4\n",
       "4            4      5\n",
       "5            5      5\n",
       "6            6     15\n",
       "7            7     19\n",
       "8            8      3\n",
       "9            9     12\n",
       "10          10     14\n",
       "11          11      2\n",
       "12          12      5\n",
       "13          13     13\n",
       "14          14     15\n",
       "15          15     18\n",
       "16          16      9\n",
       "17          17      9\n",
       "18          18      3\n",
       "19          19     14\n",
       "20          20      9\n",
       "21          21     12\n",
       "22          22      9\n",
       "23          23      9\n",
       "24          24      2\n",
       "25          25     15\n",
       "26          26      3\n",
       "27          27     17\n",
       "28          28      4\n",
       "29          29     19\n",
       "...        ...    ...\n",
       "102247  102247     16\n",
       "102248  102248      8\n",
       "102249  102249     10\n",
       "102250  102250      2\n",
       "102251  102251     18\n",
       "102252  102252      5\n",
       "102253  102253     18\n",
       "102254  102254      2\n",
       "102255  102255     14\n",
       "102256  102256      1\n",
       "102257  102257      6\n",
       "102258  102258     12\n",
       "102259  102259     15\n",
       "102260  102260     14\n",
       "102261  102261     19\n",
       "102262  102262      2\n",
       "102263  102263     12\n",
       "102264  102264     15\n",
       "102265  102265     14\n",
       "102266  102266     19\n",
       "102267  102267      3\n",
       "102268  102268      8\n",
       "102269  102269     14\n",
       "102270  102270      6\n",
       "102271  102271     15\n",
       "102272  102272      8\n",
       "102273  102273     12\n",
       "102274  102274      6\n",
       "102275  102275     14\n",
       "102276  102276     13\n",
       "\n",
       "[102277 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
