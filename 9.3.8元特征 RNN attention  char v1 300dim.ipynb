{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "window.load_remote_theme = true\n",
       "var theme_js = \"https://odhk.github.io/hyrule_theme/custom.js\";\n",
       "\n",
       "window.load_local_theme = function(){\n",
       "    var hostname = document.location.hostname\n",
       "    return ((hostname == \"localhost\" || hostname == '127.0.0.1') && !load_remote_theme)\n",
       "}\n",
       "\n",
       "var url = load_local_theme() ? document.location.origin + \"/files/theme/custom.js\" : theme_js\n",
       "\n",
       "$.getScript(url)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "window.load_remote_theme = true\n",
    "var theme_js = \"https://odhk.github.io/hyrule_theme/custom.js\";\n",
    "\n",
    "window.load_local_theme = function(){\n",
    "    var hostname = document.location.hostname\n",
    "    return ((hostname == \"localhost\" || hostname == '127.0.0.1') && !load_remote_theme)\n",
    "}\n",
    "\n",
    "var url = load_local_theme() ? document.location.origin + \"/files/theme/custom.js\" : theme_js\n",
    "\n",
    "$.getScript(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16053it [00:01, 12752.21it/s]\n",
      "100%|██████████| 16052/16052 [00:00<00:00, 417345.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16052 unique tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import keras\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Lambda, Embedding,constraints,\\\n",
    "Dropout, Activation,GRU,Bidirectional,Subtract, Permute, TimeDistributed, Reshape\n",
    "from keras.layers import Conv1D,Conv2D,MaxPooling2D,GlobalAveragePooling1D,GlobalMaxPooling1D, MaxPooling1D, Flatten\n",
    "from keras.layers import CuDNNGRU, CuDNNLSTM, SpatialDropout1D,Layer, initializers, regularizers\n",
    "from keras.layers.merge import concatenate, Concatenate, Average, Dot, Maximum, Multiply, Subtract\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from keras.activations import softmax\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import *\n",
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy.special import erfinv\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train = pd.read_csv('train_set.csv')\n",
    "test = pd.read_csv('test_set.csv')\n",
    "\n",
    "label = train['class']\n",
    "\n",
    "char_embedding_index = {}\n",
    "f = open('word_embedding_300dim_new.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0].lower()\n",
    "    embeddings = np.asarray(values[1:], dtype = 'float32')\n",
    "    char_embedding_index[word] = embeddings\n",
    "f.close()\n",
    "\n",
    "len(char_embedding_index)\n",
    "\n",
    "api_embedding_file_path = 'word_embedding_300dim_new.txt'\n",
    "embedding_dim = 300\n",
    "max_nb_api = len(char_embedding_index) + 1\n",
    "max_sequence_length_api =1500\n",
    "\n",
    "all_word = list(train['article'].values) + list(test['article'].values)\n",
    "\n",
    "all_word_list = []\n",
    "for i in list(all_word):\n",
    "    all_word_list.append(i.split(' '))\n",
    "\n",
    "len(all_word_list)\n",
    "\n",
    "train['article'].apply(lambda x:len(x.split(' '))).describe()\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "tokenizer_api = Tokenizer()\n",
    "\n",
    "tokenizer_api.fit_on_texts(all_word_list)\n",
    "\n",
    "api_seq = tokenizer_api.texts_to_sequences(all_word_list)\n",
    "\n",
    "data['article'] = pad_sequences(api_seq, maxlen = max_sequence_length_api, padding='post').tolist()\n",
    "\n",
    "api_index = tokenizer_api.word_index\n",
    "print('Found %s unique tokens' % len(api_index))\n",
    "nb_words = min(max_nb_api, len(char_embedding_index)+1)\n",
    "api_embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "\n",
    "for word, i in tqdm(api_index.items()):\n",
    "    embedding_vector = char_embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        api_embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        print('no char%s'%word)\n",
    "\n",
    "data['id'] = train['id']\n",
    "\n",
    "train_FE = data.iloc[:len(label),:]\n",
    "test_FE = data.iloc[len(label):,:]\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6, 7\"\n",
    "\n",
    "def get_input(df):\n",
    "    \n",
    "    _input = [np.array(df.article.values.tolist())]\n",
    "    \n",
    "    \n",
    "    return _input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention(x):\n",
    "    \n",
    "    q1, q2 = x[0], x[1]\n",
    "    \n",
    "    #compute match matrix euclidean match score\n",
    "    match_matrix = 1. + K.sqrt(\n",
    "        -2 * K.batch_dot(q1, q2, axes=[2, 2]) +\n",
    "        K.expand_dims(K.sum(K.square(q1), axis=2), 2) +\n",
    "        K.expand_dims(K.sum(K.square(q2), axis=2), 1)\n",
    "    )\n",
    "    \n",
    "    match_matrix = K.maximum(match_matrix, K.epsilon())\n",
    "    match_matrix = 1. / match_matrix\n",
    "    #compute attention output\n",
    "    q1_new = K.batch_dot(K.softmax(match_matrix, axis = 1), q2)\n",
    "    match_matrix_T = Permute((2, 1))(match_matrix)\n",
    "    q2_new = K.batch_dot(K.softmax(match_matrix_T, axis = 1), q1)\n",
    "    \n",
    "    return [q1_new, q2_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Globel_attention_DR_Stack_RNN():\n",
    "    \n",
    "    #Input layer\n",
    "    char_input = Input(shape=(max_sequence_length_api,), dtype='int32')\n",
    "    \n",
    "    api_embedding_layer = Embedding(nb_words,\n",
    "        embedding_dim,\n",
    "        weights=[api_embedding_matrix],\n",
    "        input_length=max_sequence_length_api,\n",
    "        trainable=False)\n",
    "    \n",
    "    #RNN\n",
    "    char_rnn_layer_1 = Bidirectional(CuDNNLSTM(150, \n",
    "                               return_sequences=True))\n",
    "    char_rnn_layer_2 = Bidirectional(CuDNNLSTM(150, \n",
    "                               return_sequences=True))\n",
    "    \n",
    "    char_embedding = api_embedding_layer(char_input)\n",
    "    char_embedding = SpatialDropout1D(0.2)(char_embedding)\n",
    "    \n",
    "    #双层BiLSTM\n",
    "    char_rnn_1= char_rnn_layer_1(char_embedding)\n",
    "    char_rnn_2= char_rnn_layer_2(char_rnn_1)\n",
    "    \n",
    "    #max avg \n",
    "    char_att_maxpooling_1 = GlobalMaxPooling1D()(char_rnn_1)\n",
    "    char_att_avgpooling_1 = GlobalAveragePooling1D()(char_rnn_1)\n",
    "    \n",
    "    char_att_maxpooling_2 = GlobalMaxPooling1D()(char_rnn_2)\n",
    "    char_att_avgpooling_2 = GlobalAveragePooling1D()(char_rnn_2)\n",
    "    \n",
    "    #batchnormalization\n",
    "    char_embedding = BatchNormalization()(char_embedding)\n",
    "    char_rnn_2 = BatchNormalization()(char_rnn_2)\n",
    "    \n",
    "    #globel attention information\n",
    "    r = Lambda(attention, name = 'attention')([char_embedding, char_rnn_2])\n",
    "    \n",
    "    attention_map_left = r[0]\n",
    "    attention_map_right = r[1]\n",
    "    \n",
    "    att_maxpooling_left = GlobalMaxPooling1D()(attention_map_left)\n",
    "    att_maxpooling_right = GlobalMaxPooling1D()(attention_map_right)\n",
    "    \n",
    "    att_avgpooling_left = GlobalAveragePooling1D()(attention_map_left)\n",
    "    att_avgpooling_right = GlobalAveragePooling1D()(attention_map_right)\n",
    "   \n",
    "    max_sub_char_abs_attention = Lambda(lambda x:K.abs(x[0] - x[1]))([att_maxpooling_left, att_maxpooling_right])\n",
    "    max_multi_char_attention = Lambda(lambda x: x[0] * x[1])([att_maxpooling_left, att_maxpooling_right])\n",
    "\n",
    "    avg_sub_char_abs_attention = Lambda(lambda x:K.abs(x[0] - x[1]))([att_avgpooling_left, att_avgpooling_right])\n",
    "    avg_multi_char_attention = Lambda(lambda x: x[0] * x[1])([att_avgpooling_left, att_avgpooling_right])\n",
    "\n",
    "    merged = concatenate([char_att_maxpooling_1, char_att_avgpooling_1, \\\n",
    "                          char_att_maxpooling_2, char_att_avgpooling_2,\\\n",
    "                         max_sub_char_abs_attention, max_multi_char_attention, \\\n",
    "                         avg_sub_char_abs_attention, avg_multi_char_attention, \\\n",
    "                         att_maxpooling_left, att_maxpooling_right, \\\n",
    "                         att_avgpooling_left, att_avgpooling_right])\n",
    "    merged = Dropout(0.4)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dense(512, activation='relu')(merged)\n",
    "    merged = Dropout(0.1)(merged)\n",
    "    merged = BatchNormalization()(merged) \n",
    "    preds = Dense(19, activation = 'softmax')(merged)\n",
    "    \n",
    "    model = Model(inputs=[char_input],outputs=preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train = pd.DataFrame(np.array([None]*train_FE.shape[0]*19).reshape(train_FE.shape[0], 19))\n",
    "meta_train.columns = ['prob' + str(x) for x in range(19)]\n",
    "meta_test = pd.DataFrame()\n",
    "\n",
    "label = train['class']\n",
    "\n",
    "label = label - 1\n",
    "\n",
    "categorical_labels = keras.utils.np_utils.to_categorical(label, num_classes=19)\n",
    "\n",
    "index = pd.read_csv('fold.csv')\n",
    "index['id'] = train['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del test, all_word\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******0*******\n",
      "Train on 92040 samples, validate on 10237 samples\n",
      "Epoch 1/10\n",
      "92040/92040 [==============================] - 1083s 12ms/step - loss: 1.4212 - acc: 0.5843 - val_loss: 1.1379 - val_acc: 0.6685\n",
      "Epoch 2/10\n",
      "92040/92040 [==============================] - 1082s 12ms/step - loss: 1.0931 - acc: 0.6754 - val_loss: 1.0125 - val_acc: 0.7039\n",
      "Epoch 3/10\n",
      "92040/92040 [==============================] - 1085s 12ms/step - loss: 0.8367 - acc: 0.7476 - val_loss: 0.8689 - val_acc: 0.7401\n",
      "Epoch 8/10\n",
      "92040/92040 [==============================] - 1086s 12ms/step - loss: 0.8025 - acc: 0.7573 - val_loss: 0.8330 - val_acc: 0.7513\n",
      "Epoch 9/10\n",
      "92040/92040 [==============================] - 1085s 12ms/step - loss: 0.7769 - acc: 0.7639 - val_loss: 0.8293 - val_acc: 0.7525\n",
      "Epoch 10/10\n",
      "92040/92040 [==============================] - 1086s 12ms/step - loss: 0.7511 - acc: 0.7726 - val_loss: 0.8415 - val_acc: 0.7494\n",
      "10237/10237 [==============================] - 33s 3ms/step\n",
      "102277/102277 [==============================] - 332s 3ms/step\n",
      "*******1*******\n",
      "Train on 92043 samples, validate on 10234 samples\n",
      "Epoch 1/10\n",
      "92043/92043 [==============================] - 1094s 12ms/step - loss: 1.4156 - acc: 0.5888 - val_loss: 1.1922 - val_acc: 0.6617\n",
      "Epoch 2/10\n",
      "92043/92043 [==============================] - 1083s 12ms/step - loss: 1.0806 - acc: 0.6797 - val_loss: 1.0627 - val_acc: 0.6954\n",
      "Epoch 3/10\n",
      "92043/92043 [==============================] - 1084s 12ms/step - loss: 0.9871 - acc: 0.7072 - val_loss: 0.9537 - val_acc: 0.7199\n",
      "Epoch 4/10\n",
      "92043/92043 [==============================] - 1083s 12ms/step - loss: 0.9283 - acc: 0.7231 - val_loss: 0.9690 - val_acc: 0.7192\n",
      "Epoch 5/10\n",
      "92043/92043 [==============================] - 1083s 12ms/step - loss: 0.8858 - acc: 0.7356 - val_loss: 0.9123 - val_acc: 0.7304\n",
      "Epoch 6/10\n",
      "92043/92043 [==============================] - 1085s 12ms/step - loss: 0.8491 - acc: 0.7448 - val_loss: 0.8871 - val_acc: 0.7418\n",
      "Epoch 7/10\n",
      "92043/92043 [==============================] - 1086s 12ms/step - loss: 0.8154 - acc: 0.7546 - val_loss: 0.8932 - val_acc: 0.7363\n",
      "Epoch 8/10\n",
      "92043/92043 [==============================] - 1085s 12ms/step - loss: 0.7870 - acc: 0.7618 - val_loss: 0.8847 - val_acc: 0.7374\n",
      "Epoch 9/10\n",
      "92043/92043 [==============================] - 1085s 12ms/step - loss: 0.7576 - acc: 0.7692 - val_loss: 0.8752 - val_acc: 0.7461\n",
      "Epoch 10/10\n",
      "92043/92043 [==============================] - 1086s 12ms/step - loss: 0.7318 - acc: 0.7771 - val_loss: 0.8488 - val_acc: 0.7545\n",
      "10234/10234 [==============================] - 34s 3ms/step\n",
      "102277/102277 [==============================] - 334s 3ms/step\n",
      "*******2*******\n",
      "Train on 92044 samples, validate on 10233 samples\n",
      "Epoch 1/10\n",
      "92044/92044 [==============================] - 1098s 12ms/step - loss: 1.4105 - acc: 0.5889 - val_loss: 1.2266 - val_acc: 0.6519\n",
      "Epoch 2/10\n",
      "92044/92044 [==============================] - 1086s 12ms/step - loss: 1.0855 - acc: 0.6786 - val_loss: 1.0404 - val_acc: 0.6950\n",
      "Epoch 3/10\n",
      "92044/92044 [==============================] - 1087s 12ms/step - loss: 0.9926 - acc: 0.7049 - val_loss: 1.0103 - val_acc: 0.6998\n",
      "Epoch 4/10\n",
      "92044/92044 [==============================] - 1086s 12ms/step - loss: 0.9357 - acc: 0.7220 - val_loss: 0.9005 - val_acc: 0.7389\n",
      "Epoch 5/10\n",
      "92044/92044 [==============================] - 1087s 12ms/step - loss: 0.8928 - acc: 0.7318 - val_loss: 0.9007 - val_acc: 0.7350\n",
      "Epoch 6/10\n",
      "92044/92044 [==============================] - 1087s 12ms/step - loss: 0.8575 - acc: 0.7427 - val_loss: 0.8648 - val_acc: 0.7463\n",
      "Epoch 7/10\n",
      "92044/92044 [==============================] - 1086s 12ms/step - loss: 0.8305 - acc: 0.7503 - val_loss: 0.8905 - val_acc: 0.7356\n",
      "Epoch 8/10\n",
      "92044/92044 [==============================] - 1086s 12ms/step - loss: 0.8031 - acc: 0.7579 - val_loss: 0.8589 - val_acc: 0.7421\n",
      "Epoch 9/10\n",
      "92044/92044 [==============================] - 1086s 12ms/step - loss: 0.7697 - acc: 0.7668 - val_loss: 0.8150 - val_acc: 0.7570\n",
      "Epoch 10/10\n",
      "92044/92044 [==============================] - 1087s 12ms/step - loss: 0.7480 - acc: 0.7720 - val_loss: 0.8176 - val_acc: 0.7607\n",
      "10233/10233 [==============================] - 34s 3ms/step\n",
      "102277/102277 [==============================] - 333s 3ms/step\n",
      "*******3*******\n",
      "Train on 92046 samples, validate on 10231 samples\n",
      "Epoch 1/10\n",
      "92046/92046 [==============================] - 1094s 12ms/step - loss: 1.4135 - acc: 0.5868 - val_loss: 1.2216 - val_acc: 0.6378\n",
      "Epoch 2/10\n",
      "92046/92046 [==============================] - 1084s 12ms/step - loss: 1.0888 - acc: 0.6780 - val_loss: 0.9716 - val_acc: 0.7185\n",
      "Epoch 3/10\n",
      "92046/92046 [==============================] - 1084s 12ms/step - loss: 0.9906 - acc: 0.7073 - val_loss: 0.9382 - val_acc: 0.7215\n",
      "Epoch 4/10\n",
      "92046/92046 [==============================] - 1084s 12ms/step - loss: 0.9335 - acc: 0.7206 - val_loss: 0.9058 - val_acc: 0.7295\n",
      "Epoch 5/10\n",
      "92046/92046 [==============================] - 1085s 12ms/step - loss: 0.8902 - acc: 0.7346 - val_loss: 0.9233 - val_acc: 0.7251\n",
      "Epoch 6/10\n",
      "92046/92046 [==============================] - 1084s 12ms/step - loss: 0.8570 - acc: 0.7429 - val_loss: 0.8486 - val_acc: 0.7450\n",
      "Epoch 7/10\n",
      "92046/92046 [==============================] - 1085s 12ms/step - loss: 0.8193 - acc: 0.7522 - val_loss: 0.8347 - val_acc: 0.7546\n",
      "Epoch 8/10\n",
      "92046/92046 [==============================] - 1084s 12ms/step - loss: 0.7942 - acc: 0.7605 - val_loss: 0.8292 - val_acc: 0.7551\n",
      "Epoch 9/10\n",
      "92046/92046 [==============================] - 1085s 12ms/step - loss: 0.7672 - acc: 0.7663 - val_loss: 0.8298 - val_acc: 0.7534\n",
      "Epoch 10/10\n",
      "92046/92046 [==============================] - 1085s 12ms/step - loss: 0.7405 - acc: 0.7744 - val_loss: 0.8302 - val_acc: 0.7548\n",
      "10231/10231 [==============================] - 34s 3ms/step\n",
      "102277/102277 [==============================] - 333s 3ms/step\n",
      "*******4*******\n",
      "Train on 92049 samples, validate on 10228 samples\n",
      "Epoch 1/10\n",
      "92049/92049 [==============================] - 1101s 12ms/step - loss: 1.4298 - acc: 0.5824 - val_loss: 1.1656 - val_acc: 0.6515\n",
      "Epoch 2/10\n",
      "92049/92049 [==============================] - 1085s 12ms/step - loss: 1.0976 - acc: 0.6752 - val_loss: 1.0452 - val_acc: 0.6967\n",
      "Epoch 3/10\n",
      "92049/92049 [==============================] - 1087s 12ms/step - loss: 1.0005 - acc: 0.7028 - val_loss: 1.0368 - val_acc: 0.6858\n",
      "Epoch 4/10\n",
      "92049/92049 [==============================] - 1086s 12ms/step - loss: 0.9418 - acc: 0.7188 - val_loss: 0.9408 - val_acc: 0.7204\n",
      "Epoch 5/10\n",
      "92049/92049 [==============================] - 1086s 12ms/step - loss: 0.8938 - acc: 0.7321 - val_loss: 0.8811 - val_acc: 0.7392\n",
      "Epoch 6/10\n",
      "92049/92049 [==============================] - 1087s 12ms/step - loss: 0.8571 - acc: 0.7420 - val_loss: 0.8409 - val_acc: 0.7494\n",
      "Epoch 7/10\n",
      "92049/92049 [==============================] - 1086s 12ms/step - loss: 0.7678 - acc: 0.7677 - val_loss: 0.8111 - val_acc: 0.7584\n",
      "Epoch 10/10\n",
      "92049/92049 [==============================] - 1086s 12ms/step - loss: 0.7397 - acc: 0.7744 - val_loss: 0.8085 - val_acc: 0.7624\n",
      "10228/10228 [==============================] - 33s 3ms/step\n",
      "102277/102277 [==============================] - 332s 3ms/step\n",
      "*******5*******\n",
      "Train on 92051 samples, validate on 10226 samples\n",
      "Epoch 1/10\n",
      "92051/92051 [==============================] - 1098s 12ms/step - loss: 1.4119 - acc: 0.5868 - val_loss: 1.1048 - val_acc: 0.6708\n",
      "Epoch 2/10\n",
      "92051/92051 [==============================] - 1086s 12ms/step - loss: 1.1009 - acc: 0.6755 - val_loss: 1.0013 - val_acc: 0.7054\n",
      "Epoch 3/10\n",
      "92051/92051 [==============================] - 1086s 12ms/step - loss: 0.9942 - acc: 0.7045 - val_loss: 0.9792 - val_acc: 0.7146\n",
      "Epoch 4/10\n",
      "92051/92051 [==============================] - 1086s 12ms/step - loss: 0.9367 - acc: 0.7205 - val_loss: 0.9100 - val_acc: 0.7341\n",
      "Epoch 5/10\n",
      "92051/92051 [==============================] - 1086s 12ms/step - loss: 0.8947 - acc: 0.7327 - val_loss: 0.8787 - val_acc: 0.7399\n",
      "Epoch 6/10\n",
      "92051/92051 [==============================] - 1086s 12ms/step - loss: 0.8567 - acc: 0.7428 - val_loss: 0.8816 - val_acc: 0.7405\n",
      "Epoch 7/10\n",
      "92051/92051 [==============================] - 1086s 12ms/step - loss: 0.8264 - acc: 0.7510 - val_loss: 0.8640 - val_acc: 0.7449\n",
      "Epoch 8/10\n",
      "92051/92051 [==============================] - 1085s 12ms/step - loss: 0.7970 - acc: 0.7592 - val_loss: 0.8448 - val_acc: 0.7468\n",
      "Epoch 9/10\n",
      "92051/92051 [==============================] - 1086s 12ms/step - loss: 0.7685 - acc: 0.7667 - val_loss: 0.8250 - val_acc: 0.7515\n",
      "Epoch 10/10\n",
      "92051/92051 [==============================] - 1086s 12ms/step - loss: 0.7437 - acc: 0.7727 - val_loss: 0.8171 - val_acc: 0.7594\n",
      "10226/10226 [==============================] - 34s 3ms/step\n",
      "102277/102277 [==============================] - 333s 3ms/step\n",
      "*******6*******\n",
      "Train on 92053 samples, validate on 10224 samples\n",
      "Epoch 1/10\n",
      "92053/92053 [==============================] - 1092s 12ms/step - loss: 1.4190 - acc: 0.5862 - val_loss: 1.1297 - val_acc: 0.6692\n",
      "Epoch 2/10\n",
      "92053/92053 [==============================] - 1083s 12ms/step - loss: 1.0856 - acc: 0.6789 - val_loss: 1.0267 - val_acc: 0.6954\n",
      "Epoch 3/10\n",
      "92053/92053 [==============================] - 1083s 12ms/step - loss: 0.9954 - acc: 0.7029 - val_loss: 0.8977 - val_acc: 0.7359\n",
      "Epoch 4/10\n",
      "92053/92053 [==============================] - 1084s 12ms/step - loss: 0.9370 - acc: 0.7204 - val_loss: 0.9508 - val_acc: 0.7210\n",
      "Epoch 5/10\n",
      "92053/92053 [==============================] - 1082s 12ms/step - loss: 0.8931 - acc: 0.7323 - val_loss: 0.8618 - val_acc: 0.7520\n",
      "Epoch 6/10\n",
      "92053/92053 [==============================] - 1084s 12ms/step - loss: 0.8582 - acc: 0.7415 - val_loss: 0.8299 - val_acc: 0.7550\n",
      "Epoch 7/10\n",
      "92053/92053 [==============================] - 1084s 12ms/step - loss: 0.8252 - acc: 0.7506 - val_loss: 0.8403 - val_acc: 0.7524\n",
      "Epoch 8/10\n",
      "92053/92053 [==============================] - 1085s 12ms/step - loss: 0.7917 - acc: 0.7608 - val_loss: 0.8070 - val_acc: 0.7624\n",
      "Epoch 9/10\n",
      "92053/92053 [==============================] - 1084s 12ms/step - loss: 0.7674 - acc: 0.7675 - val_loss: 0.7954 - val_acc: 0.7646\n",
      "Epoch 10/10\n",
      "74944/92053 [=======================>......] - ETA: 3:13 - loss: 0.7369 - acc: 0.7745"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#logloss_list = []\n",
    "for i in range(10):\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    print('*******%s*******'%i)\n",
    "    idx_train = np.array(index[index['fold'] != i]['id'])\n",
    "    idx_val = np.array(index[index['fold'] == i]['id'])\n",
    "    \n",
    "    \n",
    "    X_train = train_FE.loc[idx_train, :]\n",
    "    y_train = categorical_labels[idx_train]\n",
    "\n",
    "    X_val = train_FE.loc[idx_val, :]\n",
    "    y_val = categorical_labels[idx_val]\n",
    "\n",
    "    #dtest = test.loc[:,user_col]\n",
    "    \n",
    "    X_train = get_input(X_train)\n",
    "    X_val = get_input(X_val)\n",
    "    X_test = get_input(test_FE)\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    model = Globel_attention_DR_Stack_RNN()\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',\\\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    hist = model.fit(X_train, y_train, \\\n",
    "            validation_data=(X_val, y_val), \\\n",
    "            epochs=10, batch_size=BATCH_SIZE, shuffle=True)#callbacks=callbacks\n",
    "\n",
    "    \n",
    "    pred = pd.DataFrame(model.predict(X_val,batch_size=128,verbose=1))\n",
    "    \n",
    "    for i in range(19):\n",
    "        meta_train.loc[idx_val, 'prob' + str(i)] = pred[i].values\n",
    "    \n",
    "    prob = model.predict(X_test,batch_size=128,verbose=1)\n",
    "    meta_test = pd.concat([meta_test, pd.DataFrame(prob)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train.columns = ['meta_rnn_v7_dim_300_' + x for x in meta_train.columns]\n",
    "\n",
    "p = pd.DataFrame()\n",
    "\n",
    "for i in range(19):\n",
    "    p[i] = meta_test[i].mean(axis = 1)\n",
    "\n",
    "p.columns = meta_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((102277, 19), (102277, 19))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_train.shape, p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train.to_csv('/home/libo/daguan/stack/train_meta_rnn_v7_dim_300.csv', index = None ,encoding = 'utf-8')\n",
    "\n",
    "p.to_csv('/home/libo/daguan/stack/test_meta_rnn_v7_dim_300.csv', index = None ,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
